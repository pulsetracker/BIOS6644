{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################################################################################\n",
    "#                                                                                        #\n",
    "#  888888b.  8888888 .d88888b.   .d8888b.     .d8888b.   .d8888b.      d8888      d8888  #\n",
    "#  888  \"88b   888  d88P\" \"Y88b d88P  Y88b   d88P  Y88b d88P  Y88b    d8P888     d8P888  #\n",
    "#  888  .88P   888  888     888 Y88b.        888        888          d8P 888    d8P 888  #\n",
    "#  8888888K.   888  888     888  \"Y888b.     888d888b.  888d888b.   d8P  888   d8P  888  #\n",
    "#  888  \"Y88b  888  888     888     \"Y88b.   888P \"Y88b 888P \"Y88b d88   888  d88   888  #\n",
    "#  888    888  888  888     888       \"888   888    888 888    888 8888888888 8888888888 #\n",
    "#  888   d88P  888  Y88b. .d88P Y88b  d88P   Y88b  d88P Y88b  d88P       888        888  #\n",
    "#  8888888P\" 8888888 \"Y88888P\"   \"Y8888P\"     \"Y8888P\"   \"Y8888P\"        888        888  #\n",
    "#                                                                                        # \n",
    "##########################################################################################\n",
    "#\n",
    "# Wrangling Sugar Metabolomics Data\n",
    "#\n",
    "##########################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################################################################################\n",
    "#\n",
    "# YouDo:\n",
    "#    1) Make a copy of this notebook with your name as a suffix:  \n",
    "#       BIOS6644_CSV_Sugar_Metabolomics_FirstLast.ipynb\n",
    "#    2) Do all work in this new notebook.\n",
    "#    3) Submit completed work via GitHub pull request\n",
    "#\n",
    "##################################################################################################################\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wrangling Sugar Metabolomics Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pylab as plt\n",
    "import numpy as np\n",
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General info here:\n",
    "#   https://data.mendeley.com/datasets/9z7ncwvxnz/1\n",
    "#\n",
    "# Data here:\n",
    "#   https://data.mendeley.com/datasets/9z7ncwvxnz/1/files/b921d0ef-075c-4a2d-9810-ff8df787c9c9\n",
    "\n",
    "# I renamed mine to \"sugar_metabolomics.csv\" so I could remember what it is\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Label</th>\n",
       "      <th>Pyruvate</th>\n",
       "      <th>MethylSuccinate</th>\n",
       "      <th>Phosphoglyceric Acid</th>\n",
       "      <th>Glucose 1-phosphate</th>\n",
       "      <th>Malonic Acid</th>\n",
       "      <th>Fumaric Acid</th>\n",
       "      <th>Alpha-Ketoglutaric Acid</th>\n",
       "      <th>Sarcosine</th>\n",
       "      <th>...</th>\n",
       "      <th>3-Hydroxybenzoic acid</th>\n",
       "      <th>Succinate</th>\n",
       "      <th>Glucose</th>\n",
       "      <th>V127</th>\n",
       "      <th>V128</th>\n",
       "      <th>V129</th>\n",
       "      <th>V130</th>\n",
       "      <th>V131</th>\n",
       "      <th>V132</th>\n",
       "      <th>V133</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>14.158545</td>\n",
       "      <td>15.624522</td>\n",
       "      <td>13.383600</td>\n",
       "      <td>11.923638</td>\n",
       "      <td>13.933560</td>\n",
       "      <td>12.787696</td>\n",
       "      <td>14.981440</td>\n",
       "      <td>11.739484</td>\n",
       "      <td>...</td>\n",
       "      <td>3.161390</td>\n",
       "      <td>2.165842</td>\n",
       "      <td>8.310291624</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>24</td>\n",
       "      <td>0</td>\n",
       "      <td>14.180287</td>\n",
       "      <td>15.325665</td>\n",
       "      <td>13.468363</td>\n",
       "      <td>12.046294</td>\n",
       "      <td>13.043221</td>\n",
       "      <td>12.536227</td>\n",
       "      <td>14.593418</td>\n",
       "      <td>11.833211</td>\n",
       "      <td>...</td>\n",
       "      <td>2.355910</td>\n",
       "      <td>2.260621</td>\n",
       "      <td>8.423733003</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>13.921329</td>\n",
       "      <td>16.156867</td>\n",
       "      <td>14.615061</td>\n",
       "      <td>13.051929</td>\n",
       "      <td>13.381985</td>\n",
       "      <td>12.419600</td>\n",
       "      <td>14.701978</td>\n",
       "      <td>12.232070</td>\n",
       "      <td>...</td>\n",
       "      <td>2.472365</td>\n",
       "      <td>2.295447</td>\n",
       "      <td>8.357070943</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>52</td>\n",
       "      <td>0</td>\n",
       "      <td>13.785605</td>\n",
       "      <td>15.939535</td>\n",
       "      <td>13.206615</td>\n",
       "      <td>12.342156</td>\n",
       "      <td>12.448068</td>\n",
       "      <td>12.211098</td>\n",
       "      <td>14.860913</td>\n",
       "      <td>11.949448</td>\n",
       "      <td>...</td>\n",
       "      <td>1.844567</td>\n",
       "      <td>2.278589</td>\n",
       "      <td>8.414503996</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>54</td>\n",
       "      <td>0</td>\n",
       "      <td>13.674234</td>\n",
       "      <td>15.743668</td>\n",
       "      <td>11.542008</td>\n",
       "      <td>11.807872</td>\n",
       "      <td>14.582068</td>\n",
       "      <td>11.635154</td>\n",
       "      <td>14.970451</td>\n",
       "      <td>11.457569</td>\n",
       "      <td>...</td>\n",
       "      <td>3.773262</td>\n",
       "      <td>2.136011</td>\n",
       "      <td>8.193979575</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>56</td>\n",
       "      <td>0</td>\n",
       "      <td>13.447093</td>\n",
       "      <td>15.595200</td>\n",
       "      <td>13.118302</td>\n",
       "      <td>12.002027</td>\n",
       "      <td>12.817962</td>\n",
       "      <td>12.065246</td>\n",
       "      <td>14.796780</td>\n",
       "      <td>11.831409</td>\n",
       "      <td>...</td>\n",
       "      <td>2.165502</td>\n",
       "      <td>2.439154</td>\n",
       "      <td>8.346804747</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>57</td>\n",
       "      <td>0</td>\n",
       "      <td>13.616625</td>\n",
       "      <td>15.406841</td>\n",
       "      <td>14.124967</td>\n",
       "      <td>12.499758</td>\n",
       "      <td>13.168693</td>\n",
       "      <td>12.235455</td>\n",
       "      <td>14.721283</td>\n",
       "      <td>11.691816</td>\n",
       "      <td>...</td>\n",
       "      <td>2.431075</td>\n",
       "      <td>2.396853</td>\n",
       "      <td>8.139349683</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>58</td>\n",
       "      <td>0</td>\n",
       "      <td>13.731086</td>\n",
       "      <td>15.449381</td>\n",
       "      <td>13.108050</td>\n",
       "      <td>11.948349</td>\n",
       "      <td>14.492458</td>\n",
       "      <td>12.008819</td>\n",
       "      <td>14.727796</td>\n",
       "      <td>11.733394</td>\n",
       "      <td>...</td>\n",
       "      <td>3.595748</td>\n",
       "      <td>2.325542</td>\n",
       "      <td>8.359634173</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>59</td>\n",
       "      <td>0</td>\n",
       "      <td>14.468678</td>\n",
       "      <td>15.438259</td>\n",
       "      <td>13.737836</td>\n",
       "      <td>12.204044</td>\n",
       "      <td>12.950250</td>\n",
       "      <td>12.466876</td>\n",
       "      <td>14.826535</td>\n",
       "      <td>12.037085</td>\n",
       "      <td>...</td>\n",
       "      <td>2.148865</td>\n",
       "      <td>2.286202</td>\n",
       "      <td>8.402214167</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>60</td>\n",
       "      <td>0</td>\n",
       "      <td>13.589152</td>\n",
       "      <td>15.895696</td>\n",
       "      <td>13.992094</td>\n",
       "      <td>12.260070</td>\n",
       "      <td>12.513580</td>\n",
       "      <td>12.256111</td>\n",
       "      <td>14.934873</td>\n",
       "      <td>11.807985</td>\n",
       "      <td>...</td>\n",
       "      <td>1.805982</td>\n",
       "      <td>2.657117</td>\n",
       "      <td>8.319195401</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>61</td>\n",
       "      <td>0</td>\n",
       "      <td>14.124827</td>\n",
       "      <td>15.667980</td>\n",
       "      <td>13.885415</td>\n",
       "      <td>12.434999</td>\n",
       "      <td>12.962366</td>\n",
       "      <td>12.152578</td>\n",
       "      <td>14.616602</td>\n",
       "      <td>11.767767</td>\n",
       "      <td>...</td>\n",
       "      <td>2.258414</td>\n",
       "      <td>2.491088</td>\n",
       "      <td>8.240600446</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>62</td>\n",
       "      <td>0</td>\n",
       "      <td>14.125212</td>\n",
       "      <td>15.848679</td>\n",
       "      <td>14.611221</td>\n",
       "      <td>12.768474</td>\n",
       "      <td>12.380620</td>\n",
       "      <td>12.660110</td>\n",
       "      <td>14.852526</td>\n",
       "      <td>12.026815</td>\n",
       "      <td>...</td>\n",
       "      <td>1.683385</td>\n",
       "      <td>2.430159</td>\n",
       "      <td>8.36950741</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>63</td>\n",
       "      <td>0</td>\n",
       "      <td>13.911841</td>\n",
       "      <td>15.770673</td>\n",
       "      <td>13.616139</td>\n",
       "      <td>12.200233</td>\n",
       "      <td>13.897249</td>\n",
       "      <td>12.203694</td>\n",
       "      <td>14.965501</td>\n",
       "      <td>11.984423</td>\n",
       "      <td>...</td>\n",
       "      <td>3.227728</td>\n",
       "      <td>2.456011</td>\n",
       "      <td>8.212264012</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>66</td>\n",
       "      <td>0</td>\n",
       "      <td>13.905019</td>\n",
       "      <td>16.145448</td>\n",
       "      <td>13.694255</td>\n",
       "      <td>12.297953</td>\n",
       "      <td>13.897956</td>\n",
       "      <td>12.847235</td>\n",
       "      <td>14.765581</td>\n",
       "      <td>11.922889</td>\n",
       "      <td>...</td>\n",
       "      <td>3.111999</td>\n",
       "      <td>2.169131</td>\n",
       "      <td>8.390142143</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>67</td>\n",
       "      <td>0</td>\n",
       "      <td>13.587752</td>\n",
       "      <td>15.693374</td>\n",
       "      <td>13.750024</td>\n",
       "      <td>12.287812</td>\n",
       "      <td>13.601792</td>\n",
       "      <td>12.299811</td>\n",
       "      <td>15.022294</td>\n",
       "      <td>11.774581</td>\n",
       "      <td>...</td>\n",
       "      <td>2.871030</td>\n",
       "      <td>2.530671</td>\n",
       "      <td>8.470634543</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>68</td>\n",
       "      <td>0</td>\n",
       "      <td>14.187646</td>\n",
       "      <td>15.470626</td>\n",
       "      <td>13.781765</td>\n",
       "      <td>12.357138</td>\n",
       "      <td>14.225409</td>\n",
       "      <td>14.940561</td>\n",
       "      <td>14.684765</td>\n",
       "      <td>11.810785</td>\n",
       "      <td>...</td>\n",
       "      <td>2.047146</td>\n",
       "      <td>2.173930</td>\n",
       "      <td>8.38220495</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>69</td>\n",
       "      <td>0</td>\n",
       "      <td>14.094374</td>\n",
       "      <td>15.391283</td>\n",
       "      <td>13.156052</td>\n",
       "      <td>12.299693</td>\n",
       "      <td>12.538918</td>\n",
       "      <td>12.572195</td>\n",
       "      <td>14.731909</td>\n",
       "      <td>11.818707</td>\n",
       "      <td>...</td>\n",
       "      <td>1.879761</td>\n",
       "      <td>2.061995</td>\n",
       "      <td>8.421292649</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>70</td>\n",
       "      <td>0</td>\n",
       "      <td>13.887737</td>\n",
       "      <td>15.651142</td>\n",
       "      <td>13.677427</td>\n",
       "      <td>12.106190</td>\n",
       "      <td>12.443795</td>\n",
       "      <td>12.495065</td>\n",
       "      <td>15.046157</td>\n",
       "      <td>11.758245</td>\n",
       "      <td>...</td>\n",
       "      <td>1.847846</td>\n",
       "      <td>2.536259</td>\n",
       "      <td>8.222391052</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>71</td>\n",
       "      <td>0</td>\n",
       "      <td>14.229909</td>\n",
       "      <td>15.692926</td>\n",
       "      <td>14.235475</td>\n",
       "      <td>12.749272</td>\n",
       "      <td>12.978876</td>\n",
       "      <td>11.558824</td>\n",
       "      <td>14.839368</td>\n",
       "      <td>11.590611</td>\n",
       "      <td>...</td>\n",
       "      <td>2.321552</td>\n",
       "      <td>2.765492</td>\n",
       "      <td>8.528618926</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>72</td>\n",
       "      <td>0</td>\n",
       "      <td>13.470893</td>\n",
       "      <td>15.776606</td>\n",
       "      <td>12.803612</td>\n",
       "      <td>11.874306</td>\n",
       "      <td>13.856436</td>\n",
       "      <td>11.851316</td>\n",
       "      <td>14.723356</td>\n",
       "      <td>11.897736</td>\n",
       "      <td>...</td>\n",
       "      <td>3.178995</td>\n",
       "      <td>2.031551</td>\n",
       "      <td>8.339872357</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>73</td>\n",
       "      <td>0</td>\n",
       "      <td>13.791416</td>\n",
       "      <td>15.660444</td>\n",
       "      <td>12.416035</td>\n",
       "      <td>11.807934</td>\n",
       "      <td>14.847563</td>\n",
       "      <td>12.695139</td>\n",
       "      <td>14.803905</td>\n",
       "      <td>11.960705</td>\n",
       "      <td>...</td>\n",
       "      <td>3.772191</td>\n",
       "      <td>2.185031</td>\n",
       "      <td>8.260333046</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>75</td>\n",
       "      <td>0</td>\n",
       "      <td>13.811276</td>\n",
       "      <td>15.877942</td>\n",
       "      <td>13.655066</td>\n",
       "      <td>12.005657</td>\n",
       "      <td>13.359262</td>\n",
       "      <td>12.686008</td>\n",
       "      <td>14.837980</td>\n",
       "      <td>12.442368</td>\n",
       "      <td>...</td>\n",
       "      <td>2.656716</td>\n",
       "      <td>2.359158</td>\n",
       "      <td>8.223552495</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>78</td>\n",
       "      <td>0</td>\n",
       "      <td>13.408338</td>\n",
       "      <td>15.825618</td>\n",
       "      <td>13.032500</td>\n",
       "      <td>12.104698</td>\n",
       "      <td>13.393705</td>\n",
       "      <td>12.255903</td>\n",
       "      <td>14.912439</td>\n",
       "      <td>12.352381</td>\n",
       "      <td>...</td>\n",
       "      <td>2.625084</td>\n",
       "      <td>2.334525</td>\n",
       "      <td>8.273687644</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>79</td>\n",
       "      <td>0</td>\n",
       "      <td>13.626238</td>\n",
       "      <td>15.683295</td>\n",
       "      <td>13.745549</td>\n",
       "      <td>12.419539</td>\n",
       "      <td>12.659798</td>\n",
       "      <td>12.046584</td>\n",
       "      <td>14.929399</td>\n",
       "      <td>12.618666</td>\n",
       "      <td>...</td>\n",
       "      <td>2.050419</td>\n",
       "      <td>2.167668</td>\n",
       "      <td>8.489779286</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>80</td>\n",
       "      <td>0</td>\n",
       "      <td>13.732388</td>\n",
       "      <td>15.614327</td>\n",
       "      <td>13.550485</td>\n",
       "      <td>12.214790</td>\n",
       "      <td>12.295163</td>\n",
       "      <td>11.849885</td>\n",
       "      <td>14.638246</td>\n",
       "      <td>12.493920</td>\n",
       "      <td>...</td>\n",
       "      <td>1.587524</td>\n",
       "      <td>2.198980</td>\n",
       "      <td>8.301009626</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>82</td>\n",
       "      <td>0</td>\n",
       "      <td>13.434239</td>\n",
       "      <td>15.599394</td>\n",
       "      <td>12.937875</td>\n",
       "      <td>11.949690</td>\n",
       "      <td>14.685660</td>\n",
       "      <td>12.535203</td>\n",
       "      <td>14.994927</td>\n",
       "      <td>12.625845</td>\n",
       "      <td>...</td>\n",
       "      <td>3.662519</td>\n",
       "      <td>2.141213</td>\n",
       "      <td>8.289687552</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>83</td>\n",
       "      <td>0</td>\n",
       "      <td>14.095026</td>\n",
       "      <td>15.480027</td>\n",
       "      <td>13.737106</td>\n",
       "      <td>12.276233</td>\n",
       "      <td>13.373069</td>\n",
       "      <td>12.241929</td>\n",
       "      <td>14.765373</td>\n",
       "      <td>12.465197</td>\n",
       "      <td>...</td>\n",
       "      <td>2.678659</td>\n",
       "      <td>2.204938</td>\n",
       "      <td>8.636030152</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>84</td>\n",
       "      <td>0</td>\n",
       "      <td>13.948679</td>\n",
       "      <td>15.707015</td>\n",
       "      <td>14.538745</td>\n",
       "      <td>12.775609</td>\n",
       "      <td>13.379634</td>\n",
       "      <td>12.260513</td>\n",
       "      <td>14.727783</td>\n",
       "      <td>12.237720</td>\n",
       "      <td>...</td>\n",
       "      <td>2.675655</td>\n",
       "      <td>2.335937</td>\n",
       "      <td>8.484635078</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>85</td>\n",
       "      <td>0</td>\n",
       "      <td>14.298264</td>\n",
       "      <td>15.355092</td>\n",
       "      <td>14.103120</td>\n",
       "      <td>12.766971</td>\n",
       "      <td>12.750475</td>\n",
       "      <td>11.651945</td>\n",
       "      <td>14.502873</td>\n",
       "      <td>12.194344</td>\n",
       "      <td>...</td>\n",
       "      <td>2.027413</td>\n",
       "      <td>2.646200</td>\n",
       "      <td>8.288551441</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>86</td>\n",
       "      <td>0</td>\n",
       "      <td>14.130082</td>\n",
       "      <td>15.892906</td>\n",
       "      <td>13.303000</td>\n",
       "      <td>12.037620</td>\n",
       "      <td>12.079807</td>\n",
       "      <td>12.701836</td>\n",
       "      <td>14.945113</td>\n",
       "      <td>12.122186</td>\n",
       "      <td>...</td>\n",
       "      <td>1.347397</td>\n",
       "      <td>2.130309</td>\n",
       "      <td>8.263915892</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>34</td>\n",
       "      <td>1</td>\n",
       "      <td>14.074076</td>\n",
       "      <td>15.550370</td>\n",
       "      <td>14.360856</td>\n",
       "      <td>12.789551</td>\n",
       "      <td>12.534986</td>\n",
       "      <td>13.780358</td>\n",
       "      <td>14.695148</td>\n",
       "      <td>11.888787</td>\n",
       "      <td>...</td>\n",
       "      <td>1.628613</td>\n",
       "      <td>2.728490</td>\n",
       "      <td>8.435363069</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>36</td>\n",
       "      <td>1</td>\n",
       "      <td>13.608710</td>\n",
       "      <td>15.620534</td>\n",
       "      <td>13.926261</td>\n",
       "      <td>11.985488</td>\n",
       "      <td>13.138026</td>\n",
       "      <td>13.473093</td>\n",
       "      <td>14.805259</td>\n",
       "      <td>12.167358</td>\n",
       "      <td>...</td>\n",
       "      <td>2.296785</td>\n",
       "      <td>2.157262</td>\n",
       "      <td>8.497522047</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>37</td>\n",
       "      <td>1</td>\n",
       "      <td>13.912395</td>\n",
       "      <td>15.710430</td>\n",
       "      <td>12.546500</td>\n",
       "      <td>11.658677</td>\n",
       "      <td>12.798666</td>\n",
       "      <td>12.761682</td>\n",
       "      <td>14.730217</td>\n",
       "      <td>12.118298</td>\n",
       "      <td>...</td>\n",
       "      <td>1.977932</td>\n",
       "      <td>2.284340</td>\n",
       "      <td>8.44909701</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>39</td>\n",
       "      <td>1</td>\n",
       "      <td>14.272681</td>\n",
       "      <td>15.398227</td>\n",
       "      <td>13.917771</td>\n",
       "      <td>12.557395</td>\n",
       "      <td>13.113209</td>\n",
       "      <td>13.035004</td>\n",
       "      <td>14.700396</td>\n",
       "      <td>12.477438</td>\n",
       "      <td>...</td>\n",
       "      <td>2.358397</td>\n",
       "      <td>2.501465</td>\n",
       "      <td>8.431409302</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>40</td>\n",
       "      <td>1</td>\n",
       "      <td>13.553682</td>\n",
       "      <td>15.332108</td>\n",
       "      <td>12.975258</td>\n",
       "      <td>12.172884</td>\n",
       "      <td>13.984951</td>\n",
       "      <td>12.718311</td>\n",
       "      <td>14.723451</td>\n",
       "      <td>11.626854</td>\n",
       "      <td>...</td>\n",
       "      <td>3.179794</td>\n",
       "      <td>1.858448</td>\n",
       "      <td>8.375819933</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>41</td>\n",
       "      <td>1</td>\n",
       "      <td>13.503647</td>\n",
       "      <td>15.798457</td>\n",
       "      <td>13.363521</td>\n",
       "      <td>12.329034</td>\n",
       "      <td>12.712400</td>\n",
       "      <td>13.083894</td>\n",
       "      <td>14.792074</td>\n",
       "      <td>12.206497</td>\n",
       "      <td>...</td>\n",
       "      <td>1.908500</td>\n",
       "      <td>2.339160</td>\n",
       "      <td>8.278816188</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>42</td>\n",
       "      <td>1</td>\n",
       "      <td>13.185622</td>\n",
       "      <td>15.649060</td>\n",
       "      <td>13.128470</td>\n",
       "      <td>12.195693</td>\n",
       "      <td>14.377665</td>\n",
       "      <td>15.325704</td>\n",
       "      <td>14.478469</td>\n",
       "      <td>12.226616</td>\n",
       "      <td>...</td>\n",
       "      <td>3.341021</td>\n",
       "      <td>1.845781</td>\n",
       "      <td>8.399841476</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>43</td>\n",
       "      <td>1</td>\n",
       "      <td>13.855509</td>\n",
       "      <td>15.827115</td>\n",
       "      <td>13.265267</td>\n",
       "      <td>11.844197</td>\n",
       "      <td>15.126767</td>\n",
       "      <td>17.093881</td>\n",
       "      <td>15.067586</td>\n",
       "      <td>12.164017</td>\n",
       "      <td>...</td>\n",
       "      <td>2.673680</td>\n",
       "      <td>2.548548</td>\n",
       "      <td>8.398240299</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>44</td>\n",
       "      <td>1</td>\n",
       "      <td>13.542743</td>\n",
       "      <td>15.697666</td>\n",
       "      <td>14.144507</td>\n",
       "      <td>12.567397</td>\n",
       "      <td>12.971915</td>\n",
       "      <td>14.868657</td>\n",
       "      <td>14.950939</td>\n",
       "      <td>12.337996</td>\n",
       "      <td>...</td>\n",
       "      <td>1.865180</td>\n",
       "      <td>2.135729</td>\n",
       "      <td>8.237084206</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>45</td>\n",
       "      <td>1</td>\n",
       "      <td>13.995393</td>\n",
       "      <td>15.763045</td>\n",
       "      <td>13.422012</td>\n",
       "      <td>12.303995</td>\n",
       "      <td>13.350982</td>\n",
       "      <td>13.343193</td>\n",
       "      <td>14.944149</td>\n",
       "      <td>12.346532</td>\n",
       "      <td>...</td>\n",
       "      <td>2.551983</td>\n",
       "      <td>2.417319</td>\n",
       "      <td>8.467315869</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>46</td>\n",
       "      <td>1</td>\n",
       "      <td>13.526921</td>\n",
       "      <td>15.583914</td>\n",
       "      <td>15.236153</td>\n",
       "      <td>13.343259</td>\n",
       "      <td>14.652109</td>\n",
       "      <td>14.664571</td>\n",
       "      <td>14.872883</td>\n",
       "      <td>12.148834</td>\n",
       "      <td>...</td>\n",
       "      <td>3.679456</td>\n",
       "      <td>2.722867</td>\n",
       "      <td>8.486501487</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>47</td>\n",
       "      <td>1</td>\n",
       "      <td>14.098327</td>\n",
       "      <td>15.572972</td>\n",
       "      <td>13.839735</td>\n",
       "      <td>12.908054</td>\n",
       "      <td>12.830078</td>\n",
       "      <td>14.253377</td>\n",
       "      <td>14.688208</td>\n",
       "      <td>12.091903</td>\n",
       "      <td>...</td>\n",
       "      <td>1.250891</td>\n",
       "      <td>2.501105</td>\n",
       "      <td>8.518012162</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>48</td>\n",
       "      <td>1</td>\n",
       "      <td>14.237930</td>\n",
       "      <td>15.678887</td>\n",
       "      <td>13.976978</td>\n",
       "      <td>12.430527</td>\n",
       "      <td>12.674354</td>\n",
       "      <td>13.679608</td>\n",
       "      <td>14.850197</td>\n",
       "      <td>11.542836</td>\n",
       "      <td>...</td>\n",
       "      <td>1.756122</td>\n",
       "      <td>2.449833</td>\n",
       "      <td>8.363991765</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>49</td>\n",
       "      <td>1</td>\n",
       "      <td>13.970157</td>\n",
       "      <td>16.027721</td>\n",
       "      <td>14.244315</td>\n",
       "      <td>13.328620</td>\n",
       "      <td>13.067405</td>\n",
       "      <td>15.250113</td>\n",
       "      <td>14.968732</td>\n",
       "      <td>11.894993</td>\n",
       "      <td>...</td>\n",
       "      <td>1.745003</td>\n",
       "      <td>2.890356</td>\n",
       "      <td>8.400513643</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>53</td>\n",
       "      <td>1</td>\n",
       "      <td>13.794663</td>\n",
       "      <td>15.445801</td>\n",
       "      <td>13.475055</td>\n",
       "      <td>12.279256</td>\n",
       "      <td>13.653491</td>\n",
       "      <td>13.258665</td>\n",
       "      <td>14.796514</td>\n",
       "      <td>11.680741</td>\n",
       "      <td>...</td>\n",
       "      <td>2.840020</td>\n",
       "      <td>2.767087</td>\n",
       "      <td>8.47464557</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>55</td>\n",
       "      <td>1</td>\n",
       "      <td>14.103663</td>\n",
       "      <td>15.568095</td>\n",
       "      <td>13.472755</td>\n",
       "      <td>12.202148</td>\n",
       "      <td>12.950661</td>\n",
       "      <td>14.140104</td>\n",
       "      <td>14.573157</td>\n",
       "      <td>11.537200</td>\n",
       "      <td>...</td>\n",
       "      <td>1.777345</td>\n",
       "      <td>2.373036</td>\n",
       "      <td>8.395026425</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>64</td>\n",
       "      <td>1</td>\n",
       "      <td>13.720616</td>\n",
       "      <td>15.587086</td>\n",
       "      <td>13.224181</td>\n",
       "      <td>11.946611</td>\n",
       "      <td>12.845243</td>\n",
       "      <td>13.753623</td>\n",
       "      <td>14.738364</td>\n",
       "      <td>11.998374</td>\n",
       "      <td>...</td>\n",
       "      <td>1.822230</td>\n",
       "      <td>2.099936</td>\n",
       "      <td>8.476513526</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>65</td>\n",
       "      <td>1</td>\n",
       "      <td>13.336426</td>\n",
       "      <td>15.462282</td>\n",
       "      <td>13.441044</td>\n",
       "      <td>12.612899</td>\n",
       "      <td>14.526108</td>\n",
       "      <td>13.405522</td>\n",
       "      <td>14.843443</td>\n",
       "      <td>11.952914</td>\n",
       "      <td>...</td>\n",
       "      <td>3.595970</td>\n",
       "      <td>2.855985</td>\n",
       "      <td>8.215932421</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>76</td>\n",
       "      <td>1</td>\n",
       "      <td>13.712649</td>\n",
       "      <td>15.710702</td>\n",
       "      <td>13.450497</td>\n",
       "      <td>12.085055</td>\n",
       "      <td>13.107520</td>\n",
       "      <td>13.235522</td>\n",
       "      <td>14.892700</td>\n",
       "      <td>12.352521</td>\n",
       "      <td>...</td>\n",
       "      <td>2.106706</td>\n",
       "      <td>2.409483</td>\n",
       "      <td>8.38030435</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>77</td>\n",
       "      <td>1</td>\n",
       "      <td>14.136347</td>\n",
       "      <td>15.646664</td>\n",
       "      <td>13.707184</td>\n",
       "      <td>12.397715</td>\n",
       "      <td>12.885362</td>\n",
       "      <td>12.590245</td>\n",
       "      <td>14.732140</td>\n",
       "      <td>12.206637</td>\n",
       "      <td>...</td>\n",
       "      <td>2.070683</td>\n",
       "      <td>2.418890</td>\n",
       "      <td>8.449288316</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>81</td>\n",
       "      <td>1</td>\n",
       "      <td>13.877760</td>\n",
       "      <td>15.865054</td>\n",
       "      <td>13.652736</td>\n",
       "      <td>11.859613</td>\n",
       "      <td>12.580436</td>\n",
       "      <td>13.614601</td>\n",
       "      <td>14.953066</td>\n",
       "      <td>12.419737</td>\n",
       "      <td>...</td>\n",
       "      <td>1.688147</td>\n",
       "      <td>2.160634</td>\n",
       "      <td>8.381040067</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>88</td>\n",
       "      <td>1</td>\n",
       "      <td>13.101075</td>\n",
       "      <td>15.563113</td>\n",
       "      <td>14.134044</td>\n",
       "      <td>12.751887</td>\n",
       "      <td>14.329279</td>\n",
       "      <td>13.317395</td>\n",
       "      <td>14.881287</td>\n",
       "      <td>12.050647</td>\n",
       "      <td>...</td>\n",
       "      <td>3.425897</td>\n",
       "      <td>2.577070</td>\n",
       "      <td>8.33004216</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>89</td>\n",
       "      <td>1</td>\n",
       "      <td>13.699883</td>\n",
       "      <td>15.633531</td>\n",
       "      <td>13.078100</td>\n",
       "      <td>11.848265</td>\n",
       "      <td>14.006595</td>\n",
       "      <td>12.877403</td>\n",
       "      <td>14.893908</td>\n",
       "      <td>12.019573</td>\n",
       "      <td>...</td>\n",
       "      <td>3.208075</td>\n",
       "      <td>2.329893</td>\n",
       "      <td>8.449347238</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>98</td>\n",
       "      <td>1</td>\n",
       "      <td>13.769740</td>\n",
       "      <td>15.617562</td>\n",
       "      <td>13.983691</td>\n",
       "      <td>12.404641</td>\n",
       "      <td>14.108590</td>\n",
       "      <td>13.666401</td>\n",
       "      <td>14.984438</td>\n",
       "      <td>12.114248</td>\n",
       "      <td>...</td>\n",
       "      <td>3.288916</td>\n",
       "      <td>2.731427</td>\n",
       "      <td>8.495184688</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>99</td>\n",
       "      <td>1</td>\n",
       "      <td>13.881991</td>\n",
       "      <td>15.634942</td>\n",
       "      <td>12.687261</td>\n",
       "      <td>11.649048</td>\n",
       "      <td>12.839689</td>\n",
       "      <td>12.596961</td>\n",
       "      <td>14.787885</td>\n",
       "      <td>11.974167</td>\n",
       "      <td>...</td>\n",
       "      <td>2.093669</td>\n",
       "      <td>2.238217</td>\n",
       "      <td>8.352756627</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>100</td>\n",
       "      <td>1</td>\n",
       "      <td>13.911391</td>\n",
       "      <td>15.808535</td>\n",
       "      <td>13.255651</td>\n",
       "      <td>12.072060</td>\n",
       "      <td>12.829308</td>\n",
       "      <td>12.518926</td>\n",
       "      <td>14.914205</td>\n",
       "      <td>12.009783</td>\n",
       "      <td>...</td>\n",
       "      <td>2.099744</td>\n",
       "      <td>2.460060</td>\n",
       "      <td>8.268259167</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>101</td>\n",
       "      <td>1</td>\n",
       "      <td>14.004190</td>\n",
       "      <td>15.578057</td>\n",
       "      <td>15.050097</td>\n",
       "      <td>12.955338</td>\n",
       "      <td>12.021058</td>\n",
       "      <td>12.698247</td>\n",
       "      <td>14.838858</td>\n",
       "      <td>11.864216</td>\n",
       "      <td>...</td>\n",
       "      <td>1.215855</td>\n",
       "      <td>2.716850</td>\n",
       "      <td>8.382160574</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>102</td>\n",
       "      <td>1</td>\n",
       "      <td>13.782098</td>\n",
       "      <td>15.628488</td>\n",
       "      <td>13.800509</td>\n",
       "      <td>12.324283</td>\n",
       "      <td>13.557521</td>\n",
       "      <td>13.930284</td>\n",
       "      <td>15.010427</td>\n",
       "      <td>11.877001</td>\n",
       "      <td>...</td>\n",
       "      <td>2.567097</td>\n",
       "      <td>2.339410</td>\n",
       "      <td>8.362964723</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>103</td>\n",
       "      <td>1</td>\n",
       "      <td>13.841570</td>\n",
       "      <td>15.697214</td>\n",
       "      <td>14.467861</td>\n",
       "      <td>12.549533</td>\n",
       "      <td>13.157717</td>\n",
       "      <td>14.309882</td>\n",
       "      <td>14.518098</td>\n",
       "      <td>12.173317</td>\n",
       "      <td>...</td>\n",
       "      <td>1.637428</td>\n",
       "      <td>2.147184</td>\n",
       "      <td>8.379107727</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>104</td>\n",
       "      <td>1</td>\n",
       "      <td>13.621706</td>\n",
       "      <td>15.483609</td>\n",
       "      <td>14.060104</td>\n",
       "      <td>12.237578</td>\n",
       "      <td>13.847064</td>\n",
       "      <td>13.140827</td>\n",
       "      <td>14.923120</td>\n",
       "      <td>12.129161</td>\n",
       "      <td>...</td>\n",
       "      <td>3.024477</td>\n",
       "      <td>2.641430</td>\n",
       "      <td>8.477305855\"\"\"\"\"\"\"\"</td>\n",
       "      <td>\"\"\"\"\"\"\"\"NA\"\"\"\"\"\"\"\"</td>\n",
       "      <td>\"\"\"\"\"\"\"\"NA\"\"\"\"\"\"\"\"</td>\n",
       "      <td>\"\"\"\"\"\"\"\"NA\"\"\"\"\"\"\"\"</td>\n",
       "      <td>\"\"\"\"\"\"\"\"NA\"\"\"\"\"\"\"\"</td>\n",
       "      <td>\"\"\"\"\"\"\"\"NA\"\"\"\"\"\"\"\"</td>\n",
       "      <td>\"\"\"\"\"\"\"\"NA\"\"\"\"\"\"\"\"</td>\n",
       "      <td>\"\"\"\"\"\"\"\"0\"\"\"\"\"\"\"\"</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>93 rows × 133 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Unnamed: 0  Label   Pyruvate  MethylSuccinate  Phosphoglyceric Acid  \\\n",
       "0           20      0  14.158545        15.624522             13.383600   \n",
       "1           24      0  14.180287        15.325665             13.468363   \n",
       "2           29      0  13.921329        16.156867             14.615061   \n",
       "3           52      0  13.785605        15.939535             13.206615   \n",
       "4           54      0  13.674234        15.743668             11.542008   \n",
       "5           56      0  13.447093        15.595200             13.118302   \n",
       "6           57      0  13.616625        15.406841             14.124967   \n",
       "7           58      0  13.731086        15.449381             13.108050   \n",
       "8           59      0  14.468678        15.438259             13.737836   \n",
       "9           60      0  13.589152        15.895696             13.992094   \n",
       "10          61      0  14.124827        15.667980             13.885415   \n",
       "11          62      0  14.125212        15.848679             14.611221   \n",
       "12          63      0  13.911841        15.770673             13.616139   \n",
       "13          66      0  13.905019        16.145448             13.694255   \n",
       "14          67      0  13.587752        15.693374             13.750024   \n",
       "15          68      0  14.187646        15.470626             13.781765   \n",
       "16          69      0  14.094374        15.391283             13.156052   \n",
       "17          70      0  13.887737        15.651142             13.677427   \n",
       "18          71      0  14.229909        15.692926             14.235475   \n",
       "19          72      0  13.470893        15.776606             12.803612   \n",
       "20          73      0  13.791416        15.660444             12.416035   \n",
       "21          75      0  13.811276        15.877942             13.655066   \n",
       "22          78      0  13.408338        15.825618             13.032500   \n",
       "23          79      0  13.626238        15.683295             13.745549   \n",
       "24          80      0  13.732388        15.614327             13.550485   \n",
       "25          82      0  13.434239        15.599394             12.937875   \n",
       "26          83      0  14.095026        15.480027             13.737106   \n",
       "27          84      0  13.948679        15.707015             14.538745   \n",
       "28          85      0  14.298264        15.355092             14.103120   \n",
       "29          86      0  14.130082        15.892906             13.303000   \n",
       "..         ...    ...        ...              ...                   ...   \n",
       "63          34      1  14.074076        15.550370             14.360856   \n",
       "64          36      1  13.608710        15.620534             13.926261   \n",
       "65          37      1  13.912395        15.710430             12.546500   \n",
       "66          39      1  14.272681        15.398227             13.917771   \n",
       "67          40      1  13.553682        15.332108             12.975258   \n",
       "68          41      1  13.503647        15.798457             13.363521   \n",
       "69          42      1  13.185622        15.649060             13.128470   \n",
       "70          43      1  13.855509        15.827115             13.265267   \n",
       "71          44      1  13.542743        15.697666             14.144507   \n",
       "72          45      1  13.995393        15.763045             13.422012   \n",
       "73          46      1  13.526921        15.583914             15.236153   \n",
       "74          47      1  14.098327        15.572972             13.839735   \n",
       "75          48      1  14.237930        15.678887             13.976978   \n",
       "76          49      1  13.970157        16.027721             14.244315   \n",
       "77          53      1  13.794663        15.445801             13.475055   \n",
       "78          55      1  14.103663        15.568095             13.472755   \n",
       "79          64      1  13.720616        15.587086             13.224181   \n",
       "80          65      1  13.336426        15.462282             13.441044   \n",
       "81          76      1  13.712649        15.710702             13.450497   \n",
       "82          77      1  14.136347        15.646664             13.707184   \n",
       "83          81      1  13.877760        15.865054             13.652736   \n",
       "84          88      1  13.101075        15.563113             14.134044   \n",
       "85          89      1  13.699883        15.633531             13.078100   \n",
       "86          98      1  13.769740        15.617562             13.983691   \n",
       "87          99      1  13.881991        15.634942             12.687261   \n",
       "88         100      1  13.911391        15.808535             13.255651   \n",
       "89         101      1  14.004190        15.578057             15.050097   \n",
       "90         102      1  13.782098        15.628488             13.800509   \n",
       "91         103      1  13.841570        15.697214             14.467861   \n",
       "92         104      1  13.621706        15.483609             14.060104   \n",
       "\n",
       "    Glucose 1-phosphate  Malonic Acid  Fumaric Acid  Alpha-Ketoglutaric Acid  \\\n",
       "0             11.923638     13.933560     12.787696                14.981440   \n",
       "1             12.046294     13.043221     12.536227                14.593418   \n",
       "2             13.051929     13.381985     12.419600                14.701978   \n",
       "3             12.342156     12.448068     12.211098                14.860913   \n",
       "4             11.807872     14.582068     11.635154                14.970451   \n",
       "5             12.002027     12.817962     12.065246                14.796780   \n",
       "6             12.499758     13.168693     12.235455                14.721283   \n",
       "7             11.948349     14.492458     12.008819                14.727796   \n",
       "8             12.204044     12.950250     12.466876                14.826535   \n",
       "9             12.260070     12.513580     12.256111                14.934873   \n",
       "10            12.434999     12.962366     12.152578                14.616602   \n",
       "11            12.768474     12.380620     12.660110                14.852526   \n",
       "12            12.200233     13.897249     12.203694                14.965501   \n",
       "13            12.297953     13.897956     12.847235                14.765581   \n",
       "14            12.287812     13.601792     12.299811                15.022294   \n",
       "15            12.357138     14.225409     14.940561                14.684765   \n",
       "16            12.299693     12.538918     12.572195                14.731909   \n",
       "17            12.106190     12.443795     12.495065                15.046157   \n",
       "18            12.749272     12.978876     11.558824                14.839368   \n",
       "19            11.874306     13.856436     11.851316                14.723356   \n",
       "20            11.807934     14.847563     12.695139                14.803905   \n",
       "21            12.005657     13.359262     12.686008                14.837980   \n",
       "22            12.104698     13.393705     12.255903                14.912439   \n",
       "23            12.419539     12.659798     12.046584                14.929399   \n",
       "24            12.214790     12.295163     11.849885                14.638246   \n",
       "25            11.949690     14.685660     12.535203                14.994927   \n",
       "26            12.276233     13.373069     12.241929                14.765373   \n",
       "27            12.775609     13.379634     12.260513                14.727783   \n",
       "28            12.766971     12.750475     11.651945                14.502873   \n",
       "29            12.037620     12.079807     12.701836                14.945113   \n",
       "..                  ...           ...           ...                      ...   \n",
       "63            12.789551     12.534986     13.780358                14.695148   \n",
       "64            11.985488     13.138026     13.473093                14.805259   \n",
       "65            11.658677     12.798666     12.761682                14.730217   \n",
       "66            12.557395     13.113209     13.035004                14.700396   \n",
       "67            12.172884     13.984951     12.718311                14.723451   \n",
       "68            12.329034     12.712400     13.083894                14.792074   \n",
       "69            12.195693     14.377665     15.325704                14.478469   \n",
       "70            11.844197     15.126767     17.093881                15.067586   \n",
       "71            12.567397     12.971915     14.868657                14.950939   \n",
       "72            12.303995     13.350982     13.343193                14.944149   \n",
       "73            13.343259     14.652109     14.664571                14.872883   \n",
       "74            12.908054     12.830078     14.253377                14.688208   \n",
       "75            12.430527     12.674354     13.679608                14.850197   \n",
       "76            13.328620     13.067405     15.250113                14.968732   \n",
       "77            12.279256     13.653491     13.258665                14.796514   \n",
       "78            12.202148     12.950661     14.140104                14.573157   \n",
       "79            11.946611     12.845243     13.753623                14.738364   \n",
       "80            12.612899     14.526108     13.405522                14.843443   \n",
       "81            12.085055     13.107520     13.235522                14.892700   \n",
       "82            12.397715     12.885362     12.590245                14.732140   \n",
       "83            11.859613     12.580436     13.614601                14.953066   \n",
       "84            12.751887     14.329279     13.317395                14.881287   \n",
       "85            11.848265     14.006595     12.877403                14.893908   \n",
       "86            12.404641     14.108590     13.666401                14.984438   \n",
       "87            11.649048     12.839689     12.596961                14.787885   \n",
       "88            12.072060     12.829308     12.518926                14.914205   \n",
       "89            12.955338     12.021058     12.698247                14.838858   \n",
       "90            12.324283     13.557521     13.930284                15.010427   \n",
       "91            12.549533     13.157717     14.309882                14.518098   \n",
       "92            12.237578     13.847064     13.140827                14.923120   \n",
       "\n",
       "    Sarcosine  ...  3-Hydroxybenzoic acid  Succinate              Glucose  \\\n",
       "0   11.739484  ...               3.161390   2.165842          8.310291624   \n",
       "1   11.833211  ...               2.355910   2.260621          8.423733003   \n",
       "2   12.232070  ...               2.472365   2.295447          8.357070943   \n",
       "3   11.949448  ...               1.844567   2.278589          8.414503996   \n",
       "4   11.457569  ...               3.773262   2.136011          8.193979575   \n",
       "5   11.831409  ...               2.165502   2.439154          8.346804747   \n",
       "6   11.691816  ...               2.431075   2.396853          8.139349683   \n",
       "7   11.733394  ...               3.595748   2.325542          8.359634173   \n",
       "8   12.037085  ...               2.148865   2.286202          8.402214167   \n",
       "9   11.807985  ...               1.805982   2.657117          8.319195401   \n",
       "10  11.767767  ...               2.258414   2.491088          8.240600446   \n",
       "11  12.026815  ...               1.683385   2.430159           8.36950741   \n",
       "12  11.984423  ...               3.227728   2.456011          8.212264012   \n",
       "13  11.922889  ...               3.111999   2.169131          8.390142143   \n",
       "14  11.774581  ...               2.871030   2.530671          8.470634543   \n",
       "15  11.810785  ...               2.047146   2.173930           8.38220495   \n",
       "16  11.818707  ...               1.879761   2.061995          8.421292649   \n",
       "17  11.758245  ...               1.847846   2.536259          8.222391052   \n",
       "18  11.590611  ...               2.321552   2.765492          8.528618926   \n",
       "19  11.897736  ...               3.178995   2.031551          8.339872357   \n",
       "20  11.960705  ...               3.772191   2.185031          8.260333046   \n",
       "21  12.442368  ...               2.656716   2.359158          8.223552495   \n",
       "22  12.352381  ...               2.625084   2.334525          8.273687644   \n",
       "23  12.618666  ...               2.050419   2.167668          8.489779286   \n",
       "24  12.493920  ...               1.587524   2.198980          8.301009626   \n",
       "25  12.625845  ...               3.662519   2.141213          8.289687552   \n",
       "26  12.465197  ...               2.678659   2.204938          8.636030152   \n",
       "27  12.237720  ...               2.675655   2.335937          8.484635078   \n",
       "28  12.194344  ...               2.027413   2.646200          8.288551441   \n",
       "29  12.122186  ...               1.347397   2.130309          8.263915892   \n",
       "..        ...  ...                    ...        ...                  ...   \n",
       "63  11.888787  ...               1.628613   2.728490          8.435363069   \n",
       "64  12.167358  ...               2.296785   2.157262          8.497522047   \n",
       "65  12.118298  ...               1.977932   2.284340           8.44909701   \n",
       "66  12.477438  ...               2.358397   2.501465          8.431409302   \n",
       "67  11.626854  ...               3.179794   1.858448          8.375819933   \n",
       "68  12.206497  ...               1.908500   2.339160          8.278816188   \n",
       "69  12.226616  ...               3.341021   1.845781          8.399841476   \n",
       "70  12.164017  ...               2.673680   2.548548          8.398240299   \n",
       "71  12.337996  ...               1.865180   2.135729          8.237084206   \n",
       "72  12.346532  ...               2.551983   2.417319          8.467315869   \n",
       "73  12.148834  ...               3.679456   2.722867          8.486501487   \n",
       "74  12.091903  ...               1.250891   2.501105          8.518012162   \n",
       "75  11.542836  ...               1.756122   2.449833          8.363991765   \n",
       "76  11.894993  ...               1.745003   2.890356          8.400513643   \n",
       "77  11.680741  ...               2.840020   2.767087           8.47464557   \n",
       "78  11.537200  ...               1.777345   2.373036          8.395026425   \n",
       "79  11.998374  ...               1.822230   2.099936          8.476513526   \n",
       "80  11.952914  ...               3.595970   2.855985          8.215932421   \n",
       "81  12.352521  ...               2.106706   2.409483           8.38030435   \n",
       "82  12.206637  ...               2.070683   2.418890          8.449288316   \n",
       "83  12.419737  ...               1.688147   2.160634          8.381040067   \n",
       "84  12.050647  ...               3.425897   2.577070           8.33004216   \n",
       "85  12.019573  ...               3.208075   2.329893          8.449347238   \n",
       "86  12.114248  ...               3.288916   2.731427          8.495184688   \n",
       "87  11.974167  ...               2.093669   2.238217          8.352756627   \n",
       "88  12.009783  ...               2.099744   2.460060          8.268259167   \n",
       "89  11.864216  ...               1.215855   2.716850          8.382160574   \n",
       "90  11.877001  ...               2.567097   2.339410          8.362964723   \n",
       "91  12.173317  ...               1.637428   2.147184          8.379107727   \n",
       "92  12.129161  ...               3.024477   2.641430  8.477305855\"\"\"\"\"\"\"\"   \n",
       "\n",
       "                  V127                V128                V129  \\\n",
       "0                  NaN                 NaN                 NaN   \n",
       "1                  NaN                 NaN                 NaN   \n",
       "2                  NaN                 NaN                 NaN   \n",
       "3                  NaN                 NaN                 NaN   \n",
       "4                  NaN                 NaN                 NaN   \n",
       "5                  NaN                 NaN                 NaN   \n",
       "6                  NaN                 NaN                 NaN   \n",
       "7                  NaN                 NaN                 NaN   \n",
       "8                  NaN                 NaN                 NaN   \n",
       "9                  NaN                 NaN                 NaN   \n",
       "10                 NaN                 NaN                 NaN   \n",
       "11                 NaN                 NaN                 NaN   \n",
       "12                 NaN                 NaN                 NaN   \n",
       "13                 NaN                 NaN                 NaN   \n",
       "14                 NaN                 NaN                 NaN   \n",
       "15                 NaN                 NaN                 NaN   \n",
       "16                 NaN                 NaN                 NaN   \n",
       "17                 NaN                 NaN                 NaN   \n",
       "18                 NaN                 NaN                 NaN   \n",
       "19                 NaN                 NaN                 NaN   \n",
       "20                 NaN                 NaN                 NaN   \n",
       "21                 NaN                 NaN                 NaN   \n",
       "22                 NaN                 NaN                 NaN   \n",
       "23                 NaN                 NaN                 NaN   \n",
       "24                 NaN                 NaN                 NaN   \n",
       "25                 NaN                 NaN                 NaN   \n",
       "26                 NaN                 NaN                 NaN   \n",
       "27                 NaN                 NaN                 NaN   \n",
       "28                 NaN                 NaN                 NaN   \n",
       "29                 NaN                 NaN                 NaN   \n",
       "..                 ...                 ...                 ...   \n",
       "63                 NaN                 NaN                 NaN   \n",
       "64                 NaN                 NaN                 NaN   \n",
       "65                 NaN                 NaN                 NaN   \n",
       "66                 NaN                 NaN                 NaN   \n",
       "67                 NaN                 NaN                 NaN   \n",
       "68                 NaN                 NaN                 NaN   \n",
       "69                 NaN                 NaN                 NaN   \n",
       "70                 NaN                 NaN                 NaN   \n",
       "71                 NaN                 NaN                 NaN   \n",
       "72                 NaN                 NaN                 NaN   \n",
       "73                 NaN                 NaN                 NaN   \n",
       "74                 NaN                 NaN                 NaN   \n",
       "75                 NaN                 NaN                 NaN   \n",
       "76                 NaN                 NaN                 NaN   \n",
       "77                 NaN                 NaN                 NaN   \n",
       "78                 NaN                 NaN                 NaN   \n",
       "79                 NaN                 NaN                 NaN   \n",
       "80                 NaN                 NaN                 NaN   \n",
       "81                 NaN                 NaN                 NaN   \n",
       "82                 NaN                 NaN                 NaN   \n",
       "83                 NaN                 NaN                 NaN   \n",
       "84                 NaN                 NaN                 NaN   \n",
       "85                 NaN                 NaN                 NaN   \n",
       "86                 NaN                 NaN                 NaN   \n",
       "87                 NaN                 NaN                 NaN   \n",
       "88                 NaN                 NaN                 NaN   \n",
       "89                 NaN                 NaN                 NaN   \n",
       "90                 NaN                 NaN                 NaN   \n",
       "91                 NaN                 NaN                 NaN   \n",
       "92  \"\"\"\"\"\"\"\"NA\"\"\"\"\"\"\"\"  \"\"\"\"\"\"\"\"NA\"\"\"\"\"\"\"\"  \"\"\"\"\"\"\"\"NA\"\"\"\"\"\"\"\"   \n",
       "\n",
       "                  V130                V131                V132  \\\n",
       "0                  NaN                 NaN                 NaN   \n",
       "1                  NaN                 NaN                 NaN   \n",
       "2                  NaN                 NaN                 NaN   \n",
       "3                  NaN                 NaN                 NaN   \n",
       "4                  NaN                 NaN                 NaN   \n",
       "5                  NaN                 NaN                 NaN   \n",
       "6                  NaN                 NaN                 NaN   \n",
       "7                  NaN                 NaN                 NaN   \n",
       "8                  NaN                 NaN                 NaN   \n",
       "9                  NaN                 NaN                 NaN   \n",
       "10                 NaN                 NaN                 NaN   \n",
       "11                 NaN                 NaN                 NaN   \n",
       "12                 NaN                 NaN                 NaN   \n",
       "13                 NaN                 NaN                 NaN   \n",
       "14                 NaN                 NaN                 NaN   \n",
       "15                 NaN                 NaN                 NaN   \n",
       "16                 NaN                 NaN                 NaN   \n",
       "17                 NaN                 NaN                 NaN   \n",
       "18                 NaN                 NaN                 NaN   \n",
       "19                 NaN                 NaN                 NaN   \n",
       "20                 NaN                 NaN                 NaN   \n",
       "21                 NaN                 NaN                 NaN   \n",
       "22                 NaN                 NaN                 NaN   \n",
       "23                 NaN                 NaN                 NaN   \n",
       "24                 NaN                 NaN                 NaN   \n",
       "25                 NaN                 NaN                 NaN   \n",
       "26                 NaN                 NaN                 NaN   \n",
       "27                 NaN                 NaN                 NaN   \n",
       "28                 NaN                 NaN                 NaN   \n",
       "29                 NaN                 NaN                 NaN   \n",
       "..                 ...                 ...                 ...   \n",
       "63                 NaN                 NaN                 NaN   \n",
       "64                 NaN                 NaN                 NaN   \n",
       "65                 NaN                 NaN                 NaN   \n",
       "66                 NaN                 NaN                 NaN   \n",
       "67                 NaN                 NaN                 NaN   \n",
       "68                 NaN                 NaN                 NaN   \n",
       "69                 NaN                 NaN                 NaN   \n",
       "70                 NaN                 NaN                 NaN   \n",
       "71                 NaN                 NaN                 NaN   \n",
       "72                 NaN                 NaN                 NaN   \n",
       "73                 NaN                 NaN                 NaN   \n",
       "74                 NaN                 NaN                 NaN   \n",
       "75                 NaN                 NaN                 NaN   \n",
       "76                 NaN                 NaN                 NaN   \n",
       "77                 NaN                 NaN                 NaN   \n",
       "78                 NaN                 NaN                 NaN   \n",
       "79                 NaN                 NaN                 NaN   \n",
       "80                 NaN                 NaN                 NaN   \n",
       "81                 NaN                 NaN                 NaN   \n",
       "82                 NaN                 NaN                 NaN   \n",
       "83                 NaN                 NaN                 NaN   \n",
       "84                 NaN                 NaN                 NaN   \n",
       "85                 NaN                 NaN                 NaN   \n",
       "86                 NaN                 NaN                 NaN   \n",
       "87                 NaN                 NaN                 NaN   \n",
       "88                 NaN                 NaN                 NaN   \n",
       "89                 NaN                 NaN                 NaN   \n",
       "90                 NaN                 NaN                 NaN   \n",
       "91                 NaN                 NaN                 NaN   \n",
       "92  \"\"\"\"\"\"\"\"NA\"\"\"\"\"\"\"\"  \"\"\"\"\"\"\"\"NA\"\"\"\"\"\"\"\"  \"\"\"\"\"\"\"\"NA\"\"\"\"\"\"\"\"   \n",
       "\n",
       "                 V133  \n",
       "0                 NaN  \n",
       "1                 NaN  \n",
       "2                 NaN  \n",
       "3                 NaN  \n",
       "4                 NaN  \n",
       "5                 NaN  \n",
       "6                 NaN  \n",
       "7                 NaN  \n",
       "8                 NaN  \n",
       "9                 NaN  \n",
       "10                NaN  \n",
       "11                NaN  \n",
       "12                NaN  \n",
       "13                NaN  \n",
       "14                NaN  \n",
       "15                NaN  \n",
       "16                NaN  \n",
       "17                NaN  \n",
       "18                NaN  \n",
       "19                NaN  \n",
       "20                NaN  \n",
       "21                NaN  \n",
       "22                NaN  \n",
       "23                NaN  \n",
       "24                NaN  \n",
       "25                NaN  \n",
       "26                NaN  \n",
       "27                NaN  \n",
       "28                NaN  \n",
       "29                NaN  \n",
       "..                ...  \n",
       "63                NaN  \n",
       "64                NaN  \n",
       "65                NaN  \n",
       "66                NaN  \n",
       "67                NaN  \n",
       "68                NaN  \n",
       "69                NaN  \n",
       "70                NaN  \n",
       "71                NaN  \n",
       "72                NaN  \n",
       "73                NaN  \n",
       "74                NaN  \n",
       "75                NaN  \n",
       "76                NaN  \n",
       "77                NaN  \n",
       "78                NaN  \n",
       "79                NaN  \n",
       "80                NaN  \n",
       "81                NaN  \n",
       "82                NaN  \n",
       "83                NaN  \n",
       "84                NaN  \n",
       "85                NaN  \n",
       "86                NaN  \n",
       "87                NaN  \n",
       "88                NaN  \n",
       "89                NaN  \n",
       "90                NaN  \n",
       "91                NaN  \n",
       "92  \"\"\"\"\"\"\"\"0\"\"\"\"\"\"\"\"  \n",
       "\n",
       "[93 rows x 133 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##################################################################################################################\n",
    "#\n",
    "# YouDo:\n",
    "# \n",
    "#    1) Read the csv into a dataframe\n",
    "#######################################  BEGIN STUDENT CODE  #####################################################\n",
    "\n",
    "# 1) \n",
    "datafile = '/home/dir0417/Desktop/BIOS6644-master/Data/data_metabolomic.csv'\n",
    "df = pd.read_csv(datafile)\n",
    "\n",
    "#######################################   END STUDENT CODE   #####################################################\n",
    "\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>Pyruvate</th>\n",
       "      <th>MethylSuccinate</th>\n",
       "      <th>Phosphoglyceric Acid</th>\n",
       "      <th>Glucose 1-phosphate</th>\n",
       "      <th>Malonic Acid</th>\n",
       "      <th>Fumaric Acid</th>\n",
       "      <th>Alpha-Ketoglutaric Acid</th>\n",
       "      <th>Sarcosine</th>\n",
       "      <th>Cadaverine</th>\n",
       "      <th>...</th>\n",
       "      <th>3-Hydroxybenzoic acid</th>\n",
       "      <th>Succinate</th>\n",
       "      <th>Glucose</th>\n",
       "      <th>V127</th>\n",
       "      <th>V128</th>\n",
       "      <th>V129</th>\n",
       "      <th>V130</th>\n",
       "      <th>V131</th>\n",
       "      <th>V132</th>\n",
       "      <th>V133</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0</td>\n",
       "      <td>14.158545</td>\n",
       "      <td>15.624522</td>\n",
       "      <td>13.383600</td>\n",
       "      <td>11.923638</td>\n",
       "      <td>13.933560</td>\n",
       "      <td>12.787696</td>\n",
       "      <td>14.981440</td>\n",
       "      <td>11.739484</td>\n",
       "      <td>10.921776</td>\n",
       "      <td>...</td>\n",
       "      <td>3.161390</td>\n",
       "      <td>2.165842</td>\n",
       "      <td>8.310292</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0</td>\n",
       "      <td>14.180287</td>\n",
       "      <td>15.325665</td>\n",
       "      <td>13.468363</td>\n",
       "      <td>12.046294</td>\n",
       "      <td>13.043221</td>\n",
       "      <td>12.536227</td>\n",
       "      <td>14.593418</td>\n",
       "      <td>11.833211</td>\n",
       "      <td>11.121865</td>\n",
       "      <td>...</td>\n",
       "      <td>2.355910</td>\n",
       "      <td>2.260621</td>\n",
       "      <td>8.423733</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0</td>\n",
       "      <td>13.921329</td>\n",
       "      <td>16.156867</td>\n",
       "      <td>14.615061</td>\n",
       "      <td>13.051929</td>\n",
       "      <td>13.381985</td>\n",
       "      <td>12.419600</td>\n",
       "      <td>14.701978</td>\n",
       "      <td>12.232070</td>\n",
       "      <td>11.025333</td>\n",
       "      <td>...</td>\n",
       "      <td>2.472365</td>\n",
       "      <td>2.295447</td>\n",
       "      <td>8.357071</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>0</td>\n",
       "      <td>13.785605</td>\n",
       "      <td>15.939535</td>\n",
       "      <td>13.206615</td>\n",
       "      <td>12.342156</td>\n",
       "      <td>12.448068</td>\n",
       "      <td>12.211098</td>\n",
       "      <td>14.860913</td>\n",
       "      <td>11.949448</td>\n",
       "      <td>11.015657</td>\n",
       "      <td>...</td>\n",
       "      <td>1.844567</td>\n",
       "      <td>2.278589</td>\n",
       "      <td>8.414504</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>0</td>\n",
       "      <td>13.674234</td>\n",
       "      <td>15.743668</td>\n",
       "      <td>11.542008</td>\n",
       "      <td>11.807872</td>\n",
       "      <td>14.582068</td>\n",
       "      <td>11.635154</td>\n",
       "      <td>14.970451</td>\n",
       "      <td>11.457569</td>\n",
       "      <td>11.103239</td>\n",
       "      <td>...</td>\n",
       "      <td>3.773262</td>\n",
       "      <td>2.136011</td>\n",
       "      <td>8.193980</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>0</td>\n",
       "      <td>13.447093</td>\n",
       "      <td>15.595200</td>\n",
       "      <td>13.118302</td>\n",
       "      <td>12.002027</td>\n",
       "      <td>12.817962</td>\n",
       "      <td>12.065246</td>\n",
       "      <td>14.796780</td>\n",
       "      <td>11.831409</td>\n",
       "      <td>11.219337</td>\n",
       "      <td>...</td>\n",
       "      <td>2.165502</td>\n",
       "      <td>2.439154</td>\n",
       "      <td>8.346805</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>0</td>\n",
       "      <td>13.616625</td>\n",
       "      <td>15.406841</td>\n",
       "      <td>14.124967</td>\n",
       "      <td>12.499758</td>\n",
       "      <td>13.168693</td>\n",
       "      <td>12.235455</td>\n",
       "      <td>14.721283</td>\n",
       "      <td>11.691816</td>\n",
       "      <td>11.189055</td>\n",
       "      <td>...</td>\n",
       "      <td>2.431075</td>\n",
       "      <td>2.396853</td>\n",
       "      <td>8.139350</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>0</td>\n",
       "      <td>13.731086</td>\n",
       "      <td>15.449381</td>\n",
       "      <td>13.108050</td>\n",
       "      <td>11.948349</td>\n",
       "      <td>14.492458</td>\n",
       "      <td>12.008819</td>\n",
       "      <td>14.727796</td>\n",
       "      <td>11.733394</td>\n",
       "      <td>10.864286</td>\n",
       "      <td>...</td>\n",
       "      <td>3.595748</td>\n",
       "      <td>2.325542</td>\n",
       "      <td>8.359634</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>0</td>\n",
       "      <td>14.468678</td>\n",
       "      <td>15.438259</td>\n",
       "      <td>13.737836</td>\n",
       "      <td>12.204044</td>\n",
       "      <td>12.950250</td>\n",
       "      <td>12.466876</td>\n",
       "      <td>14.826535</td>\n",
       "      <td>12.037085</td>\n",
       "      <td>11.032339</td>\n",
       "      <td>...</td>\n",
       "      <td>2.148865</td>\n",
       "      <td>2.286202</td>\n",
       "      <td>8.402214</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>0</td>\n",
       "      <td>13.589152</td>\n",
       "      <td>15.895696</td>\n",
       "      <td>13.992094</td>\n",
       "      <td>12.260070</td>\n",
       "      <td>12.513580</td>\n",
       "      <td>12.256111</td>\n",
       "      <td>14.934873</td>\n",
       "      <td>11.807985</td>\n",
       "      <td>10.975466</td>\n",
       "      <td>...</td>\n",
       "      <td>1.805982</td>\n",
       "      <td>2.657117</td>\n",
       "      <td>8.319195</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>0</td>\n",
       "      <td>14.124827</td>\n",
       "      <td>15.667980</td>\n",
       "      <td>13.885415</td>\n",
       "      <td>12.434999</td>\n",
       "      <td>12.962366</td>\n",
       "      <td>12.152578</td>\n",
       "      <td>14.616602</td>\n",
       "      <td>11.767767</td>\n",
       "      <td>10.930519</td>\n",
       "      <td>...</td>\n",
       "      <td>2.258414</td>\n",
       "      <td>2.491088</td>\n",
       "      <td>8.240600</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>0</td>\n",
       "      <td>14.125212</td>\n",
       "      <td>15.848679</td>\n",
       "      <td>14.611221</td>\n",
       "      <td>12.768474</td>\n",
       "      <td>12.380620</td>\n",
       "      <td>12.660110</td>\n",
       "      <td>14.852526</td>\n",
       "      <td>12.026815</td>\n",
       "      <td>10.841767</td>\n",
       "      <td>...</td>\n",
       "      <td>1.683385</td>\n",
       "      <td>2.430159</td>\n",
       "      <td>8.369507</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>0</td>\n",
       "      <td>13.911841</td>\n",
       "      <td>15.770673</td>\n",
       "      <td>13.616139</td>\n",
       "      <td>12.200233</td>\n",
       "      <td>13.897249</td>\n",
       "      <td>12.203694</td>\n",
       "      <td>14.965501</td>\n",
       "      <td>11.984423</td>\n",
       "      <td>11.260172</td>\n",
       "      <td>...</td>\n",
       "      <td>3.227728</td>\n",
       "      <td>2.456011</td>\n",
       "      <td>8.212264</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>0</td>\n",
       "      <td>13.905019</td>\n",
       "      <td>16.145448</td>\n",
       "      <td>13.694255</td>\n",
       "      <td>12.297953</td>\n",
       "      <td>13.897956</td>\n",
       "      <td>12.847235</td>\n",
       "      <td>14.765581</td>\n",
       "      <td>11.922889</td>\n",
       "      <td>11.323538</td>\n",
       "      <td>...</td>\n",
       "      <td>3.111999</td>\n",
       "      <td>2.169131</td>\n",
       "      <td>8.390142</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>0</td>\n",
       "      <td>13.587752</td>\n",
       "      <td>15.693374</td>\n",
       "      <td>13.750024</td>\n",
       "      <td>12.287812</td>\n",
       "      <td>13.601792</td>\n",
       "      <td>12.299811</td>\n",
       "      <td>15.022294</td>\n",
       "      <td>11.774581</td>\n",
       "      <td>10.902013</td>\n",
       "      <td>...</td>\n",
       "      <td>2.871030</td>\n",
       "      <td>2.530671</td>\n",
       "      <td>8.470635</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>0</td>\n",
       "      <td>14.187646</td>\n",
       "      <td>15.470626</td>\n",
       "      <td>13.781765</td>\n",
       "      <td>12.357138</td>\n",
       "      <td>14.225409</td>\n",
       "      <td>14.940561</td>\n",
       "      <td>14.684765</td>\n",
       "      <td>11.810785</td>\n",
       "      <td>10.959663</td>\n",
       "      <td>...</td>\n",
       "      <td>2.047146</td>\n",
       "      <td>2.173930</td>\n",
       "      <td>8.382205</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>0</td>\n",
       "      <td>14.094374</td>\n",
       "      <td>15.391283</td>\n",
       "      <td>13.156052</td>\n",
       "      <td>12.299693</td>\n",
       "      <td>12.538918</td>\n",
       "      <td>12.572195</td>\n",
       "      <td>14.731909</td>\n",
       "      <td>11.818707</td>\n",
       "      <td>11.256166</td>\n",
       "      <td>...</td>\n",
       "      <td>1.879761</td>\n",
       "      <td>2.061995</td>\n",
       "      <td>8.421293</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>0</td>\n",
       "      <td>13.887737</td>\n",
       "      <td>15.651142</td>\n",
       "      <td>13.677427</td>\n",
       "      <td>12.106190</td>\n",
       "      <td>12.443795</td>\n",
       "      <td>12.495065</td>\n",
       "      <td>15.046157</td>\n",
       "      <td>11.758245</td>\n",
       "      <td>11.162599</td>\n",
       "      <td>...</td>\n",
       "      <td>1.847846</td>\n",
       "      <td>2.536259</td>\n",
       "      <td>8.222391</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>0</td>\n",
       "      <td>14.229909</td>\n",
       "      <td>15.692926</td>\n",
       "      <td>14.235475</td>\n",
       "      <td>12.749272</td>\n",
       "      <td>12.978876</td>\n",
       "      <td>11.558824</td>\n",
       "      <td>14.839368</td>\n",
       "      <td>11.590611</td>\n",
       "      <td>11.228472</td>\n",
       "      <td>...</td>\n",
       "      <td>2.321552</td>\n",
       "      <td>2.765492</td>\n",
       "      <td>8.528619</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>0</td>\n",
       "      <td>13.470893</td>\n",
       "      <td>15.776606</td>\n",
       "      <td>12.803612</td>\n",
       "      <td>11.874306</td>\n",
       "      <td>13.856436</td>\n",
       "      <td>11.851316</td>\n",
       "      <td>14.723356</td>\n",
       "      <td>11.897736</td>\n",
       "      <td>11.275317</td>\n",
       "      <td>...</td>\n",
       "      <td>3.178995</td>\n",
       "      <td>2.031551</td>\n",
       "      <td>8.339872</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>0</td>\n",
       "      <td>13.791416</td>\n",
       "      <td>15.660444</td>\n",
       "      <td>12.416035</td>\n",
       "      <td>11.807934</td>\n",
       "      <td>14.847563</td>\n",
       "      <td>12.695139</td>\n",
       "      <td>14.803905</td>\n",
       "      <td>11.960705</td>\n",
       "      <td>11.263613</td>\n",
       "      <td>...</td>\n",
       "      <td>3.772191</td>\n",
       "      <td>2.185031</td>\n",
       "      <td>8.260333</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>0</td>\n",
       "      <td>13.811276</td>\n",
       "      <td>15.877942</td>\n",
       "      <td>13.655066</td>\n",
       "      <td>12.005657</td>\n",
       "      <td>13.359262</td>\n",
       "      <td>12.686008</td>\n",
       "      <td>14.837980</td>\n",
       "      <td>12.442368</td>\n",
       "      <td>11.095155</td>\n",
       "      <td>...</td>\n",
       "      <td>2.656716</td>\n",
       "      <td>2.359158</td>\n",
       "      <td>8.223552</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>0</td>\n",
       "      <td>13.408338</td>\n",
       "      <td>15.825618</td>\n",
       "      <td>13.032500</td>\n",
       "      <td>12.104698</td>\n",
       "      <td>13.393705</td>\n",
       "      <td>12.255903</td>\n",
       "      <td>14.912439</td>\n",
       "      <td>12.352381</td>\n",
       "      <td>11.293471</td>\n",
       "      <td>...</td>\n",
       "      <td>2.625084</td>\n",
       "      <td>2.334525</td>\n",
       "      <td>8.273688</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>0</td>\n",
       "      <td>13.626238</td>\n",
       "      <td>15.683295</td>\n",
       "      <td>13.745549</td>\n",
       "      <td>12.419539</td>\n",
       "      <td>12.659798</td>\n",
       "      <td>12.046584</td>\n",
       "      <td>14.929399</td>\n",
       "      <td>12.618666</td>\n",
       "      <td>11.181892</td>\n",
       "      <td>...</td>\n",
       "      <td>2.050419</td>\n",
       "      <td>2.167668</td>\n",
       "      <td>8.489779</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>0</td>\n",
       "      <td>13.732388</td>\n",
       "      <td>15.614327</td>\n",
       "      <td>13.550485</td>\n",
       "      <td>12.214790</td>\n",
       "      <td>12.295163</td>\n",
       "      <td>11.849885</td>\n",
       "      <td>14.638246</td>\n",
       "      <td>12.493920</td>\n",
       "      <td>11.128544</td>\n",
       "      <td>...</td>\n",
       "      <td>1.587524</td>\n",
       "      <td>2.198980</td>\n",
       "      <td>8.301010</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>0</td>\n",
       "      <td>13.434239</td>\n",
       "      <td>15.599394</td>\n",
       "      <td>12.937875</td>\n",
       "      <td>11.949690</td>\n",
       "      <td>14.685660</td>\n",
       "      <td>12.535203</td>\n",
       "      <td>14.994927</td>\n",
       "      <td>12.625845</td>\n",
       "      <td>11.330293</td>\n",
       "      <td>...</td>\n",
       "      <td>3.662519</td>\n",
       "      <td>2.141213</td>\n",
       "      <td>8.289688</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>0</td>\n",
       "      <td>14.095026</td>\n",
       "      <td>15.480027</td>\n",
       "      <td>13.737106</td>\n",
       "      <td>12.276233</td>\n",
       "      <td>13.373069</td>\n",
       "      <td>12.241929</td>\n",
       "      <td>14.765373</td>\n",
       "      <td>12.465197</td>\n",
       "      <td>11.278841</td>\n",
       "      <td>...</td>\n",
       "      <td>2.678659</td>\n",
       "      <td>2.204938</td>\n",
       "      <td>8.636030</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>0</td>\n",
       "      <td>13.948679</td>\n",
       "      <td>15.707015</td>\n",
       "      <td>14.538745</td>\n",
       "      <td>12.775609</td>\n",
       "      <td>13.379634</td>\n",
       "      <td>12.260513</td>\n",
       "      <td>14.727783</td>\n",
       "      <td>12.237720</td>\n",
       "      <td>11.051486</td>\n",
       "      <td>...</td>\n",
       "      <td>2.675655</td>\n",
       "      <td>2.335937</td>\n",
       "      <td>8.484635</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>0</td>\n",
       "      <td>14.298264</td>\n",
       "      <td>15.355092</td>\n",
       "      <td>14.103120</td>\n",
       "      <td>12.766971</td>\n",
       "      <td>12.750475</td>\n",
       "      <td>11.651945</td>\n",
       "      <td>14.502873</td>\n",
       "      <td>12.194344</td>\n",
       "      <td>11.072816</td>\n",
       "      <td>...</td>\n",
       "      <td>2.027413</td>\n",
       "      <td>2.646200</td>\n",
       "      <td>8.288551</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>0</td>\n",
       "      <td>14.130082</td>\n",
       "      <td>15.892906</td>\n",
       "      <td>13.303000</td>\n",
       "      <td>12.037620</td>\n",
       "      <td>12.079807</td>\n",
       "      <td>12.701836</td>\n",
       "      <td>14.945113</td>\n",
       "      <td>12.122186</td>\n",
       "      <td>11.100334</td>\n",
       "      <td>...</td>\n",
       "      <td>1.347397</td>\n",
       "      <td>2.130309</td>\n",
       "      <td>8.263916</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>1</td>\n",
       "      <td>13.607465</td>\n",
       "      <td>15.742973</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11.700570</td>\n",
       "      <td>13.821512</td>\n",
       "      <td>13.444548</td>\n",
       "      <td>14.891810</td>\n",
       "      <td>11.082123</td>\n",
       "      <td>11.350869</td>\n",
       "      <td>...</td>\n",
       "      <td>3.029353</td>\n",
       "      <td>2.386811</td>\n",
       "      <td>8.446067</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>1</td>\n",
       "      <td>14.074076</td>\n",
       "      <td>15.550370</td>\n",
       "      <td>14.360856</td>\n",
       "      <td>12.789551</td>\n",
       "      <td>12.534986</td>\n",
       "      <td>13.780358</td>\n",
       "      <td>14.695148</td>\n",
       "      <td>11.888787</td>\n",
       "      <td>11.228459</td>\n",
       "      <td>...</td>\n",
       "      <td>1.628613</td>\n",
       "      <td>2.728490</td>\n",
       "      <td>8.435363</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>1</td>\n",
       "      <td>13.608710</td>\n",
       "      <td>15.620534</td>\n",
       "      <td>13.926261</td>\n",
       "      <td>11.985488</td>\n",
       "      <td>13.138026</td>\n",
       "      <td>13.473093</td>\n",
       "      <td>14.805259</td>\n",
       "      <td>12.167358</td>\n",
       "      <td>10.928987</td>\n",
       "      <td>...</td>\n",
       "      <td>2.296785</td>\n",
       "      <td>2.157262</td>\n",
       "      <td>8.497522</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>1</td>\n",
       "      <td>13.912395</td>\n",
       "      <td>15.710430</td>\n",
       "      <td>12.546500</td>\n",
       "      <td>11.658677</td>\n",
       "      <td>12.798666</td>\n",
       "      <td>12.761682</td>\n",
       "      <td>14.730217</td>\n",
       "      <td>12.118298</td>\n",
       "      <td>10.966498</td>\n",
       "      <td>...</td>\n",
       "      <td>1.977932</td>\n",
       "      <td>2.284340</td>\n",
       "      <td>8.449097</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>1</td>\n",
       "      <td>14.272681</td>\n",
       "      <td>15.398227</td>\n",
       "      <td>13.917771</td>\n",
       "      <td>12.557395</td>\n",
       "      <td>13.113209</td>\n",
       "      <td>13.035004</td>\n",
       "      <td>14.700396</td>\n",
       "      <td>12.477438</td>\n",
       "      <td>10.884966</td>\n",
       "      <td>...</td>\n",
       "      <td>2.358397</td>\n",
       "      <td>2.501465</td>\n",
       "      <td>8.431409</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>1</td>\n",
       "      <td>13.553682</td>\n",
       "      <td>15.332108</td>\n",
       "      <td>12.975258</td>\n",
       "      <td>12.172884</td>\n",
       "      <td>13.984951</td>\n",
       "      <td>12.718311</td>\n",
       "      <td>14.723451</td>\n",
       "      <td>11.626854</td>\n",
       "      <td>11.235005</td>\n",
       "      <td>...</td>\n",
       "      <td>3.179794</td>\n",
       "      <td>1.858448</td>\n",
       "      <td>8.375820</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>1</td>\n",
       "      <td>13.503647</td>\n",
       "      <td>15.798457</td>\n",
       "      <td>13.363521</td>\n",
       "      <td>12.329034</td>\n",
       "      <td>12.712400</td>\n",
       "      <td>13.083894</td>\n",
       "      <td>14.792074</td>\n",
       "      <td>12.206497</td>\n",
       "      <td>11.103387</td>\n",
       "      <td>...</td>\n",
       "      <td>1.908500</td>\n",
       "      <td>2.339160</td>\n",
       "      <td>8.278816</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>1</td>\n",
       "      <td>13.185622</td>\n",
       "      <td>15.649060</td>\n",
       "      <td>13.128470</td>\n",
       "      <td>12.195693</td>\n",
       "      <td>14.377665</td>\n",
       "      <td>15.325704</td>\n",
       "      <td>14.478469</td>\n",
       "      <td>12.226616</td>\n",
       "      <td>11.094455</td>\n",
       "      <td>...</td>\n",
       "      <td>3.341021</td>\n",
       "      <td>1.845781</td>\n",
       "      <td>8.399841</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>1</td>\n",
       "      <td>13.855509</td>\n",
       "      <td>15.827115</td>\n",
       "      <td>13.265267</td>\n",
       "      <td>11.844197</td>\n",
       "      <td>15.126767</td>\n",
       "      <td>17.093881</td>\n",
       "      <td>15.067586</td>\n",
       "      <td>12.164017</td>\n",
       "      <td>11.151177</td>\n",
       "      <td>...</td>\n",
       "      <td>2.673680</td>\n",
       "      <td>2.548548</td>\n",
       "      <td>8.398240</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>1</td>\n",
       "      <td>13.542743</td>\n",
       "      <td>15.697666</td>\n",
       "      <td>14.144507</td>\n",
       "      <td>12.567397</td>\n",
       "      <td>12.971915</td>\n",
       "      <td>14.868657</td>\n",
       "      <td>14.950939</td>\n",
       "      <td>12.337996</td>\n",
       "      <td>11.316903</td>\n",
       "      <td>...</td>\n",
       "      <td>1.865180</td>\n",
       "      <td>2.135729</td>\n",
       "      <td>8.237084</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>1</td>\n",
       "      <td>13.995393</td>\n",
       "      <td>15.763045</td>\n",
       "      <td>13.422012</td>\n",
       "      <td>12.303995</td>\n",
       "      <td>13.350982</td>\n",
       "      <td>13.343193</td>\n",
       "      <td>14.944149</td>\n",
       "      <td>12.346532</td>\n",
       "      <td>11.353562</td>\n",
       "      <td>...</td>\n",
       "      <td>2.551983</td>\n",
       "      <td>2.417319</td>\n",
       "      <td>8.467316</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>1</td>\n",
       "      <td>13.526921</td>\n",
       "      <td>15.583914</td>\n",
       "      <td>15.236153</td>\n",
       "      <td>13.343259</td>\n",
       "      <td>14.652109</td>\n",
       "      <td>14.664571</td>\n",
       "      <td>14.872883</td>\n",
       "      <td>12.148834</td>\n",
       "      <td>11.120825</td>\n",
       "      <td>...</td>\n",
       "      <td>3.679456</td>\n",
       "      <td>2.722867</td>\n",
       "      <td>8.486501</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>1</td>\n",
       "      <td>14.098327</td>\n",
       "      <td>15.572972</td>\n",
       "      <td>13.839735</td>\n",
       "      <td>12.908054</td>\n",
       "      <td>12.830078</td>\n",
       "      <td>14.253377</td>\n",
       "      <td>14.688208</td>\n",
       "      <td>12.091903</td>\n",
       "      <td>11.033918</td>\n",
       "      <td>...</td>\n",
       "      <td>1.250891</td>\n",
       "      <td>2.501105</td>\n",
       "      <td>8.518012</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>1</td>\n",
       "      <td>14.237930</td>\n",
       "      <td>15.678887</td>\n",
       "      <td>13.976978</td>\n",
       "      <td>12.430527</td>\n",
       "      <td>12.674354</td>\n",
       "      <td>13.679608</td>\n",
       "      <td>14.850197</td>\n",
       "      <td>11.542836</td>\n",
       "      <td>11.096112</td>\n",
       "      <td>...</td>\n",
       "      <td>1.756122</td>\n",
       "      <td>2.449833</td>\n",
       "      <td>8.363992</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>1</td>\n",
       "      <td>13.970157</td>\n",
       "      <td>16.027721</td>\n",
       "      <td>14.244315</td>\n",
       "      <td>13.328620</td>\n",
       "      <td>13.067405</td>\n",
       "      <td>15.250113</td>\n",
       "      <td>14.968732</td>\n",
       "      <td>11.894993</td>\n",
       "      <td>11.028224</td>\n",
       "      <td>...</td>\n",
       "      <td>1.745003</td>\n",
       "      <td>2.890356</td>\n",
       "      <td>8.400514</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>1</td>\n",
       "      <td>13.794663</td>\n",
       "      <td>15.445801</td>\n",
       "      <td>13.475055</td>\n",
       "      <td>12.279256</td>\n",
       "      <td>13.653491</td>\n",
       "      <td>13.258665</td>\n",
       "      <td>14.796514</td>\n",
       "      <td>11.680741</td>\n",
       "      <td>11.125568</td>\n",
       "      <td>...</td>\n",
       "      <td>2.840020</td>\n",
       "      <td>2.767087</td>\n",
       "      <td>8.474646</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>1</td>\n",
       "      <td>14.103663</td>\n",
       "      <td>15.568095</td>\n",
       "      <td>13.472755</td>\n",
       "      <td>12.202148</td>\n",
       "      <td>12.950661</td>\n",
       "      <td>14.140104</td>\n",
       "      <td>14.573157</td>\n",
       "      <td>11.537200</td>\n",
       "      <td>11.256968</td>\n",
       "      <td>...</td>\n",
       "      <td>1.777345</td>\n",
       "      <td>2.373036</td>\n",
       "      <td>8.395026</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>1</td>\n",
       "      <td>13.720616</td>\n",
       "      <td>15.587086</td>\n",
       "      <td>13.224181</td>\n",
       "      <td>11.946611</td>\n",
       "      <td>12.845243</td>\n",
       "      <td>13.753623</td>\n",
       "      <td>14.738364</td>\n",
       "      <td>11.998374</td>\n",
       "      <td>11.070418</td>\n",
       "      <td>...</td>\n",
       "      <td>1.822230</td>\n",
       "      <td>2.099936</td>\n",
       "      <td>8.476514</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>1</td>\n",
       "      <td>13.336426</td>\n",
       "      <td>15.462282</td>\n",
       "      <td>13.441044</td>\n",
       "      <td>12.612899</td>\n",
       "      <td>14.526108</td>\n",
       "      <td>13.405522</td>\n",
       "      <td>14.843443</td>\n",
       "      <td>11.952914</td>\n",
       "      <td>11.006261</td>\n",
       "      <td>...</td>\n",
       "      <td>3.595970</td>\n",
       "      <td>2.855985</td>\n",
       "      <td>8.215932</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>1</td>\n",
       "      <td>13.712649</td>\n",
       "      <td>15.710702</td>\n",
       "      <td>13.450497</td>\n",
       "      <td>12.085055</td>\n",
       "      <td>13.107520</td>\n",
       "      <td>13.235522</td>\n",
       "      <td>14.892700</td>\n",
       "      <td>12.352521</td>\n",
       "      <td>11.340161</td>\n",
       "      <td>...</td>\n",
       "      <td>2.106706</td>\n",
       "      <td>2.409483</td>\n",
       "      <td>8.380304</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>1</td>\n",
       "      <td>14.136347</td>\n",
       "      <td>15.646664</td>\n",
       "      <td>13.707184</td>\n",
       "      <td>12.397715</td>\n",
       "      <td>12.885362</td>\n",
       "      <td>12.590245</td>\n",
       "      <td>14.732140</td>\n",
       "      <td>12.206637</td>\n",
       "      <td>11.300098</td>\n",
       "      <td>...</td>\n",
       "      <td>2.070683</td>\n",
       "      <td>2.418890</td>\n",
       "      <td>8.449288</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>1</td>\n",
       "      <td>13.877760</td>\n",
       "      <td>15.865054</td>\n",
       "      <td>13.652736</td>\n",
       "      <td>11.859613</td>\n",
       "      <td>12.580436</td>\n",
       "      <td>13.614601</td>\n",
       "      <td>14.953066</td>\n",
       "      <td>12.419737</td>\n",
       "      <td>11.253510</td>\n",
       "      <td>...</td>\n",
       "      <td>1.688147</td>\n",
       "      <td>2.160634</td>\n",
       "      <td>8.381040</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>1</td>\n",
       "      <td>13.101075</td>\n",
       "      <td>15.563113</td>\n",
       "      <td>14.134044</td>\n",
       "      <td>12.751887</td>\n",
       "      <td>14.329279</td>\n",
       "      <td>13.317395</td>\n",
       "      <td>14.881287</td>\n",
       "      <td>12.050647</td>\n",
       "      <td>10.993768</td>\n",
       "      <td>...</td>\n",
       "      <td>3.425897</td>\n",
       "      <td>2.577070</td>\n",
       "      <td>8.330042</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>1</td>\n",
       "      <td>13.699883</td>\n",
       "      <td>15.633531</td>\n",
       "      <td>13.078100</td>\n",
       "      <td>11.848265</td>\n",
       "      <td>14.006595</td>\n",
       "      <td>12.877403</td>\n",
       "      <td>14.893908</td>\n",
       "      <td>12.019573</td>\n",
       "      <td>10.943293</td>\n",
       "      <td>...</td>\n",
       "      <td>3.208075</td>\n",
       "      <td>2.329893</td>\n",
       "      <td>8.449347</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>1</td>\n",
       "      <td>13.769740</td>\n",
       "      <td>15.617562</td>\n",
       "      <td>13.983691</td>\n",
       "      <td>12.404641</td>\n",
       "      <td>14.108590</td>\n",
       "      <td>13.666401</td>\n",
       "      <td>14.984438</td>\n",
       "      <td>12.114248</td>\n",
       "      <td>11.192342</td>\n",
       "      <td>...</td>\n",
       "      <td>3.288916</td>\n",
       "      <td>2.731427</td>\n",
       "      <td>8.495185</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>1</td>\n",
       "      <td>13.881991</td>\n",
       "      <td>15.634942</td>\n",
       "      <td>12.687261</td>\n",
       "      <td>11.649048</td>\n",
       "      <td>12.839689</td>\n",
       "      <td>12.596961</td>\n",
       "      <td>14.787885</td>\n",
       "      <td>11.974167</td>\n",
       "      <td>11.090949</td>\n",
       "      <td>...</td>\n",
       "      <td>2.093669</td>\n",
       "      <td>2.238217</td>\n",
       "      <td>8.352757</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>1</td>\n",
       "      <td>13.911391</td>\n",
       "      <td>15.808535</td>\n",
       "      <td>13.255651</td>\n",
       "      <td>12.072060</td>\n",
       "      <td>12.829308</td>\n",
       "      <td>12.518926</td>\n",
       "      <td>14.914205</td>\n",
       "      <td>12.009783</td>\n",
       "      <td>11.241416</td>\n",
       "      <td>...</td>\n",
       "      <td>2.099744</td>\n",
       "      <td>2.460060</td>\n",
       "      <td>8.268259</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>1</td>\n",
       "      <td>14.004190</td>\n",
       "      <td>15.578057</td>\n",
       "      <td>15.050097</td>\n",
       "      <td>12.955338</td>\n",
       "      <td>12.021058</td>\n",
       "      <td>12.698247</td>\n",
       "      <td>14.838858</td>\n",
       "      <td>11.864216</td>\n",
       "      <td>11.361720</td>\n",
       "      <td>...</td>\n",
       "      <td>1.215855</td>\n",
       "      <td>2.716850</td>\n",
       "      <td>8.382161</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>1</td>\n",
       "      <td>13.782098</td>\n",
       "      <td>15.628488</td>\n",
       "      <td>13.800509</td>\n",
       "      <td>12.324283</td>\n",
       "      <td>13.557521</td>\n",
       "      <td>13.930284</td>\n",
       "      <td>15.010427</td>\n",
       "      <td>11.877001</td>\n",
       "      <td>11.293042</td>\n",
       "      <td>...</td>\n",
       "      <td>2.567097</td>\n",
       "      <td>2.339410</td>\n",
       "      <td>8.362965</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>1</td>\n",
       "      <td>13.841570</td>\n",
       "      <td>15.697214</td>\n",
       "      <td>14.467861</td>\n",
       "      <td>12.549533</td>\n",
       "      <td>13.157717</td>\n",
       "      <td>14.309882</td>\n",
       "      <td>14.518098</td>\n",
       "      <td>12.173317</td>\n",
       "      <td>11.058203</td>\n",
       "      <td>...</td>\n",
       "      <td>1.637428</td>\n",
       "      <td>2.147184</td>\n",
       "      <td>8.379108</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>92 rows × 132 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Label   Pyruvate  MethylSuccinate  Phosphoglyceric Acid  \\\n",
       "20       0  14.158545        15.624522             13.383600   \n",
       "24       0  14.180287        15.325665             13.468363   \n",
       "29       0  13.921329        16.156867             14.615061   \n",
       "52       0  13.785605        15.939535             13.206615   \n",
       "54       0  13.674234        15.743668             11.542008   \n",
       "56       0  13.447093        15.595200             13.118302   \n",
       "57       0  13.616625        15.406841             14.124967   \n",
       "58       0  13.731086        15.449381             13.108050   \n",
       "59       0  14.468678        15.438259             13.737836   \n",
       "60       0  13.589152        15.895696             13.992094   \n",
       "61       0  14.124827        15.667980             13.885415   \n",
       "62       0  14.125212        15.848679             14.611221   \n",
       "63       0  13.911841        15.770673             13.616139   \n",
       "66       0  13.905019        16.145448             13.694255   \n",
       "67       0  13.587752        15.693374             13.750024   \n",
       "68       0  14.187646        15.470626             13.781765   \n",
       "69       0  14.094374        15.391283             13.156052   \n",
       "70       0  13.887737        15.651142             13.677427   \n",
       "71       0  14.229909        15.692926             14.235475   \n",
       "72       0  13.470893        15.776606             12.803612   \n",
       "73       0  13.791416        15.660444             12.416035   \n",
       "75       0  13.811276        15.877942             13.655066   \n",
       "78       0  13.408338        15.825618             13.032500   \n",
       "79       0  13.626238        15.683295             13.745549   \n",
       "80       0  13.732388        15.614327             13.550485   \n",
       "82       0  13.434239        15.599394             12.937875   \n",
       "83       0  14.095026        15.480027             13.737106   \n",
       "84       0  13.948679        15.707015             14.538745   \n",
       "85       0  14.298264        15.355092             14.103120   \n",
       "86       0  14.130082        15.892906             13.303000   \n",
       "..     ...        ...              ...                   ...   \n",
       "33       1  13.607465        15.742973                   NaN   \n",
       "34       1  14.074076        15.550370             14.360856   \n",
       "36       1  13.608710        15.620534             13.926261   \n",
       "37       1  13.912395        15.710430             12.546500   \n",
       "39       1  14.272681        15.398227             13.917771   \n",
       "40       1  13.553682        15.332108             12.975258   \n",
       "41       1  13.503647        15.798457             13.363521   \n",
       "42       1  13.185622        15.649060             13.128470   \n",
       "43       1  13.855509        15.827115             13.265267   \n",
       "44       1  13.542743        15.697666             14.144507   \n",
       "45       1  13.995393        15.763045             13.422012   \n",
       "46       1  13.526921        15.583914             15.236153   \n",
       "47       1  14.098327        15.572972             13.839735   \n",
       "48       1  14.237930        15.678887             13.976978   \n",
       "49       1  13.970157        16.027721             14.244315   \n",
       "53       1  13.794663        15.445801             13.475055   \n",
       "55       1  14.103663        15.568095             13.472755   \n",
       "64       1  13.720616        15.587086             13.224181   \n",
       "65       1  13.336426        15.462282             13.441044   \n",
       "76       1  13.712649        15.710702             13.450497   \n",
       "77       1  14.136347        15.646664             13.707184   \n",
       "81       1  13.877760        15.865054             13.652736   \n",
       "88       1  13.101075        15.563113             14.134044   \n",
       "89       1  13.699883        15.633531             13.078100   \n",
       "98       1  13.769740        15.617562             13.983691   \n",
       "99       1  13.881991        15.634942             12.687261   \n",
       "100      1  13.911391        15.808535             13.255651   \n",
       "101      1  14.004190        15.578057             15.050097   \n",
       "102      1  13.782098        15.628488             13.800509   \n",
       "103      1  13.841570        15.697214             14.467861   \n",
       "\n",
       "     Glucose 1-phosphate  Malonic Acid  Fumaric Acid  Alpha-Ketoglutaric Acid  \\\n",
       "20             11.923638     13.933560     12.787696                14.981440   \n",
       "24             12.046294     13.043221     12.536227                14.593418   \n",
       "29             13.051929     13.381985     12.419600                14.701978   \n",
       "52             12.342156     12.448068     12.211098                14.860913   \n",
       "54             11.807872     14.582068     11.635154                14.970451   \n",
       "56             12.002027     12.817962     12.065246                14.796780   \n",
       "57             12.499758     13.168693     12.235455                14.721283   \n",
       "58             11.948349     14.492458     12.008819                14.727796   \n",
       "59             12.204044     12.950250     12.466876                14.826535   \n",
       "60             12.260070     12.513580     12.256111                14.934873   \n",
       "61             12.434999     12.962366     12.152578                14.616602   \n",
       "62             12.768474     12.380620     12.660110                14.852526   \n",
       "63             12.200233     13.897249     12.203694                14.965501   \n",
       "66             12.297953     13.897956     12.847235                14.765581   \n",
       "67             12.287812     13.601792     12.299811                15.022294   \n",
       "68             12.357138     14.225409     14.940561                14.684765   \n",
       "69             12.299693     12.538918     12.572195                14.731909   \n",
       "70             12.106190     12.443795     12.495065                15.046157   \n",
       "71             12.749272     12.978876     11.558824                14.839368   \n",
       "72             11.874306     13.856436     11.851316                14.723356   \n",
       "73             11.807934     14.847563     12.695139                14.803905   \n",
       "75             12.005657     13.359262     12.686008                14.837980   \n",
       "78             12.104698     13.393705     12.255903                14.912439   \n",
       "79             12.419539     12.659798     12.046584                14.929399   \n",
       "80             12.214790     12.295163     11.849885                14.638246   \n",
       "82             11.949690     14.685660     12.535203                14.994927   \n",
       "83             12.276233     13.373069     12.241929                14.765373   \n",
       "84             12.775609     13.379634     12.260513                14.727783   \n",
       "85             12.766971     12.750475     11.651945                14.502873   \n",
       "86             12.037620     12.079807     12.701836                14.945113   \n",
       "..                   ...           ...           ...                      ...   \n",
       "33             11.700570     13.821512     13.444548                14.891810   \n",
       "34             12.789551     12.534986     13.780358                14.695148   \n",
       "36             11.985488     13.138026     13.473093                14.805259   \n",
       "37             11.658677     12.798666     12.761682                14.730217   \n",
       "39             12.557395     13.113209     13.035004                14.700396   \n",
       "40             12.172884     13.984951     12.718311                14.723451   \n",
       "41             12.329034     12.712400     13.083894                14.792074   \n",
       "42             12.195693     14.377665     15.325704                14.478469   \n",
       "43             11.844197     15.126767     17.093881                15.067586   \n",
       "44             12.567397     12.971915     14.868657                14.950939   \n",
       "45             12.303995     13.350982     13.343193                14.944149   \n",
       "46             13.343259     14.652109     14.664571                14.872883   \n",
       "47             12.908054     12.830078     14.253377                14.688208   \n",
       "48             12.430527     12.674354     13.679608                14.850197   \n",
       "49             13.328620     13.067405     15.250113                14.968732   \n",
       "53             12.279256     13.653491     13.258665                14.796514   \n",
       "55             12.202148     12.950661     14.140104                14.573157   \n",
       "64             11.946611     12.845243     13.753623                14.738364   \n",
       "65             12.612899     14.526108     13.405522                14.843443   \n",
       "76             12.085055     13.107520     13.235522                14.892700   \n",
       "77             12.397715     12.885362     12.590245                14.732140   \n",
       "81             11.859613     12.580436     13.614601                14.953066   \n",
       "88             12.751887     14.329279     13.317395                14.881287   \n",
       "89             11.848265     14.006595     12.877403                14.893908   \n",
       "98             12.404641     14.108590     13.666401                14.984438   \n",
       "99             11.649048     12.839689     12.596961                14.787885   \n",
       "100            12.072060     12.829308     12.518926                14.914205   \n",
       "101            12.955338     12.021058     12.698247                14.838858   \n",
       "102            12.324283     13.557521     13.930284                15.010427   \n",
       "103            12.549533     13.157717     14.309882                14.518098   \n",
       "\n",
       "     Sarcosine  Cadaverine  ...  3-Hydroxybenzoic acid  Succinate   Glucose  \\\n",
       "20   11.739484   10.921776  ...               3.161390   2.165842  8.310292   \n",
       "24   11.833211   11.121865  ...               2.355910   2.260621  8.423733   \n",
       "29   12.232070   11.025333  ...               2.472365   2.295447  8.357071   \n",
       "52   11.949448   11.015657  ...               1.844567   2.278589  8.414504   \n",
       "54   11.457569   11.103239  ...               3.773262   2.136011  8.193980   \n",
       "56   11.831409   11.219337  ...               2.165502   2.439154  8.346805   \n",
       "57   11.691816   11.189055  ...               2.431075   2.396853  8.139350   \n",
       "58   11.733394   10.864286  ...               3.595748   2.325542  8.359634   \n",
       "59   12.037085   11.032339  ...               2.148865   2.286202  8.402214   \n",
       "60   11.807985   10.975466  ...               1.805982   2.657117  8.319195   \n",
       "61   11.767767   10.930519  ...               2.258414   2.491088  8.240600   \n",
       "62   12.026815   10.841767  ...               1.683385   2.430159  8.369507   \n",
       "63   11.984423   11.260172  ...               3.227728   2.456011  8.212264   \n",
       "66   11.922889   11.323538  ...               3.111999   2.169131  8.390142   \n",
       "67   11.774581   10.902013  ...               2.871030   2.530671  8.470635   \n",
       "68   11.810785   10.959663  ...               2.047146   2.173930  8.382205   \n",
       "69   11.818707   11.256166  ...               1.879761   2.061995  8.421293   \n",
       "70   11.758245   11.162599  ...               1.847846   2.536259  8.222391   \n",
       "71   11.590611   11.228472  ...               2.321552   2.765492  8.528619   \n",
       "72   11.897736   11.275317  ...               3.178995   2.031551  8.339872   \n",
       "73   11.960705   11.263613  ...               3.772191   2.185031  8.260333   \n",
       "75   12.442368   11.095155  ...               2.656716   2.359158  8.223552   \n",
       "78   12.352381   11.293471  ...               2.625084   2.334525  8.273688   \n",
       "79   12.618666   11.181892  ...               2.050419   2.167668  8.489779   \n",
       "80   12.493920   11.128544  ...               1.587524   2.198980  8.301010   \n",
       "82   12.625845   11.330293  ...               3.662519   2.141213  8.289688   \n",
       "83   12.465197   11.278841  ...               2.678659   2.204938  8.636030   \n",
       "84   12.237720   11.051486  ...               2.675655   2.335937  8.484635   \n",
       "85   12.194344   11.072816  ...               2.027413   2.646200  8.288551   \n",
       "86   12.122186   11.100334  ...               1.347397   2.130309  8.263916   \n",
       "..         ...         ...  ...                    ...        ...       ...   \n",
       "33   11.082123   11.350869  ...               3.029353   2.386811  8.446067   \n",
       "34   11.888787   11.228459  ...               1.628613   2.728490  8.435363   \n",
       "36   12.167358   10.928987  ...               2.296785   2.157262  8.497522   \n",
       "37   12.118298   10.966498  ...               1.977932   2.284340  8.449097   \n",
       "39   12.477438   10.884966  ...               2.358397   2.501465  8.431409   \n",
       "40   11.626854   11.235005  ...               3.179794   1.858448  8.375820   \n",
       "41   12.206497   11.103387  ...               1.908500   2.339160  8.278816   \n",
       "42   12.226616   11.094455  ...               3.341021   1.845781  8.399841   \n",
       "43   12.164017   11.151177  ...               2.673680   2.548548  8.398240   \n",
       "44   12.337996   11.316903  ...               1.865180   2.135729  8.237084   \n",
       "45   12.346532   11.353562  ...               2.551983   2.417319  8.467316   \n",
       "46   12.148834   11.120825  ...               3.679456   2.722867  8.486501   \n",
       "47   12.091903   11.033918  ...               1.250891   2.501105  8.518012   \n",
       "48   11.542836   11.096112  ...               1.756122   2.449833  8.363992   \n",
       "49   11.894993   11.028224  ...               1.745003   2.890356  8.400514   \n",
       "53   11.680741   11.125568  ...               2.840020   2.767087  8.474646   \n",
       "55   11.537200   11.256968  ...               1.777345   2.373036  8.395026   \n",
       "64   11.998374   11.070418  ...               1.822230   2.099936  8.476514   \n",
       "65   11.952914   11.006261  ...               3.595970   2.855985  8.215932   \n",
       "76   12.352521   11.340161  ...               2.106706   2.409483  8.380304   \n",
       "77   12.206637   11.300098  ...               2.070683   2.418890  8.449288   \n",
       "81   12.419737   11.253510  ...               1.688147   2.160634  8.381040   \n",
       "88   12.050647   10.993768  ...               3.425897   2.577070  8.330042   \n",
       "89   12.019573   10.943293  ...               3.208075   2.329893  8.449347   \n",
       "98   12.114248   11.192342  ...               3.288916   2.731427  8.495185   \n",
       "99   11.974167   11.090949  ...               2.093669   2.238217  8.352757   \n",
       "100  12.009783   11.241416  ...               2.099744   2.460060  8.268259   \n",
       "101  11.864216   11.361720  ...               1.215855   2.716850  8.382161   \n",
       "102  11.877001   11.293042  ...               2.567097   2.339410  8.362965   \n",
       "103  12.173317   11.058203  ...               1.637428   2.147184  8.379108   \n",
       "\n",
       "     V127  V128  V129  V130  V131  V132  V133  \n",
       "20    NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
       "24    NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
       "29    NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
       "52    NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
       "54    NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
       "56    NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
       "57    NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
       "58    NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
       "59    NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
       "60    NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
       "61    NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
       "62    NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
       "63    NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
       "66    NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
       "67    NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
       "68    NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
       "69    NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
       "70    NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
       "71    NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
       "72    NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
       "73    NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
       "75    NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
       "78    NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
       "79    NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
       "80    NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
       "82    NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
       "83    NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
       "84    NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
       "85    NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
       "86    NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
       "..    ...   ...   ...   ...   ...   ...   ...  \n",
       "33    NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
       "34    NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
       "36    NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
       "37    NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
       "39    NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
       "40    NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
       "41    NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
       "42    NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
       "43    NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
       "44    NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
       "45    NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
       "46    NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
       "47    NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
       "48    NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
       "49    NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
       "53    NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
       "55    NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
       "64    NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
       "65    NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
       "76    NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
       "77    NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
       "81    NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
       "88    NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
       "89    NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
       "98    NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
       "99    NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
       "100   NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
       "101   NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
       "102   NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
       "103   NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
       "\n",
       "[92 rows x 132 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##################################################################################################################\n",
    "#\n",
    "# YouDo:\n",
    "# \n",
    "#    1) Notice that the import is wonky -- examine the data set\n",
    "#       manually and figure out how to make it import cleanly.\n",
    "#      \n",
    "#       Ensure your data frame has the following handled correctly\n",
    "#         a) the column names (1st row of the csv)\n",
    "#.        b) The unlabeled first column is read as an index\n",
    "#         c) The final row is totally weird--don't import it\n",
    "#\n",
    "#\n",
    "#######################################  BEGIN STUDENT CODE  #####################################################\n",
    "\n",
    "# 1)\n",
    "df = pd.read_csv(datafile, index_col = 0, skipfooter = 1, engine = 'python')\n",
    "df\n",
    "\n",
    "\n",
    "#######################################   END STUDENT CODE   #####################################################\n",
    "#assert df.shape == (92, 132)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################################################################################\n",
    "#\n",
    "# YouDo:\n",
    "# \n",
    "#    1) Get rid of any columns that have only NaN values\n",
    "# https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.dropna.html\n",
    "#\n",
    "#######################################  BEGIN STUDENT CODE  #####################################################\n",
    "\n",
    "df_1 = df.dropna(axis=1, how='all')\n",
    "#removes all the columns with NaN\n",
    "# if put df_1 = df.dropna, then run with df_1, the NaN will still be in the data (unexpected)\n",
    "#give it a new name and assigns a new name, it will drop in new name\n",
    "#df.dropna(axis=1, how=\"all\", inplace=True) if not giving it a new name and it will change on the df.\n",
    "\n",
    "\n",
    "#######################################   END STUDENT CODE   #####################################################\n",
    "\n",
    "#assert df.shape == (92, 125)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>Pyruvate</th>\n",
       "      <th>MethylSuccinate</th>\n",
       "      <th>Phosphoglyceric Acid</th>\n",
       "      <th>Glucose 1-phosphate</th>\n",
       "      <th>Malonic Acid</th>\n",
       "      <th>Fumaric Acid</th>\n",
       "      <th>Alpha-Ketoglutaric Acid</th>\n",
       "      <th>Sarcosine</th>\n",
       "      <th>Cadaverine</th>\n",
       "      <th>...</th>\n",
       "      <th>Histidine</th>\n",
       "      <th>Phenylalanine</th>\n",
       "      <th>Arginine</th>\n",
       "      <th>Tyrosine</th>\n",
       "      <th>Tryptophan</th>\n",
       "      <th>Cystine</th>\n",
       "      <th>lactate</th>\n",
       "      <th>3-Hydroxybenzoic acid</th>\n",
       "      <th>Succinate</th>\n",
       "      <th>Glucose</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0</td>\n",
       "      <td>14.158545</td>\n",
       "      <td>15.624522</td>\n",
       "      <td>13.383600</td>\n",
       "      <td>11.923638</td>\n",
       "      <td>13.933560</td>\n",
       "      <td>12.787696</td>\n",
       "      <td>14.981440</td>\n",
       "      <td>11.739484</td>\n",
       "      <td>10.921776</td>\n",
       "      <td>...</td>\n",
       "      <td>4.439493</td>\n",
       "      <td>3.934856</td>\n",
       "      <td>4.362485</td>\n",
       "      <td>3.125658</td>\n",
       "      <td>3.344071</td>\n",
       "      <td>3.223674</td>\n",
       "      <td>7.391850</td>\n",
       "      <td>3.161390</td>\n",
       "      <td>2.165842</td>\n",
       "      <td>8.310292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0</td>\n",
       "      <td>14.180287</td>\n",
       "      <td>15.325665</td>\n",
       "      <td>13.468363</td>\n",
       "      <td>12.046294</td>\n",
       "      <td>13.043221</td>\n",
       "      <td>12.536227</td>\n",
       "      <td>14.593418</td>\n",
       "      <td>11.833211</td>\n",
       "      <td>11.121865</td>\n",
       "      <td>...</td>\n",
       "      <td>4.091856</td>\n",
       "      <td>3.799364</td>\n",
       "      <td>4.055512</td>\n",
       "      <td>3.812430</td>\n",
       "      <td>3.537357</td>\n",
       "      <td>3.297459</td>\n",
       "      <td>6.926229</td>\n",
       "      <td>2.355910</td>\n",
       "      <td>2.260621</td>\n",
       "      <td>8.423733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0</td>\n",
       "      <td>13.921329</td>\n",
       "      <td>16.156867</td>\n",
       "      <td>14.615061</td>\n",
       "      <td>13.051929</td>\n",
       "      <td>13.381985</td>\n",
       "      <td>12.419600</td>\n",
       "      <td>14.701978</td>\n",
       "      <td>12.232070</td>\n",
       "      <td>11.025333</td>\n",
       "      <td>...</td>\n",
       "      <td>4.647577</td>\n",
       "      <td>4.217314</td>\n",
       "      <td>4.789008</td>\n",
       "      <td>3.910396</td>\n",
       "      <td>4.078951</td>\n",
       "      <td>3.158093</td>\n",
       "      <td>7.132086</td>\n",
       "      <td>2.472365</td>\n",
       "      <td>2.295447</td>\n",
       "      <td>8.357071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>0</td>\n",
       "      <td>13.785605</td>\n",
       "      <td>15.939535</td>\n",
       "      <td>13.206615</td>\n",
       "      <td>12.342156</td>\n",
       "      <td>12.448068</td>\n",
       "      <td>12.211098</td>\n",
       "      <td>14.860913</td>\n",
       "      <td>11.949448</td>\n",
       "      <td>11.015657</td>\n",
       "      <td>...</td>\n",
       "      <td>4.531611</td>\n",
       "      <td>4.166923</td>\n",
       "      <td>4.573985</td>\n",
       "      <td>3.953711</td>\n",
       "      <td>3.698545</td>\n",
       "      <td>2.442535</td>\n",
       "      <td>6.605744</td>\n",
       "      <td>1.844567</td>\n",
       "      <td>2.278589</td>\n",
       "      <td>8.414504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>0</td>\n",
       "      <td>13.674234</td>\n",
       "      <td>15.743668</td>\n",
       "      <td>11.542008</td>\n",
       "      <td>11.807872</td>\n",
       "      <td>14.582068</td>\n",
       "      <td>11.635154</td>\n",
       "      <td>14.970451</td>\n",
       "      <td>11.457569</td>\n",
       "      <td>11.103239</td>\n",
       "      <td>...</td>\n",
       "      <td>4.551687</td>\n",
       "      <td>3.767357</td>\n",
       "      <td>4.449439</td>\n",
       "      <td>3.553009</td>\n",
       "      <td>3.434078</td>\n",
       "      <td>2.675290</td>\n",
       "      <td>6.318173</td>\n",
       "      <td>3.773262</td>\n",
       "      <td>2.136011</td>\n",
       "      <td>8.193980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>0</td>\n",
       "      <td>13.447093</td>\n",
       "      <td>15.595200</td>\n",
       "      <td>13.118302</td>\n",
       "      <td>12.002027</td>\n",
       "      <td>12.817962</td>\n",
       "      <td>12.065246</td>\n",
       "      <td>14.796780</td>\n",
       "      <td>11.831409</td>\n",
       "      <td>11.219337</td>\n",
       "      <td>...</td>\n",
       "      <td>4.473621</td>\n",
       "      <td>3.847232</td>\n",
       "      <td>4.282759</td>\n",
       "      <td>3.600317</td>\n",
       "      <td>3.363340</td>\n",
       "      <td>2.875249</td>\n",
       "      <td>6.822323</td>\n",
       "      <td>2.165502</td>\n",
       "      <td>2.439154</td>\n",
       "      <td>8.346805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>0</td>\n",
       "      <td>13.616625</td>\n",
       "      <td>15.406841</td>\n",
       "      <td>14.124967</td>\n",
       "      <td>12.499758</td>\n",
       "      <td>13.168693</td>\n",
       "      <td>12.235455</td>\n",
       "      <td>14.721283</td>\n",
       "      <td>11.691816</td>\n",
       "      <td>11.189055</td>\n",
       "      <td>...</td>\n",
       "      <td>4.377434</td>\n",
       "      <td>4.196924</td>\n",
       "      <td>4.341699</td>\n",
       "      <td>3.955967</td>\n",
       "      <td>3.573760</td>\n",
       "      <td>3.155289</td>\n",
       "      <td>6.469863</td>\n",
       "      <td>2.431075</td>\n",
       "      <td>2.396853</td>\n",
       "      <td>8.139350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>0</td>\n",
       "      <td>13.731086</td>\n",
       "      <td>15.449381</td>\n",
       "      <td>13.108050</td>\n",
       "      <td>11.948349</td>\n",
       "      <td>14.492458</td>\n",
       "      <td>12.008819</td>\n",
       "      <td>14.727796</td>\n",
       "      <td>11.733394</td>\n",
       "      <td>10.864286</td>\n",
       "      <td>...</td>\n",
       "      <td>4.315900</td>\n",
       "      <td>3.956130</td>\n",
       "      <td>4.369007</td>\n",
       "      <td>3.808282</td>\n",
       "      <td>3.271158</td>\n",
       "      <td>2.944483</td>\n",
       "      <td>6.317708</td>\n",
       "      <td>3.595748</td>\n",
       "      <td>2.325542</td>\n",
       "      <td>8.359634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>0</td>\n",
       "      <td>14.468678</td>\n",
       "      <td>15.438259</td>\n",
       "      <td>13.737836</td>\n",
       "      <td>12.204044</td>\n",
       "      <td>12.950250</td>\n",
       "      <td>12.466876</td>\n",
       "      <td>14.826535</td>\n",
       "      <td>12.037085</td>\n",
       "      <td>11.032339</td>\n",
       "      <td>...</td>\n",
       "      <td>4.233620</td>\n",
       "      <td>3.847929</td>\n",
       "      <td>4.498490</td>\n",
       "      <td>3.642743</td>\n",
       "      <td>3.588967</td>\n",
       "      <td>3.240158</td>\n",
       "      <td>7.335289</td>\n",
       "      <td>2.148865</td>\n",
       "      <td>2.286202</td>\n",
       "      <td>8.402214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>0</td>\n",
       "      <td>13.589152</td>\n",
       "      <td>15.895696</td>\n",
       "      <td>13.992094</td>\n",
       "      <td>12.260070</td>\n",
       "      <td>12.513580</td>\n",
       "      <td>12.256111</td>\n",
       "      <td>14.934873</td>\n",
       "      <td>11.807985</td>\n",
       "      <td>10.975466</td>\n",
       "      <td>...</td>\n",
       "      <td>4.435110</td>\n",
       "      <td>4.002727</td>\n",
       "      <td>4.540823</td>\n",
       "      <td>3.568290</td>\n",
       "      <td>3.629008</td>\n",
       "      <td>3.000813</td>\n",
       "      <td>6.460277</td>\n",
       "      <td>1.805982</td>\n",
       "      <td>2.657117</td>\n",
       "      <td>8.319195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>0</td>\n",
       "      <td>14.124827</td>\n",
       "      <td>15.667980</td>\n",
       "      <td>13.885415</td>\n",
       "      <td>12.434999</td>\n",
       "      <td>12.962366</td>\n",
       "      <td>12.152578</td>\n",
       "      <td>14.616602</td>\n",
       "      <td>11.767767</td>\n",
       "      <td>10.930519</td>\n",
       "      <td>...</td>\n",
       "      <td>4.428947</td>\n",
       "      <td>4.017820</td>\n",
       "      <td>4.510121</td>\n",
       "      <td>4.218307</td>\n",
       "      <td>3.700230</td>\n",
       "      <td>2.817706</td>\n",
       "      <td>6.841190</td>\n",
       "      <td>2.258414</td>\n",
       "      <td>2.491088</td>\n",
       "      <td>8.240600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>0</td>\n",
       "      <td>14.125212</td>\n",
       "      <td>15.848679</td>\n",
       "      <td>14.611221</td>\n",
       "      <td>12.768474</td>\n",
       "      <td>12.380620</td>\n",
       "      <td>12.660110</td>\n",
       "      <td>14.852526</td>\n",
       "      <td>12.026815</td>\n",
       "      <td>10.841767</td>\n",
       "      <td>...</td>\n",
       "      <td>4.533552</td>\n",
       "      <td>4.057604</td>\n",
       "      <td>4.142390</td>\n",
       "      <td>3.918772</td>\n",
       "      <td>3.687081</td>\n",
       "      <td>3.043648</td>\n",
       "      <td>7.135819</td>\n",
       "      <td>1.683385</td>\n",
       "      <td>2.430159</td>\n",
       "      <td>8.369507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>0</td>\n",
       "      <td>13.911841</td>\n",
       "      <td>15.770673</td>\n",
       "      <td>13.616139</td>\n",
       "      <td>12.200233</td>\n",
       "      <td>13.897249</td>\n",
       "      <td>12.203694</td>\n",
       "      <td>14.965501</td>\n",
       "      <td>11.984423</td>\n",
       "      <td>11.260172</td>\n",
       "      <td>...</td>\n",
       "      <td>4.474527</td>\n",
       "      <td>4.037906</td>\n",
       "      <td>4.546264</td>\n",
       "      <td>3.504973</td>\n",
       "      <td>3.716985</td>\n",
       "      <td>3.313695</td>\n",
       "      <td>6.701857</td>\n",
       "      <td>3.227728</td>\n",
       "      <td>2.456011</td>\n",
       "      <td>8.212264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>0</td>\n",
       "      <td>13.905019</td>\n",
       "      <td>16.145448</td>\n",
       "      <td>13.694255</td>\n",
       "      <td>12.297953</td>\n",
       "      <td>13.897956</td>\n",
       "      <td>12.847235</td>\n",
       "      <td>14.765581</td>\n",
       "      <td>11.922889</td>\n",
       "      <td>11.323538</td>\n",
       "      <td>...</td>\n",
       "      <td>4.284418</td>\n",
       "      <td>3.928457</td>\n",
       "      <td>4.388637</td>\n",
       "      <td>3.715859</td>\n",
       "      <td>3.550387</td>\n",
       "      <td>3.223100</td>\n",
       "      <td>6.573248</td>\n",
       "      <td>3.111999</td>\n",
       "      <td>2.169131</td>\n",
       "      <td>8.390142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>0</td>\n",
       "      <td>13.587752</td>\n",
       "      <td>15.693374</td>\n",
       "      <td>13.750024</td>\n",
       "      <td>12.287812</td>\n",
       "      <td>13.601792</td>\n",
       "      <td>12.299811</td>\n",
       "      <td>15.022294</td>\n",
       "      <td>11.774581</td>\n",
       "      <td>10.902013</td>\n",
       "      <td>...</td>\n",
       "      <td>4.558268</td>\n",
       "      <td>4.146181</td>\n",
       "      <td>4.304421</td>\n",
       "      <td>4.035720</td>\n",
       "      <td>3.363633</td>\n",
       "      <td>3.290479</td>\n",
       "      <td>6.652491</td>\n",
       "      <td>2.871030</td>\n",
       "      <td>2.530671</td>\n",
       "      <td>8.470635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>0</td>\n",
       "      <td>14.187646</td>\n",
       "      <td>15.470626</td>\n",
       "      <td>13.781765</td>\n",
       "      <td>12.357138</td>\n",
       "      <td>14.225409</td>\n",
       "      <td>14.940561</td>\n",
       "      <td>14.684765</td>\n",
       "      <td>11.810785</td>\n",
       "      <td>10.959663</td>\n",
       "      <td>...</td>\n",
       "      <td>4.419363</td>\n",
       "      <td>4.030334</td>\n",
       "      <td>4.324335</td>\n",
       "      <td>3.712206</td>\n",
       "      <td>3.342642</td>\n",
       "      <td>3.099874</td>\n",
       "      <td>6.938621</td>\n",
       "      <td>2.047146</td>\n",
       "      <td>2.173930</td>\n",
       "      <td>8.382205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>0</td>\n",
       "      <td>14.094374</td>\n",
       "      <td>15.391283</td>\n",
       "      <td>13.156052</td>\n",
       "      <td>12.299693</td>\n",
       "      <td>12.538918</td>\n",
       "      <td>12.572195</td>\n",
       "      <td>14.731909</td>\n",
       "      <td>11.818707</td>\n",
       "      <td>11.256166</td>\n",
       "      <td>...</td>\n",
       "      <td>4.325519</td>\n",
       "      <td>4.046423</td>\n",
       "      <td>4.316005</td>\n",
       "      <td>3.589466</td>\n",
       "      <td>3.665739</td>\n",
       "      <td>2.796427</td>\n",
       "      <td>6.780874</td>\n",
       "      <td>1.879761</td>\n",
       "      <td>2.061995</td>\n",
       "      <td>8.421293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>0</td>\n",
       "      <td>13.887737</td>\n",
       "      <td>15.651142</td>\n",
       "      <td>13.677427</td>\n",
       "      <td>12.106190</td>\n",
       "      <td>12.443795</td>\n",
       "      <td>12.495065</td>\n",
       "      <td>15.046157</td>\n",
       "      <td>11.758245</td>\n",
       "      <td>11.162599</td>\n",
       "      <td>...</td>\n",
       "      <td>4.312835</td>\n",
       "      <td>3.870939</td>\n",
       "      <td>4.421158</td>\n",
       "      <td>3.628347</td>\n",
       "      <td>3.223843</td>\n",
       "      <td>2.857471</td>\n",
       "      <td>6.622154</td>\n",
       "      <td>1.847846</td>\n",
       "      <td>2.536259</td>\n",
       "      <td>8.222391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>0</td>\n",
       "      <td>14.229909</td>\n",
       "      <td>15.692926</td>\n",
       "      <td>14.235475</td>\n",
       "      <td>12.749272</td>\n",
       "      <td>12.978876</td>\n",
       "      <td>11.558824</td>\n",
       "      <td>14.839368</td>\n",
       "      <td>11.590611</td>\n",
       "      <td>11.228472</td>\n",
       "      <td>...</td>\n",
       "      <td>4.354817</td>\n",
       "      <td>4.021493</td>\n",
       "      <td>4.472819</td>\n",
       "      <td>3.474888</td>\n",
       "      <td>3.376076</td>\n",
       "      <td>3.033125</td>\n",
       "      <td>7.116761</td>\n",
       "      <td>2.321552</td>\n",
       "      <td>2.765492</td>\n",
       "      <td>8.528619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>0</td>\n",
       "      <td>13.470893</td>\n",
       "      <td>15.776606</td>\n",
       "      <td>12.803612</td>\n",
       "      <td>11.874306</td>\n",
       "      <td>13.856436</td>\n",
       "      <td>11.851316</td>\n",
       "      <td>14.723356</td>\n",
       "      <td>11.897736</td>\n",
       "      <td>11.275317</td>\n",
       "      <td>...</td>\n",
       "      <td>4.427935</td>\n",
       "      <td>3.785503</td>\n",
       "      <td>4.406671</td>\n",
       "      <td>3.383026</td>\n",
       "      <td>3.345017</td>\n",
       "      <td>2.906815</td>\n",
       "      <td>6.176695</td>\n",
       "      <td>3.178995</td>\n",
       "      <td>2.031551</td>\n",
       "      <td>8.339872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>0</td>\n",
       "      <td>13.791416</td>\n",
       "      <td>15.660444</td>\n",
       "      <td>12.416035</td>\n",
       "      <td>11.807934</td>\n",
       "      <td>14.847563</td>\n",
       "      <td>12.695139</td>\n",
       "      <td>14.803905</td>\n",
       "      <td>11.960705</td>\n",
       "      <td>11.263613</td>\n",
       "      <td>...</td>\n",
       "      <td>4.397896</td>\n",
       "      <td>4.015200</td>\n",
       "      <td>4.228379</td>\n",
       "      <td>3.893781</td>\n",
       "      <td>3.607930</td>\n",
       "      <td>3.201855</td>\n",
       "      <td>6.518707</td>\n",
       "      <td>3.772191</td>\n",
       "      <td>2.185031</td>\n",
       "      <td>8.260333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>0</td>\n",
       "      <td>13.811276</td>\n",
       "      <td>15.877942</td>\n",
       "      <td>13.655066</td>\n",
       "      <td>12.005657</td>\n",
       "      <td>13.359262</td>\n",
       "      <td>12.686008</td>\n",
       "      <td>14.837980</td>\n",
       "      <td>12.442368</td>\n",
       "      <td>11.095155</td>\n",
       "      <td>...</td>\n",
       "      <td>4.588332</td>\n",
       "      <td>4.091954</td>\n",
       "      <td>4.249224</td>\n",
       "      <td>3.846444</td>\n",
       "      <td>3.804142</td>\n",
       "      <td>3.043344</td>\n",
       "      <td>6.796378</td>\n",
       "      <td>2.656716</td>\n",
       "      <td>2.359158</td>\n",
       "      <td>8.223552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>0</td>\n",
       "      <td>13.408338</td>\n",
       "      <td>15.825618</td>\n",
       "      <td>13.032500</td>\n",
       "      <td>12.104698</td>\n",
       "      <td>13.393705</td>\n",
       "      <td>12.255903</td>\n",
       "      <td>14.912439</td>\n",
       "      <td>12.352381</td>\n",
       "      <td>11.293471</td>\n",
       "      <td>...</td>\n",
       "      <td>4.452905</td>\n",
       "      <td>3.993252</td>\n",
       "      <td>4.394225</td>\n",
       "      <td>3.620936</td>\n",
       "      <td>3.243048</td>\n",
       "      <td>2.900005</td>\n",
       "      <td>6.472523</td>\n",
       "      <td>2.625084</td>\n",
       "      <td>2.334525</td>\n",
       "      <td>8.273688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>0</td>\n",
       "      <td>13.626238</td>\n",
       "      <td>15.683295</td>\n",
       "      <td>13.745549</td>\n",
       "      <td>12.419539</td>\n",
       "      <td>12.659798</td>\n",
       "      <td>12.046584</td>\n",
       "      <td>14.929399</td>\n",
       "      <td>12.618666</td>\n",
       "      <td>11.181892</td>\n",
       "      <td>...</td>\n",
       "      <td>4.474952</td>\n",
       "      <td>3.950093</td>\n",
       "      <td>4.426210</td>\n",
       "      <td>3.672063</td>\n",
       "      <td>3.510290</td>\n",
       "      <td>3.137008</td>\n",
       "      <td>6.686656</td>\n",
       "      <td>2.050419</td>\n",
       "      <td>2.167668</td>\n",
       "      <td>8.489779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>0</td>\n",
       "      <td>13.732388</td>\n",
       "      <td>15.614327</td>\n",
       "      <td>13.550485</td>\n",
       "      <td>12.214790</td>\n",
       "      <td>12.295163</td>\n",
       "      <td>11.849885</td>\n",
       "      <td>14.638246</td>\n",
       "      <td>12.493920</td>\n",
       "      <td>11.128544</td>\n",
       "      <td>...</td>\n",
       "      <td>4.355447</td>\n",
       "      <td>3.928209</td>\n",
       "      <td>4.096122</td>\n",
       "      <td>3.751882</td>\n",
       "      <td>3.610717</td>\n",
       "      <td>2.799030</td>\n",
       "      <td>6.593750</td>\n",
       "      <td>1.587524</td>\n",
       "      <td>2.198980</td>\n",
       "      <td>8.301010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>0</td>\n",
       "      <td>13.434239</td>\n",
       "      <td>15.599394</td>\n",
       "      <td>12.937875</td>\n",
       "      <td>11.949690</td>\n",
       "      <td>14.685660</td>\n",
       "      <td>12.535203</td>\n",
       "      <td>14.994927</td>\n",
       "      <td>12.625845</td>\n",
       "      <td>11.330293</td>\n",
       "      <td>...</td>\n",
       "      <td>4.281106</td>\n",
       "      <td>3.846768</td>\n",
       "      <td>4.413113</td>\n",
       "      <td>3.621619</td>\n",
       "      <td>3.889846</td>\n",
       "      <td>2.566960</td>\n",
       "      <td>6.572328</td>\n",
       "      <td>3.662519</td>\n",
       "      <td>2.141213</td>\n",
       "      <td>8.289688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>0</td>\n",
       "      <td>14.095026</td>\n",
       "      <td>15.480027</td>\n",
       "      <td>13.737106</td>\n",
       "      <td>12.276233</td>\n",
       "      <td>13.373069</td>\n",
       "      <td>12.241929</td>\n",
       "      <td>14.765373</td>\n",
       "      <td>12.465197</td>\n",
       "      <td>11.278841</td>\n",
       "      <td>...</td>\n",
       "      <td>4.338421</td>\n",
       "      <td>4.063158</td>\n",
       "      <td>4.435247</td>\n",
       "      <td>3.620287</td>\n",
       "      <td>3.450654</td>\n",
       "      <td>3.158188</td>\n",
       "      <td>6.861233</td>\n",
       "      <td>2.678659</td>\n",
       "      <td>2.204938</td>\n",
       "      <td>8.636030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>0</td>\n",
       "      <td>13.948679</td>\n",
       "      <td>15.707015</td>\n",
       "      <td>14.538745</td>\n",
       "      <td>12.775609</td>\n",
       "      <td>13.379634</td>\n",
       "      <td>12.260513</td>\n",
       "      <td>14.727783</td>\n",
       "      <td>12.237720</td>\n",
       "      <td>11.051486</td>\n",
       "      <td>...</td>\n",
       "      <td>4.564437</td>\n",
       "      <td>4.025111</td>\n",
       "      <td>4.309160</td>\n",
       "      <td>3.936639</td>\n",
       "      <td>3.564414</td>\n",
       "      <td>3.246105</td>\n",
       "      <td>6.974689</td>\n",
       "      <td>2.675655</td>\n",
       "      <td>2.335937</td>\n",
       "      <td>8.484635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>0</td>\n",
       "      <td>14.298264</td>\n",
       "      <td>15.355092</td>\n",
       "      <td>14.103120</td>\n",
       "      <td>12.766971</td>\n",
       "      <td>12.750475</td>\n",
       "      <td>11.651945</td>\n",
       "      <td>14.502873</td>\n",
       "      <td>12.194344</td>\n",
       "      <td>11.072816</td>\n",
       "      <td>...</td>\n",
       "      <td>4.346522</td>\n",
       "      <td>4.000151</td>\n",
       "      <td>3.883285</td>\n",
       "      <td>3.681679</td>\n",
       "      <td>3.724252</td>\n",
       "      <td>2.904710</td>\n",
       "      <td>7.030989</td>\n",
       "      <td>2.027413</td>\n",
       "      <td>2.646200</td>\n",
       "      <td>8.288551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>0</td>\n",
       "      <td>14.130082</td>\n",
       "      <td>15.892906</td>\n",
       "      <td>13.303000</td>\n",
       "      <td>12.037620</td>\n",
       "      <td>12.079807</td>\n",
       "      <td>12.701836</td>\n",
       "      <td>14.945113</td>\n",
       "      <td>12.122186</td>\n",
       "      <td>11.100334</td>\n",
       "      <td>...</td>\n",
       "      <td>4.205628</td>\n",
       "      <td>3.876827</td>\n",
       "      <td>4.403199</td>\n",
       "      <td>3.572489</td>\n",
       "      <td>3.592217</td>\n",
       "      <td>2.910169</td>\n",
       "      <td>6.929935</td>\n",
       "      <td>1.347397</td>\n",
       "      <td>2.130309</td>\n",
       "      <td>8.263916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>1</td>\n",
       "      <td>13.607465</td>\n",
       "      <td>15.742973</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11.700570</td>\n",
       "      <td>13.821512</td>\n",
       "      <td>13.444548</td>\n",
       "      <td>14.891810</td>\n",
       "      <td>11.082123</td>\n",
       "      <td>11.350869</td>\n",
       "      <td>...</td>\n",
       "      <td>4.577268</td>\n",
       "      <td>4.106763</td>\n",
       "      <td>4.567740</td>\n",
       "      <td>3.708711</td>\n",
       "      <td>3.606496</td>\n",
       "      <td>3.751611</td>\n",
       "      <td>7.213011</td>\n",
       "      <td>3.029353</td>\n",
       "      <td>2.386811</td>\n",
       "      <td>8.446067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>1</td>\n",
       "      <td>14.074076</td>\n",
       "      <td>15.550370</td>\n",
       "      <td>14.360856</td>\n",
       "      <td>12.789551</td>\n",
       "      <td>12.534986</td>\n",
       "      <td>13.780358</td>\n",
       "      <td>14.695148</td>\n",
       "      <td>11.888787</td>\n",
       "      <td>11.228459</td>\n",
       "      <td>...</td>\n",
       "      <td>4.291381</td>\n",
       "      <td>4.077516</td>\n",
       "      <td>4.344123</td>\n",
       "      <td>3.585938</td>\n",
       "      <td>3.161292</td>\n",
       "      <td>3.124340</td>\n",
       "      <td>6.830208</td>\n",
       "      <td>1.628613</td>\n",
       "      <td>2.728490</td>\n",
       "      <td>8.435363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>1</td>\n",
       "      <td>13.608710</td>\n",
       "      <td>15.620534</td>\n",
       "      <td>13.926261</td>\n",
       "      <td>11.985488</td>\n",
       "      <td>13.138026</td>\n",
       "      <td>13.473093</td>\n",
       "      <td>14.805259</td>\n",
       "      <td>12.167358</td>\n",
       "      <td>10.928987</td>\n",
       "      <td>...</td>\n",
       "      <td>4.285596</td>\n",
       "      <td>3.863159</td>\n",
       "      <td>4.289512</td>\n",
       "      <td>3.555084</td>\n",
       "      <td>3.317755</td>\n",
       "      <td>3.128143</td>\n",
       "      <td>6.305648</td>\n",
       "      <td>2.296785</td>\n",
       "      <td>2.157262</td>\n",
       "      <td>8.497522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>1</td>\n",
       "      <td>13.912395</td>\n",
       "      <td>15.710430</td>\n",
       "      <td>12.546500</td>\n",
       "      <td>11.658677</td>\n",
       "      <td>12.798666</td>\n",
       "      <td>12.761682</td>\n",
       "      <td>14.730217</td>\n",
       "      <td>12.118298</td>\n",
       "      <td>10.966498</td>\n",
       "      <td>...</td>\n",
       "      <td>4.480622</td>\n",
       "      <td>4.158515</td>\n",
       "      <td>4.632123</td>\n",
       "      <td>3.660860</td>\n",
       "      <td>3.665273</td>\n",
       "      <td>3.127071</td>\n",
       "      <td>6.749542</td>\n",
       "      <td>1.977932</td>\n",
       "      <td>2.284340</td>\n",
       "      <td>8.449097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>1</td>\n",
       "      <td>14.272681</td>\n",
       "      <td>15.398227</td>\n",
       "      <td>13.917771</td>\n",
       "      <td>12.557395</td>\n",
       "      <td>13.113209</td>\n",
       "      <td>13.035004</td>\n",
       "      <td>14.700396</td>\n",
       "      <td>12.477438</td>\n",
       "      <td>10.884966</td>\n",
       "      <td>...</td>\n",
       "      <td>4.450148</td>\n",
       "      <td>4.136141</td>\n",
       "      <td>4.556577</td>\n",
       "      <td>3.851915</td>\n",
       "      <td>3.734718</td>\n",
       "      <td>3.369579</td>\n",
       "      <td>6.911310</td>\n",
       "      <td>2.358397</td>\n",
       "      <td>2.501465</td>\n",
       "      <td>8.431409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>1</td>\n",
       "      <td>13.553682</td>\n",
       "      <td>15.332108</td>\n",
       "      <td>12.975258</td>\n",
       "      <td>12.172884</td>\n",
       "      <td>13.984951</td>\n",
       "      <td>12.718311</td>\n",
       "      <td>14.723451</td>\n",
       "      <td>11.626854</td>\n",
       "      <td>11.235005</td>\n",
       "      <td>...</td>\n",
       "      <td>4.254776</td>\n",
       "      <td>3.911283</td>\n",
       "      <td>4.286570</td>\n",
       "      <td>3.484795</td>\n",
       "      <td>3.369478</td>\n",
       "      <td>3.082252</td>\n",
       "      <td>6.165783</td>\n",
       "      <td>3.179794</td>\n",
       "      <td>1.858448</td>\n",
       "      <td>8.375820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>1</td>\n",
       "      <td>13.503647</td>\n",
       "      <td>15.798457</td>\n",
       "      <td>13.363521</td>\n",
       "      <td>12.329034</td>\n",
       "      <td>12.712400</td>\n",
       "      <td>13.083894</td>\n",
       "      <td>14.792074</td>\n",
       "      <td>12.206497</td>\n",
       "      <td>11.103387</td>\n",
       "      <td>...</td>\n",
       "      <td>4.456378</td>\n",
       "      <td>4.022867</td>\n",
       "      <td>4.650120</td>\n",
       "      <td>3.656073</td>\n",
       "      <td>3.353292</td>\n",
       "      <td>3.435832</td>\n",
       "      <td>6.318271</td>\n",
       "      <td>1.908500</td>\n",
       "      <td>2.339160</td>\n",
       "      <td>8.278816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>1</td>\n",
       "      <td>13.185622</td>\n",
       "      <td>15.649060</td>\n",
       "      <td>13.128470</td>\n",
       "      <td>12.195693</td>\n",
       "      <td>14.377665</td>\n",
       "      <td>15.325704</td>\n",
       "      <td>14.478469</td>\n",
       "      <td>12.226616</td>\n",
       "      <td>11.094455</td>\n",
       "      <td>...</td>\n",
       "      <td>4.278459</td>\n",
       "      <td>3.892099</td>\n",
       "      <td>4.119692</td>\n",
       "      <td>3.115186</td>\n",
       "      <td>3.187931</td>\n",
       "      <td>3.122705</td>\n",
       "      <td>6.074876</td>\n",
       "      <td>3.341021</td>\n",
       "      <td>1.845781</td>\n",
       "      <td>8.399841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>1</td>\n",
       "      <td>13.855509</td>\n",
       "      <td>15.827115</td>\n",
       "      <td>13.265267</td>\n",
       "      <td>11.844197</td>\n",
       "      <td>15.126767</td>\n",
       "      <td>17.093881</td>\n",
       "      <td>15.067586</td>\n",
       "      <td>12.164017</td>\n",
       "      <td>11.151177</td>\n",
       "      <td>...</td>\n",
       "      <td>4.527473</td>\n",
       "      <td>4.084901</td>\n",
       "      <td>4.305352</td>\n",
       "      <td>3.930183</td>\n",
       "      <td>3.063406</td>\n",
       "      <td>3.838334</td>\n",
       "      <td>6.857591</td>\n",
       "      <td>2.673680</td>\n",
       "      <td>2.548548</td>\n",
       "      <td>8.398240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>1</td>\n",
       "      <td>13.542743</td>\n",
       "      <td>15.697666</td>\n",
       "      <td>14.144507</td>\n",
       "      <td>12.567397</td>\n",
       "      <td>12.971915</td>\n",
       "      <td>14.868657</td>\n",
       "      <td>14.950939</td>\n",
       "      <td>12.337996</td>\n",
       "      <td>11.316903</td>\n",
       "      <td>...</td>\n",
       "      <td>4.316029</td>\n",
       "      <td>4.155610</td>\n",
       "      <td>4.251034</td>\n",
       "      <td>3.623233</td>\n",
       "      <td>3.400403</td>\n",
       "      <td>3.573865</td>\n",
       "      <td>6.495785</td>\n",
       "      <td>1.865180</td>\n",
       "      <td>2.135729</td>\n",
       "      <td>8.237084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>1</td>\n",
       "      <td>13.995393</td>\n",
       "      <td>15.763045</td>\n",
       "      <td>13.422012</td>\n",
       "      <td>12.303995</td>\n",
       "      <td>13.350982</td>\n",
       "      <td>13.343193</td>\n",
       "      <td>14.944149</td>\n",
       "      <td>12.346532</td>\n",
       "      <td>11.353562</td>\n",
       "      <td>...</td>\n",
       "      <td>4.557091</td>\n",
       "      <td>4.191058</td>\n",
       "      <td>4.233932</td>\n",
       "      <td>4.075762</td>\n",
       "      <td>3.850118</td>\n",
       "      <td>3.194136</td>\n",
       "      <td>6.841580</td>\n",
       "      <td>2.551983</td>\n",
       "      <td>2.417319</td>\n",
       "      <td>8.467316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>1</td>\n",
       "      <td>13.526921</td>\n",
       "      <td>15.583914</td>\n",
       "      <td>15.236153</td>\n",
       "      <td>13.343259</td>\n",
       "      <td>14.652109</td>\n",
       "      <td>14.664571</td>\n",
       "      <td>14.872883</td>\n",
       "      <td>12.148834</td>\n",
       "      <td>11.120825</td>\n",
       "      <td>...</td>\n",
       "      <td>4.480357</td>\n",
       "      <td>3.970858</td>\n",
       "      <td>4.432734</td>\n",
       "      <td>3.383253</td>\n",
       "      <td>3.350181</td>\n",
       "      <td>3.313648</td>\n",
       "      <td>6.379664</td>\n",
       "      <td>3.679456</td>\n",
       "      <td>2.722867</td>\n",
       "      <td>8.486501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>1</td>\n",
       "      <td>14.098327</td>\n",
       "      <td>15.572972</td>\n",
       "      <td>13.839735</td>\n",
       "      <td>12.908054</td>\n",
       "      <td>12.830078</td>\n",
       "      <td>14.253377</td>\n",
       "      <td>14.688208</td>\n",
       "      <td>12.091903</td>\n",
       "      <td>11.033918</td>\n",
       "      <td>...</td>\n",
       "      <td>4.492121</td>\n",
       "      <td>4.102819</td>\n",
       "      <td>4.716647</td>\n",
       "      <td>3.649471</td>\n",
       "      <td>3.322210</td>\n",
       "      <td>3.487048</td>\n",
       "      <td>7.053267</td>\n",
       "      <td>1.250891</td>\n",
       "      <td>2.501105</td>\n",
       "      <td>8.518012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>1</td>\n",
       "      <td>14.237930</td>\n",
       "      <td>15.678887</td>\n",
       "      <td>13.976978</td>\n",
       "      <td>12.430527</td>\n",
       "      <td>12.674354</td>\n",
       "      <td>13.679608</td>\n",
       "      <td>14.850197</td>\n",
       "      <td>11.542836</td>\n",
       "      <td>11.096112</td>\n",
       "      <td>...</td>\n",
       "      <td>4.369750</td>\n",
       "      <td>3.938524</td>\n",
       "      <td>4.500298</td>\n",
       "      <td>3.366916</td>\n",
       "      <td>3.343465</td>\n",
       "      <td>3.536378</td>\n",
       "      <td>7.527530</td>\n",
       "      <td>1.756122</td>\n",
       "      <td>2.449833</td>\n",
       "      <td>8.363992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>1</td>\n",
       "      <td>13.970157</td>\n",
       "      <td>16.027721</td>\n",
       "      <td>14.244315</td>\n",
       "      <td>13.328620</td>\n",
       "      <td>13.067405</td>\n",
       "      <td>15.250113</td>\n",
       "      <td>14.968732</td>\n",
       "      <td>11.894993</td>\n",
       "      <td>11.028224</td>\n",
       "      <td>...</td>\n",
       "      <td>4.656870</td>\n",
       "      <td>4.185054</td>\n",
       "      <td>4.529080</td>\n",
       "      <td>3.866081</td>\n",
       "      <td>3.421321</td>\n",
       "      <td>3.643749</td>\n",
       "      <td>6.958296</td>\n",
       "      <td>1.745003</td>\n",
       "      <td>2.890356</td>\n",
       "      <td>8.400514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>1</td>\n",
       "      <td>13.794663</td>\n",
       "      <td>15.445801</td>\n",
       "      <td>13.475055</td>\n",
       "      <td>12.279256</td>\n",
       "      <td>13.653491</td>\n",
       "      <td>13.258665</td>\n",
       "      <td>14.796514</td>\n",
       "      <td>11.680741</td>\n",
       "      <td>11.125568</td>\n",
       "      <td>...</td>\n",
       "      <td>4.321101</td>\n",
       "      <td>4.044857</td>\n",
       "      <td>4.150974</td>\n",
       "      <td>3.804070</td>\n",
       "      <td>3.573456</td>\n",
       "      <td>3.498864</td>\n",
       "      <td>6.469232</td>\n",
       "      <td>2.840020</td>\n",
       "      <td>2.767087</td>\n",
       "      <td>8.474646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>1</td>\n",
       "      <td>14.103663</td>\n",
       "      <td>15.568095</td>\n",
       "      <td>13.472755</td>\n",
       "      <td>12.202148</td>\n",
       "      <td>12.950661</td>\n",
       "      <td>14.140104</td>\n",
       "      <td>14.573157</td>\n",
       "      <td>11.537200</td>\n",
       "      <td>11.256968</td>\n",
       "      <td>...</td>\n",
       "      <td>4.331688</td>\n",
       "      <td>4.076661</td>\n",
       "      <td>4.417399</td>\n",
       "      <td>3.274718</td>\n",
       "      <td>3.111471</td>\n",
       "      <td>2.990823</td>\n",
       "      <td>6.765249</td>\n",
       "      <td>1.777345</td>\n",
       "      <td>2.373036</td>\n",
       "      <td>8.395026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>1</td>\n",
       "      <td>13.720616</td>\n",
       "      <td>15.587086</td>\n",
       "      <td>13.224181</td>\n",
       "      <td>11.946611</td>\n",
       "      <td>12.845243</td>\n",
       "      <td>13.753623</td>\n",
       "      <td>14.738364</td>\n",
       "      <td>11.998374</td>\n",
       "      <td>11.070418</td>\n",
       "      <td>...</td>\n",
       "      <td>4.387599</td>\n",
       "      <td>3.933511</td>\n",
       "      <td>4.246532</td>\n",
       "      <td>3.337598</td>\n",
       "      <td>3.197950</td>\n",
       "      <td>3.228279</td>\n",
       "      <td>6.869973</td>\n",
       "      <td>1.822230</td>\n",
       "      <td>2.099936</td>\n",
       "      <td>8.476514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>1</td>\n",
       "      <td>13.336426</td>\n",
       "      <td>15.462282</td>\n",
       "      <td>13.441044</td>\n",
       "      <td>12.612899</td>\n",
       "      <td>14.526108</td>\n",
       "      <td>13.405522</td>\n",
       "      <td>14.843443</td>\n",
       "      <td>11.952914</td>\n",
       "      <td>11.006261</td>\n",
       "      <td>...</td>\n",
       "      <td>4.293580</td>\n",
       "      <td>3.946865</td>\n",
       "      <td>4.445984</td>\n",
       "      <td>3.336494</td>\n",
       "      <td>3.304540</td>\n",
       "      <td>3.584052</td>\n",
       "      <td>6.367939</td>\n",
       "      <td>3.595970</td>\n",
       "      <td>2.855985</td>\n",
       "      <td>8.215932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>1</td>\n",
       "      <td>13.712649</td>\n",
       "      <td>15.710702</td>\n",
       "      <td>13.450497</td>\n",
       "      <td>12.085055</td>\n",
       "      <td>13.107520</td>\n",
       "      <td>13.235522</td>\n",
       "      <td>14.892700</td>\n",
       "      <td>12.352521</td>\n",
       "      <td>11.340161</td>\n",
       "      <td>...</td>\n",
       "      <td>4.394059</td>\n",
       "      <td>3.957019</td>\n",
       "      <td>4.286366</td>\n",
       "      <td>3.667046</td>\n",
       "      <td>3.606033</td>\n",
       "      <td>3.343059</td>\n",
       "      <td>6.648593</td>\n",
       "      <td>2.106706</td>\n",
       "      <td>2.409483</td>\n",
       "      <td>8.380304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>1</td>\n",
       "      <td>14.136347</td>\n",
       "      <td>15.646664</td>\n",
       "      <td>13.707184</td>\n",
       "      <td>12.397715</td>\n",
       "      <td>12.885362</td>\n",
       "      <td>12.590245</td>\n",
       "      <td>14.732140</td>\n",
       "      <td>12.206637</td>\n",
       "      <td>11.300098</td>\n",
       "      <td>...</td>\n",
       "      <td>4.541126</td>\n",
       "      <td>4.073425</td>\n",
       "      <td>4.413411</td>\n",
       "      <td>3.980865</td>\n",
       "      <td>3.601304</td>\n",
       "      <td>3.262284</td>\n",
       "      <td>7.191626</td>\n",
       "      <td>2.070683</td>\n",
       "      <td>2.418890</td>\n",
       "      <td>8.449288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>1</td>\n",
       "      <td>13.877760</td>\n",
       "      <td>15.865054</td>\n",
       "      <td>13.652736</td>\n",
       "      <td>11.859613</td>\n",
       "      <td>12.580436</td>\n",
       "      <td>13.614601</td>\n",
       "      <td>14.953066</td>\n",
       "      <td>12.419737</td>\n",
       "      <td>11.253510</td>\n",
       "      <td>...</td>\n",
       "      <td>4.526731</td>\n",
       "      <td>4.229098</td>\n",
       "      <td>4.611527</td>\n",
       "      <td>3.941546</td>\n",
       "      <td>3.489113</td>\n",
       "      <td>3.304206</td>\n",
       "      <td>6.659048</td>\n",
       "      <td>1.688147</td>\n",
       "      <td>2.160634</td>\n",
       "      <td>8.381040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>1</td>\n",
       "      <td>13.101075</td>\n",
       "      <td>15.563113</td>\n",
       "      <td>14.134044</td>\n",
       "      <td>12.751887</td>\n",
       "      <td>14.329279</td>\n",
       "      <td>13.317395</td>\n",
       "      <td>14.881287</td>\n",
       "      <td>12.050647</td>\n",
       "      <td>10.993768</td>\n",
       "      <td>...</td>\n",
       "      <td>4.366838</td>\n",
       "      <td>3.966853</td>\n",
       "      <td>4.288859</td>\n",
       "      <td>3.387935</td>\n",
       "      <td>3.194383</td>\n",
       "      <td>3.073232</td>\n",
       "      <td>6.376179</td>\n",
       "      <td>3.425897</td>\n",
       "      <td>2.577070</td>\n",
       "      <td>8.330042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>1</td>\n",
       "      <td>13.699883</td>\n",
       "      <td>15.633531</td>\n",
       "      <td>13.078100</td>\n",
       "      <td>11.848265</td>\n",
       "      <td>14.006595</td>\n",
       "      <td>12.877403</td>\n",
       "      <td>14.893908</td>\n",
       "      <td>12.019573</td>\n",
       "      <td>10.943293</td>\n",
       "      <td>...</td>\n",
       "      <td>4.414336</td>\n",
       "      <td>4.232457</td>\n",
       "      <td>4.329651</td>\n",
       "      <td>3.760205</td>\n",
       "      <td>3.630793</td>\n",
       "      <td>3.029616</td>\n",
       "      <td>6.743550</td>\n",
       "      <td>3.208075</td>\n",
       "      <td>2.329893</td>\n",
       "      <td>8.449347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>1</td>\n",
       "      <td>13.769740</td>\n",
       "      <td>15.617562</td>\n",
       "      <td>13.983691</td>\n",
       "      <td>12.404641</td>\n",
       "      <td>14.108590</td>\n",
       "      <td>13.666401</td>\n",
       "      <td>14.984438</td>\n",
       "      <td>12.114248</td>\n",
       "      <td>11.192342</td>\n",
       "      <td>...</td>\n",
       "      <td>4.341897</td>\n",
       "      <td>3.983024</td>\n",
       "      <td>4.571687</td>\n",
       "      <td>3.522884</td>\n",
       "      <td>3.649491</td>\n",
       "      <td>3.605751</td>\n",
       "      <td>6.585925</td>\n",
       "      <td>3.288916</td>\n",
       "      <td>2.731427</td>\n",
       "      <td>8.495185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>1</td>\n",
       "      <td>13.881991</td>\n",
       "      <td>15.634942</td>\n",
       "      <td>12.687261</td>\n",
       "      <td>11.649048</td>\n",
       "      <td>12.839689</td>\n",
       "      <td>12.596961</td>\n",
       "      <td>14.787885</td>\n",
       "      <td>11.974167</td>\n",
       "      <td>11.090949</td>\n",
       "      <td>...</td>\n",
       "      <td>4.269239</td>\n",
       "      <td>4.029189</td>\n",
       "      <td>4.106445</td>\n",
       "      <td>3.719126</td>\n",
       "      <td>3.212139</td>\n",
       "      <td>3.381057</td>\n",
       "      <td>7.102711</td>\n",
       "      <td>2.093669</td>\n",
       "      <td>2.238217</td>\n",
       "      <td>8.352757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>1</td>\n",
       "      <td>13.911391</td>\n",
       "      <td>15.808535</td>\n",
       "      <td>13.255651</td>\n",
       "      <td>12.072060</td>\n",
       "      <td>12.829308</td>\n",
       "      <td>12.518926</td>\n",
       "      <td>14.914205</td>\n",
       "      <td>12.009783</td>\n",
       "      <td>11.241416</td>\n",
       "      <td>...</td>\n",
       "      <td>4.570584</td>\n",
       "      <td>3.935102</td>\n",
       "      <td>4.676844</td>\n",
       "      <td>3.735049</td>\n",
       "      <td>3.427296</td>\n",
       "      <td>3.121327</td>\n",
       "      <td>6.751464</td>\n",
       "      <td>2.099744</td>\n",
       "      <td>2.460060</td>\n",
       "      <td>8.268259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>1</td>\n",
       "      <td>14.004190</td>\n",
       "      <td>15.578057</td>\n",
       "      <td>15.050097</td>\n",
       "      <td>12.955338</td>\n",
       "      <td>12.021058</td>\n",
       "      <td>12.698247</td>\n",
       "      <td>14.838858</td>\n",
       "      <td>11.864216</td>\n",
       "      <td>11.361720</td>\n",
       "      <td>...</td>\n",
       "      <td>4.423799</td>\n",
       "      <td>4.169760</td>\n",
       "      <td>4.259914</td>\n",
       "      <td>3.433266</td>\n",
       "      <td>3.317550</td>\n",
       "      <td>3.491284</td>\n",
       "      <td>7.009974</td>\n",
       "      <td>1.215855</td>\n",
       "      <td>2.716850</td>\n",
       "      <td>8.382161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>1</td>\n",
       "      <td>13.782098</td>\n",
       "      <td>15.628488</td>\n",
       "      <td>13.800509</td>\n",
       "      <td>12.324283</td>\n",
       "      <td>13.557521</td>\n",
       "      <td>13.930284</td>\n",
       "      <td>15.010427</td>\n",
       "      <td>11.877001</td>\n",
       "      <td>11.293042</td>\n",
       "      <td>...</td>\n",
       "      <td>4.538909</td>\n",
       "      <td>3.842126</td>\n",
       "      <td>4.282809</td>\n",
       "      <td>3.307526</td>\n",
       "      <td>2.588503</td>\n",
       "      <td>3.388397</td>\n",
       "      <td>6.825325</td>\n",
       "      <td>2.567097</td>\n",
       "      <td>2.339410</td>\n",
       "      <td>8.362965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>1</td>\n",
       "      <td>13.841570</td>\n",
       "      <td>15.697214</td>\n",
       "      <td>14.467861</td>\n",
       "      <td>12.549533</td>\n",
       "      <td>13.157717</td>\n",
       "      <td>14.309882</td>\n",
       "      <td>14.518098</td>\n",
       "      <td>12.173317</td>\n",
       "      <td>11.058203</td>\n",
       "      <td>...</td>\n",
       "      <td>4.371212</td>\n",
       "      <td>3.780527</td>\n",
       "      <td>4.339276</td>\n",
       "      <td>3.554490</td>\n",
       "      <td>3.071197</td>\n",
       "      <td>3.074653</td>\n",
       "      <td>6.561956</td>\n",
       "      <td>1.637428</td>\n",
       "      <td>2.147184</td>\n",
       "      <td>8.379108</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>92 rows × 125 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Label   Pyruvate  MethylSuccinate  Phosphoglyceric Acid  \\\n",
       "20       0  14.158545        15.624522             13.383600   \n",
       "24       0  14.180287        15.325665             13.468363   \n",
       "29       0  13.921329        16.156867             14.615061   \n",
       "52       0  13.785605        15.939535             13.206615   \n",
       "54       0  13.674234        15.743668             11.542008   \n",
       "56       0  13.447093        15.595200             13.118302   \n",
       "57       0  13.616625        15.406841             14.124967   \n",
       "58       0  13.731086        15.449381             13.108050   \n",
       "59       0  14.468678        15.438259             13.737836   \n",
       "60       0  13.589152        15.895696             13.992094   \n",
       "61       0  14.124827        15.667980             13.885415   \n",
       "62       0  14.125212        15.848679             14.611221   \n",
       "63       0  13.911841        15.770673             13.616139   \n",
       "66       0  13.905019        16.145448             13.694255   \n",
       "67       0  13.587752        15.693374             13.750024   \n",
       "68       0  14.187646        15.470626             13.781765   \n",
       "69       0  14.094374        15.391283             13.156052   \n",
       "70       0  13.887737        15.651142             13.677427   \n",
       "71       0  14.229909        15.692926             14.235475   \n",
       "72       0  13.470893        15.776606             12.803612   \n",
       "73       0  13.791416        15.660444             12.416035   \n",
       "75       0  13.811276        15.877942             13.655066   \n",
       "78       0  13.408338        15.825618             13.032500   \n",
       "79       0  13.626238        15.683295             13.745549   \n",
       "80       0  13.732388        15.614327             13.550485   \n",
       "82       0  13.434239        15.599394             12.937875   \n",
       "83       0  14.095026        15.480027             13.737106   \n",
       "84       0  13.948679        15.707015             14.538745   \n",
       "85       0  14.298264        15.355092             14.103120   \n",
       "86       0  14.130082        15.892906             13.303000   \n",
       "..     ...        ...              ...                   ...   \n",
       "33       1  13.607465        15.742973                   NaN   \n",
       "34       1  14.074076        15.550370             14.360856   \n",
       "36       1  13.608710        15.620534             13.926261   \n",
       "37       1  13.912395        15.710430             12.546500   \n",
       "39       1  14.272681        15.398227             13.917771   \n",
       "40       1  13.553682        15.332108             12.975258   \n",
       "41       1  13.503647        15.798457             13.363521   \n",
       "42       1  13.185622        15.649060             13.128470   \n",
       "43       1  13.855509        15.827115             13.265267   \n",
       "44       1  13.542743        15.697666             14.144507   \n",
       "45       1  13.995393        15.763045             13.422012   \n",
       "46       1  13.526921        15.583914             15.236153   \n",
       "47       1  14.098327        15.572972             13.839735   \n",
       "48       1  14.237930        15.678887             13.976978   \n",
       "49       1  13.970157        16.027721             14.244315   \n",
       "53       1  13.794663        15.445801             13.475055   \n",
       "55       1  14.103663        15.568095             13.472755   \n",
       "64       1  13.720616        15.587086             13.224181   \n",
       "65       1  13.336426        15.462282             13.441044   \n",
       "76       1  13.712649        15.710702             13.450497   \n",
       "77       1  14.136347        15.646664             13.707184   \n",
       "81       1  13.877760        15.865054             13.652736   \n",
       "88       1  13.101075        15.563113             14.134044   \n",
       "89       1  13.699883        15.633531             13.078100   \n",
       "98       1  13.769740        15.617562             13.983691   \n",
       "99       1  13.881991        15.634942             12.687261   \n",
       "100      1  13.911391        15.808535             13.255651   \n",
       "101      1  14.004190        15.578057             15.050097   \n",
       "102      1  13.782098        15.628488             13.800509   \n",
       "103      1  13.841570        15.697214             14.467861   \n",
       "\n",
       "     Glucose 1-phosphate  Malonic Acid  Fumaric Acid  Alpha-Ketoglutaric Acid  \\\n",
       "20             11.923638     13.933560     12.787696                14.981440   \n",
       "24             12.046294     13.043221     12.536227                14.593418   \n",
       "29             13.051929     13.381985     12.419600                14.701978   \n",
       "52             12.342156     12.448068     12.211098                14.860913   \n",
       "54             11.807872     14.582068     11.635154                14.970451   \n",
       "56             12.002027     12.817962     12.065246                14.796780   \n",
       "57             12.499758     13.168693     12.235455                14.721283   \n",
       "58             11.948349     14.492458     12.008819                14.727796   \n",
       "59             12.204044     12.950250     12.466876                14.826535   \n",
       "60             12.260070     12.513580     12.256111                14.934873   \n",
       "61             12.434999     12.962366     12.152578                14.616602   \n",
       "62             12.768474     12.380620     12.660110                14.852526   \n",
       "63             12.200233     13.897249     12.203694                14.965501   \n",
       "66             12.297953     13.897956     12.847235                14.765581   \n",
       "67             12.287812     13.601792     12.299811                15.022294   \n",
       "68             12.357138     14.225409     14.940561                14.684765   \n",
       "69             12.299693     12.538918     12.572195                14.731909   \n",
       "70             12.106190     12.443795     12.495065                15.046157   \n",
       "71             12.749272     12.978876     11.558824                14.839368   \n",
       "72             11.874306     13.856436     11.851316                14.723356   \n",
       "73             11.807934     14.847563     12.695139                14.803905   \n",
       "75             12.005657     13.359262     12.686008                14.837980   \n",
       "78             12.104698     13.393705     12.255903                14.912439   \n",
       "79             12.419539     12.659798     12.046584                14.929399   \n",
       "80             12.214790     12.295163     11.849885                14.638246   \n",
       "82             11.949690     14.685660     12.535203                14.994927   \n",
       "83             12.276233     13.373069     12.241929                14.765373   \n",
       "84             12.775609     13.379634     12.260513                14.727783   \n",
       "85             12.766971     12.750475     11.651945                14.502873   \n",
       "86             12.037620     12.079807     12.701836                14.945113   \n",
       "..                   ...           ...           ...                      ...   \n",
       "33             11.700570     13.821512     13.444548                14.891810   \n",
       "34             12.789551     12.534986     13.780358                14.695148   \n",
       "36             11.985488     13.138026     13.473093                14.805259   \n",
       "37             11.658677     12.798666     12.761682                14.730217   \n",
       "39             12.557395     13.113209     13.035004                14.700396   \n",
       "40             12.172884     13.984951     12.718311                14.723451   \n",
       "41             12.329034     12.712400     13.083894                14.792074   \n",
       "42             12.195693     14.377665     15.325704                14.478469   \n",
       "43             11.844197     15.126767     17.093881                15.067586   \n",
       "44             12.567397     12.971915     14.868657                14.950939   \n",
       "45             12.303995     13.350982     13.343193                14.944149   \n",
       "46             13.343259     14.652109     14.664571                14.872883   \n",
       "47             12.908054     12.830078     14.253377                14.688208   \n",
       "48             12.430527     12.674354     13.679608                14.850197   \n",
       "49             13.328620     13.067405     15.250113                14.968732   \n",
       "53             12.279256     13.653491     13.258665                14.796514   \n",
       "55             12.202148     12.950661     14.140104                14.573157   \n",
       "64             11.946611     12.845243     13.753623                14.738364   \n",
       "65             12.612899     14.526108     13.405522                14.843443   \n",
       "76             12.085055     13.107520     13.235522                14.892700   \n",
       "77             12.397715     12.885362     12.590245                14.732140   \n",
       "81             11.859613     12.580436     13.614601                14.953066   \n",
       "88             12.751887     14.329279     13.317395                14.881287   \n",
       "89             11.848265     14.006595     12.877403                14.893908   \n",
       "98             12.404641     14.108590     13.666401                14.984438   \n",
       "99             11.649048     12.839689     12.596961                14.787885   \n",
       "100            12.072060     12.829308     12.518926                14.914205   \n",
       "101            12.955338     12.021058     12.698247                14.838858   \n",
       "102            12.324283     13.557521     13.930284                15.010427   \n",
       "103            12.549533     13.157717     14.309882                14.518098   \n",
       "\n",
       "     Sarcosine  Cadaverine  ...  Histidine  Phenylalanine  Arginine  Tyrosine  \\\n",
       "20   11.739484   10.921776  ...   4.439493       3.934856  4.362485  3.125658   \n",
       "24   11.833211   11.121865  ...   4.091856       3.799364  4.055512  3.812430   \n",
       "29   12.232070   11.025333  ...   4.647577       4.217314  4.789008  3.910396   \n",
       "52   11.949448   11.015657  ...   4.531611       4.166923  4.573985  3.953711   \n",
       "54   11.457569   11.103239  ...   4.551687       3.767357  4.449439  3.553009   \n",
       "56   11.831409   11.219337  ...   4.473621       3.847232  4.282759  3.600317   \n",
       "57   11.691816   11.189055  ...   4.377434       4.196924  4.341699  3.955967   \n",
       "58   11.733394   10.864286  ...   4.315900       3.956130  4.369007  3.808282   \n",
       "59   12.037085   11.032339  ...   4.233620       3.847929  4.498490  3.642743   \n",
       "60   11.807985   10.975466  ...   4.435110       4.002727  4.540823  3.568290   \n",
       "61   11.767767   10.930519  ...   4.428947       4.017820  4.510121  4.218307   \n",
       "62   12.026815   10.841767  ...   4.533552       4.057604  4.142390  3.918772   \n",
       "63   11.984423   11.260172  ...   4.474527       4.037906  4.546264  3.504973   \n",
       "66   11.922889   11.323538  ...   4.284418       3.928457  4.388637  3.715859   \n",
       "67   11.774581   10.902013  ...   4.558268       4.146181  4.304421  4.035720   \n",
       "68   11.810785   10.959663  ...   4.419363       4.030334  4.324335  3.712206   \n",
       "69   11.818707   11.256166  ...   4.325519       4.046423  4.316005  3.589466   \n",
       "70   11.758245   11.162599  ...   4.312835       3.870939  4.421158  3.628347   \n",
       "71   11.590611   11.228472  ...   4.354817       4.021493  4.472819  3.474888   \n",
       "72   11.897736   11.275317  ...   4.427935       3.785503  4.406671  3.383026   \n",
       "73   11.960705   11.263613  ...   4.397896       4.015200  4.228379  3.893781   \n",
       "75   12.442368   11.095155  ...   4.588332       4.091954  4.249224  3.846444   \n",
       "78   12.352381   11.293471  ...   4.452905       3.993252  4.394225  3.620936   \n",
       "79   12.618666   11.181892  ...   4.474952       3.950093  4.426210  3.672063   \n",
       "80   12.493920   11.128544  ...   4.355447       3.928209  4.096122  3.751882   \n",
       "82   12.625845   11.330293  ...   4.281106       3.846768  4.413113  3.621619   \n",
       "83   12.465197   11.278841  ...   4.338421       4.063158  4.435247  3.620287   \n",
       "84   12.237720   11.051486  ...   4.564437       4.025111  4.309160  3.936639   \n",
       "85   12.194344   11.072816  ...   4.346522       4.000151  3.883285  3.681679   \n",
       "86   12.122186   11.100334  ...   4.205628       3.876827  4.403199  3.572489   \n",
       "..         ...         ...  ...        ...            ...       ...       ...   \n",
       "33   11.082123   11.350869  ...   4.577268       4.106763  4.567740  3.708711   \n",
       "34   11.888787   11.228459  ...   4.291381       4.077516  4.344123  3.585938   \n",
       "36   12.167358   10.928987  ...   4.285596       3.863159  4.289512  3.555084   \n",
       "37   12.118298   10.966498  ...   4.480622       4.158515  4.632123  3.660860   \n",
       "39   12.477438   10.884966  ...   4.450148       4.136141  4.556577  3.851915   \n",
       "40   11.626854   11.235005  ...   4.254776       3.911283  4.286570  3.484795   \n",
       "41   12.206497   11.103387  ...   4.456378       4.022867  4.650120  3.656073   \n",
       "42   12.226616   11.094455  ...   4.278459       3.892099  4.119692  3.115186   \n",
       "43   12.164017   11.151177  ...   4.527473       4.084901  4.305352  3.930183   \n",
       "44   12.337996   11.316903  ...   4.316029       4.155610  4.251034  3.623233   \n",
       "45   12.346532   11.353562  ...   4.557091       4.191058  4.233932  4.075762   \n",
       "46   12.148834   11.120825  ...   4.480357       3.970858  4.432734  3.383253   \n",
       "47   12.091903   11.033918  ...   4.492121       4.102819  4.716647  3.649471   \n",
       "48   11.542836   11.096112  ...   4.369750       3.938524  4.500298  3.366916   \n",
       "49   11.894993   11.028224  ...   4.656870       4.185054  4.529080  3.866081   \n",
       "53   11.680741   11.125568  ...   4.321101       4.044857  4.150974  3.804070   \n",
       "55   11.537200   11.256968  ...   4.331688       4.076661  4.417399  3.274718   \n",
       "64   11.998374   11.070418  ...   4.387599       3.933511  4.246532  3.337598   \n",
       "65   11.952914   11.006261  ...   4.293580       3.946865  4.445984  3.336494   \n",
       "76   12.352521   11.340161  ...   4.394059       3.957019  4.286366  3.667046   \n",
       "77   12.206637   11.300098  ...   4.541126       4.073425  4.413411  3.980865   \n",
       "81   12.419737   11.253510  ...   4.526731       4.229098  4.611527  3.941546   \n",
       "88   12.050647   10.993768  ...   4.366838       3.966853  4.288859  3.387935   \n",
       "89   12.019573   10.943293  ...   4.414336       4.232457  4.329651  3.760205   \n",
       "98   12.114248   11.192342  ...   4.341897       3.983024  4.571687  3.522884   \n",
       "99   11.974167   11.090949  ...   4.269239       4.029189  4.106445  3.719126   \n",
       "100  12.009783   11.241416  ...   4.570584       3.935102  4.676844  3.735049   \n",
       "101  11.864216   11.361720  ...   4.423799       4.169760  4.259914  3.433266   \n",
       "102  11.877001   11.293042  ...   4.538909       3.842126  4.282809  3.307526   \n",
       "103  12.173317   11.058203  ...   4.371212       3.780527  4.339276  3.554490   \n",
       "\n",
       "     Tryptophan   Cystine   lactate  3-Hydroxybenzoic acid  Succinate  \\\n",
       "20     3.344071  3.223674  7.391850               3.161390   2.165842   \n",
       "24     3.537357  3.297459  6.926229               2.355910   2.260621   \n",
       "29     4.078951  3.158093  7.132086               2.472365   2.295447   \n",
       "52     3.698545  2.442535  6.605744               1.844567   2.278589   \n",
       "54     3.434078  2.675290  6.318173               3.773262   2.136011   \n",
       "56     3.363340  2.875249  6.822323               2.165502   2.439154   \n",
       "57     3.573760  3.155289  6.469863               2.431075   2.396853   \n",
       "58     3.271158  2.944483  6.317708               3.595748   2.325542   \n",
       "59     3.588967  3.240158  7.335289               2.148865   2.286202   \n",
       "60     3.629008  3.000813  6.460277               1.805982   2.657117   \n",
       "61     3.700230  2.817706  6.841190               2.258414   2.491088   \n",
       "62     3.687081  3.043648  7.135819               1.683385   2.430159   \n",
       "63     3.716985  3.313695  6.701857               3.227728   2.456011   \n",
       "66     3.550387  3.223100  6.573248               3.111999   2.169131   \n",
       "67     3.363633  3.290479  6.652491               2.871030   2.530671   \n",
       "68     3.342642  3.099874  6.938621               2.047146   2.173930   \n",
       "69     3.665739  2.796427  6.780874               1.879761   2.061995   \n",
       "70     3.223843  2.857471  6.622154               1.847846   2.536259   \n",
       "71     3.376076  3.033125  7.116761               2.321552   2.765492   \n",
       "72     3.345017  2.906815  6.176695               3.178995   2.031551   \n",
       "73     3.607930  3.201855  6.518707               3.772191   2.185031   \n",
       "75     3.804142  3.043344  6.796378               2.656716   2.359158   \n",
       "78     3.243048  2.900005  6.472523               2.625084   2.334525   \n",
       "79     3.510290  3.137008  6.686656               2.050419   2.167668   \n",
       "80     3.610717  2.799030  6.593750               1.587524   2.198980   \n",
       "82     3.889846  2.566960  6.572328               3.662519   2.141213   \n",
       "83     3.450654  3.158188  6.861233               2.678659   2.204938   \n",
       "84     3.564414  3.246105  6.974689               2.675655   2.335937   \n",
       "85     3.724252  2.904710  7.030989               2.027413   2.646200   \n",
       "86     3.592217  2.910169  6.929935               1.347397   2.130309   \n",
       "..          ...       ...       ...                    ...        ...   \n",
       "33     3.606496  3.751611  7.213011               3.029353   2.386811   \n",
       "34     3.161292  3.124340  6.830208               1.628613   2.728490   \n",
       "36     3.317755  3.128143  6.305648               2.296785   2.157262   \n",
       "37     3.665273  3.127071  6.749542               1.977932   2.284340   \n",
       "39     3.734718  3.369579  6.911310               2.358397   2.501465   \n",
       "40     3.369478  3.082252  6.165783               3.179794   1.858448   \n",
       "41     3.353292  3.435832  6.318271               1.908500   2.339160   \n",
       "42     3.187931  3.122705  6.074876               3.341021   1.845781   \n",
       "43     3.063406  3.838334  6.857591               2.673680   2.548548   \n",
       "44     3.400403  3.573865  6.495785               1.865180   2.135729   \n",
       "45     3.850118  3.194136  6.841580               2.551983   2.417319   \n",
       "46     3.350181  3.313648  6.379664               3.679456   2.722867   \n",
       "47     3.322210  3.487048  7.053267               1.250891   2.501105   \n",
       "48     3.343465  3.536378  7.527530               1.756122   2.449833   \n",
       "49     3.421321  3.643749  6.958296               1.745003   2.890356   \n",
       "53     3.573456  3.498864  6.469232               2.840020   2.767087   \n",
       "55     3.111471  2.990823  6.765249               1.777345   2.373036   \n",
       "64     3.197950  3.228279  6.869973               1.822230   2.099936   \n",
       "65     3.304540  3.584052  6.367939               3.595970   2.855985   \n",
       "76     3.606033  3.343059  6.648593               2.106706   2.409483   \n",
       "77     3.601304  3.262284  7.191626               2.070683   2.418890   \n",
       "81     3.489113  3.304206  6.659048               1.688147   2.160634   \n",
       "88     3.194383  3.073232  6.376179               3.425897   2.577070   \n",
       "89     3.630793  3.029616  6.743550               3.208075   2.329893   \n",
       "98     3.649491  3.605751  6.585925               3.288916   2.731427   \n",
       "99     3.212139  3.381057  7.102711               2.093669   2.238217   \n",
       "100    3.427296  3.121327  6.751464               2.099744   2.460060   \n",
       "101    3.317550  3.491284  7.009974               1.215855   2.716850   \n",
       "102    2.588503  3.388397  6.825325               2.567097   2.339410   \n",
       "103    3.071197  3.074653  6.561956               1.637428   2.147184   \n",
       "\n",
       "      Glucose  \n",
       "20   8.310292  \n",
       "24   8.423733  \n",
       "29   8.357071  \n",
       "52   8.414504  \n",
       "54   8.193980  \n",
       "56   8.346805  \n",
       "57   8.139350  \n",
       "58   8.359634  \n",
       "59   8.402214  \n",
       "60   8.319195  \n",
       "61   8.240600  \n",
       "62   8.369507  \n",
       "63   8.212264  \n",
       "66   8.390142  \n",
       "67   8.470635  \n",
       "68   8.382205  \n",
       "69   8.421293  \n",
       "70   8.222391  \n",
       "71   8.528619  \n",
       "72   8.339872  \n",
       "73   8.260333  \n",
       "75   8.223552  \n",
       "78   8.273688  \n",
       "79   8.489779  \n",
       "80   8.301010  \n",
       "82   8.289688  \n",
       "83   8.636030  \n",
       "84   8.484635  \n",
       "85   8.288551  \n",
       "86   8.263916  \n",
       "..        ...  \n",
       "33   8.446067  \n",
       "34   8.435363  \n",
       "36   8.497522  \n",
       "37   8.449097  \n",
       "39   8.431409  \n",
       "40   8.375820  \n",
       "41   8.278816  \n",
       "42   8.399841  \n",
       "43   8.398240  \n",
       "44   8.237084  \n",
       "45   8.467316  \n",
       "46   8.486501  \n",
       "47   8.518012  \n",
       "48   8.363992  \n",
       "49   8.400514  \n",
       "53   8.474646  \n",
       "55   8.395026  \n",
       "64   8.476514  \n",
       "65   8.215932  \n",
       "76   8.380304  \n",
       "77   8.449288  \n",
       "81   8.381040  \n",
       "88   8.330042  \n",
       "89   8.449347  \n",
       "98   8.495185  \n",
       "99   8.352757  \n",
       "100  8.268259  \n",
       "101  8.382161  \n",
       "102  8.362965  \n",
       "103  8.379108  \n",
       "\n",
       "[92 rows x 125 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX4AAAEcCAYAAADA5t+tAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjAsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+17YcXAAAc3UlEQVR4nO3dfXhcdZ338ffHtkKhtVULuXkuK6CRKO2SXXCJbkq52PoEe7koREHUaNVbu+za9TEuyEMW0duHexVdu8YFEaOI4HZhRVnIyGa1hZYnoYFblsdaXMBSJAiVlu/9xzmB02GSzKSTTJLf53Vdc10z5/zOOd+ZTD5z5nd+c44iAjMzS8cLGl2AmZlNLAe/mVliHPxmZolx8JuZJcbBb2aWGAe/mVliHPw24SRdIOmcRtfRaCO9DpLeJal/omsqq2HMfyf/jSc3B3/CJN0r6UlJg5IelXSlpP0aXVeRpJB0UKPrmMoklSS9t9F12OTh4Lc3R8QcYC/gf4CvNLiecaOM3/OWPP8TGAAR8RRwKfDKoWmS5kn6tqSHJd0n6dNDwSnp65IuLbQ9T9I1ebi2S9oo6VOSHsm/WbxjuG1Lep+kuyRtlrRa0t759OvyJrfk30pOrLDsDElfyLdzj6QP598SZubzS5K6Jf0X8HvgjyTtnW9nc77d9xXWt0MXxdBzKTy+V9InJW3IvyX9i6RdC/PfJOlmSVsk/VzSqwvzFku6UdLjkr4PPLvc8C+NviLpMUl3SFqaT3yrpPVlDVdK+tEo66u0gR9I+k2+jeskHVrWZIGkq/OafybpgMKyr8jnbZZ0p6S31bp9awwHvwEgaTfgRGBNYfJXgHnAHwF/DrwTeHc+byXw6rwv+rVAJ3BqPHcOkP8FLAD2AU4FVkl6eYXtHg2cC7yN7FvHfcD3ACLidXmzwyJiTkR8v0Lp7wNeDywC/hj4ywptTgGWA3Pz9fcCG4G9gROAfxgK1Sq9A/gL4GXAIcCn8+fyx8C3gPcDLwW+AayWtIukFwI/Ai4CXgL8APirUbZzBHA32et4BnCZpJcAq4EDJTUX2p6cr7tWPwYOBvYEbgQurvBcz85ruHlovqTdgauB7+bLdgBfq/DBYZNRRPiW6A24FxgEtgDbgE3Aq/J5M4CtwCsL7d8PlAqP/xTYTBamHYXp7fn6di9MuwT4+/z+BcA5+f0e4HOFdnOAp4GF+eMADhrhOVwLvL/w+Jh8mZn54xJwVmH+fsB2YG5h2rnABeW1FZ7LxrLX7AOFx28A/ju//3Xg7LL67iT70Hxd/vqqMO/nxW2VLfeuCu2vB04pbKs7v38o8CiwyzDrKgHvreL9MD9/7eYVXovvlf1ttuev4YnAf5Yt/w3gjEqvo2+T6+Y9fvvLiJgP7AJ8GPiZpKG99ReShfqQ+8j24AGIiOvJ9khFFuxFj0bEE2XL7l1h+3sXtxERg8Bvi9sZxd7AA4XHD1RoU5y2N7A5Ih4vq63a7ZWvr/i8DgBW5t08WyRtIQvJvfPbryNPxcKyI6nUfmhbFwJvlySybzSXRMTWGp7DUDfZZyX9t6TfkX2oQfa3H/Lsc83/NpvzGg4Ajih7ru8g+6Znk5yD3wCIiO0RcRnZHl0b8AjZnvcBhWb7A78eeiDpQ2QfGJuAj5Wt8sV5d0Bx2U0VNr2puI18mZcWtzOKB4F9C48rjUoqhucm4CWS5pbVNrS9J4DdCvMqBVlxG8Xn9QDZXvj8/MO0FZgdEb15nfvkQV1cdiSV2m8CiIg1wB+A1wJvZ2zdPG8Hjif7ljQPWJhPL27z2ecqaQ5ZN9Umsuf6s6Hnmt/mRMQHx1CHTTAHvwHPjng5HngxMBAR28n24rslzc0P6n0E+E7e/hDgHLK+5VOAj0laVLbaMyW9MD8G8Cayfu1y3wXeLWmRpF2AfwDWRsS9+fwAfpkf3H1Gzw0/HcwPGF8CnCZpH0nzgY+P9Dwj4gGyLpZzJe2aH3zt5Lm+7WOBDkkvyb/5/E2F1XxI0r55f/ungKFjD/8MfEDSEXlgz85fq7nAL8i6v/5a0kxJbyHrKhvJnnn7WZLeCjQD/16Y/23gq8C2iBhtzP/M/PkO3WaRHfPYSvYNazey177cGyS15ccozib72zwAXAEcIumUvL5Zkv6k7LiDTVIOfvs3SYPA74BusgO0t+fzVpDtAd8N9JOF9LeUjZj5DnBeRNwSEb8iC8CL8vAG+A1Zv/MmslD9QETcUb7xiLgG+Hvgh2R7xS8DTio0+d88dwziEfLhp/ntYrKw/SlwK3ATcFXedvsIz7mDbO92E3A5Wb/01fm8h4D7ybo9fspzoV703Xze3fntnPy5rCM72PzV/LlfU3iefwDeQtZ3/yhZH/llI9QIsJbswOsjZH+bEyLit4X5FwEtVLe3/3XgycLtX8g+OO4j+7azgR0P7Bef6xlkXTyHk3XnkHeVHUv2t9pE9vc+j+wboE12jT7I4Nv0u1F2QLSO670XOKZs2jlk4dwLPE52UPEZYH6hzRFkwTQTeC9wHfA14DFgAFiStzuP7APjKbKD3l/Op7cB6/L2W4EPF9bdTxbKQ/MvB16czzuI7BvLO8lGET0MfKKw7GvIwnYL2YfePwKz8nkz82XfD9xF9mHxj2XPfXb+nA9u9N/ct6l18x6/TXUzyfakv0fWFdJM1v/81kKbk4HeiNiWP/4z4A6yg5hnA5dLmh8RHyfrkvlAZN8o/kbSAuBK4Atkxx5+Rzb888WF9b8zv+1N1j/+pbIa/4zsQ+AvyLq/Ds6nbwNOy+s4ClhGFvRFbyDb014MnCzpmMK8DwI3RPaNy6xqDn6bDp4k63q6kazL4myysCfvljqRHbtDHgS+EhFPR8R3ybprXj/Mut8M3B4RQx8cT+TLv7HQ5sKI2BDZKKbTgZPKDsp+JiKeiogbgduBwwAi4oaIWBsR2yLibmAV2dDPonMj4rHIjnmUyH6vgKR7yT40VlbzApkVzWx0ATb9RESJHUfajKdtwOURcerQhPzHaP9H0v7Aq4GH89AdsjEihhsmWa58uOlCZb9YLg7/LB/euQvZ6JehZX5TmP97svHwSHoF2TeJw8kOrs4k69cvqrhsRCwcpl6zUXmP36aD2OFBxO/JDha/g2zEUfnBz/IPpeKQzCibt8Nw00L74nDT8uGdW8kOho7mG8BtZD9QexHZtwWNvIjZznPw23T1beA9ZF0y3ymbt5eyc/rMlHQS2Uiiq/J5/0N2ioohVwCHSjoxb/92sv764rDKd+bnrdkdOJPsx1TlHyCVzCU7IPxEPgyyvH/fbFw4+G26uo7stBNrI2Jj2byfk53mYDPwGeCvIuLRfN6Xycbxb5H0xYh4GDiO7PcBvwX+FnhTRBT36C8i+3B5MN9mpbH/lawkO4/R42R7/5WGjprVnarbMTGbepSd3fNbEXFBYdp7gZMjor1O2+gHvlnchtlk5z1+m5YkHUn246ZKvxY2S5qD36YdSReT9dmfFjueKM7McFePmVlyvMdvZpYYB7+ZWWIa9svdBQsWxMKFCxu1+WntiSeeYPfddx+9odkk4Pfr+Fm/fv0jEbFH+fSGBf/ChQtZt25dozY/rZVKJdrb2xtdhllV/H4dP5IqXuXNXT1mZolx8JuZJcbBb2aWGAe/mVliHPxmZolx8E8jvb29tLS0sHTpUlpaWujt7W10SWY2CfkKXNNEb28vXV1d9PT0sH37dmbMmEFnZycAHR0dDa7OzCYT7/FPE93d3fT09LBkyRJmzpzJkiVL6Onpobu7u9Glmdkk4+CfJgYGBmhra9thWltbGwMDAw2qyMwmKwf/NNHc3Ex/f/8O0/r7+2lubm5QRWY2WTn4p4muri46Ozvp6+tj27Zt9PX10dnZSVdXV6NLM7NJxgd3p4mhA7grVqxgYGCA5uZmuru7fWDXzJ7HwT+NdHR00NHR4ZNemdmI3NVjZpaYqoNf0gxJN0m6osK8XSR9X9JdktZKWljPIs3MrH5q2eM/DRhubGAn8GhEHAR8CThvZwszM7PxUVXwS9oXeCPwzWGaHA9cmN+/FFgqSTtfnpmZ1Vu1e/xfBj4GPDPM/H2ABwAiYhvwGPDSna7OzMzqbtRRPZLeBDwUEesltQ/XrMK0qLCu5cBygKamJkqlUvWVWtUGBwf92tqU4ffrxKtmOOdRwHGS3gDsCrxI0nci4uRCm43AfsBGSTOBecDm8hVFxCpgFUBra2t4yOH48HBOm0r8fp14o3b1RMQnI2LfiFgInARcWxb6AKuBU/P7J+RtnrfHb2ZmjTfmH3BJOgtYFxGrgR7gIkl3ke3pn1Sn+szMrM5qCv6IKAGl/P7phelPAW+tZ2FmZjY+/MtdM7PEOPjNzBLj4DczS4yD38wsMQ5+M7PEOPjNzBLj4DczS4yD38waore3l5aWFpYuXUpLSwu9vb2NLikZvvSimU243t5eurq66OnpYfv27cyYMYPOzk4AXyd6AniP38wmXHd3Nz09PSxZsoSZM2eyZMkSenp66O7ubnRpSfAe/xQ11uvc+Nx5NhkMDAzQ1ta2w7S2tjYGBoa7yJ/Vk/f4p6iIGPZ2wMevGHae2WTQ3NxMf3//DtP6+/tpbm5uUEVpcfCb2YTr6uqis7OTvr4+tm3bRl9fH52dnXR1dTW6tCS4q8fMJtzQAdwVK1YwMDBAc3Mz3d3dPrA7QRz8ZtYQHR0ddHR0+ApcDeDgN7MJ4QEJk4f7+M1sQoxlMIJDf3w4+M3MEjNq8EvaVdL1km6RdLukMyu02V9Sn6SbJN0q6Q3jU66Zme2savb4twJHR8RhwCJgmaQjy9p8GrgkIhaTXWj9a/Ut08zM6mXUg7uRdbIN5g9n5bfyjrcAXpTfnwdsqleBZmZWX1WN6pE0A1gPHAScHxFry5p8BvippBXA7sAxw6xnObAcoKmpiVKpNLaqbVR+bW0q8ft1YlUV/BGxHVgkaT5wuaSWiLit0KQDuCAiviDpNcBFeZtnytazClgF0NraGh67O06uutLjom3q8Pt1wtU0qicitgAlYFnZrE7gkrzNL4BdgQV1qM/MzOqsmlE9e+R7+kiaTdaNc0dZs/uBpXmbZrLgf7i+pZqZWT1U09WzF3Bh3s//ArLRO1dIOgtYFxGrgZXAP0v6W7IDve8K//LCzGxSqmZUz63A4grTTy/c3wAcVd/SzMxsPPiXu2ZmiXHwm5klxsFvZpYYB7+ZWWIc/GZmiXHwm5klxsFvZpYYB7+ZWWIc/GZmifHF1ie5w878KY89+XTNyy38xJU1tZ83exa3nHFszdsxs6nHwT/JPfbk09z72TfWtEypVKr5NLe1flCY2dTlrh4zs8Q4+M3MEuPgNzNLjIPfzCwxDn4zs8Q4+M3MElPNNXd3lXS9pFsk3S7pzGHavU3ShrzNd+tfqpmZ1UM14/i3AkdHxKCkWUC/pB9HxJqhBpIOBj4JHBURj0rac5zqNTOznVTNNXcDGMwfzspv5RdSfx9wfkQ8mi/zUD2LNDOz+qmqj1/SDEk3Aw8BV0fE2rImhwCHSPovSWskLat3oWZmVh9VnbIhIrYDiyTNBy6X1BIRt5Wt52CgHdgX+M+8zZbieiQtB5YDNDU1USqVdv4ZJKDW12lwcHBMr63/HtYofu9NrJrO1RMRWySVgGVAMfg3Amsi4mngHkl3kn0Q3FC2/CpgFUBra2vUej6ZFM2971WsuG8MC/62xu00Q3v7L8ewIbOddNWVNZ9bynbOqMEvaQ/g6Tz0ZwPHAOeVNfsR0AFcIGkBWdfP3fUuNkWPD3zWJ2kzs7qqZo9/L+BCSTPIjglcEhFXSDoLWBcRq4GfAMdK2gBsBz4aETXuc5qZ2USoZlTPrcDiCtNPL9wP4CP5zczMJjH/ctfMLDEOfjOzxDj4zcwS4+A3M0uMg9/MLDEOfjOzxDj4zcwS4+A3M0uMg9/MLDEOfjOzxNR0dk4zs5EcduZPeezJp2tertaTBM6bPYtbzji25u1YxsFvZnXz2JNP+2yyU4C7eszMEuPgNzNLjIPfzCwxDn4zs8Q4+M3MEuNRPVPAmEYwXFX78DgzS0M1F1vfFbgO2CVvf2lEnDFM2xOAHwB/EhHr6lloqmodGgfZB8VYljOzNFSzx78VODoiBiXNAvol/Tgi1hQbSZoL/DWwdhzqNDOzOhm1jz8yg/nDWfktKjQ9G/gc8FT9yjMzs3qrqo9f0gxgPXAQcH5ErC2bvxjYLyKukPR3I6xnObAcoKmpiVKpNNa6bRR+ba1Ran3vDQ4Ojun96vf42FUV/BGxHVgkaT5wuaSWiLgNQNILgC8B76piPauAVQCtra1R68+0rUpXXVnzT+DN6mIM772xnLLB7/GdU9NwzojYApSAZYXJc4EWoCTpXuBIYLWk1jrVaGZmdTRq8EvaI9/TR9Js4BjgjqH5EfFYRCyIiIURsRBYAxznUT1mZpNTNXv8ewF9km4FbgCuzvvyz5J03PiWZ2Zm9TZqH39E3AosrjD99GHat+98WWZmNl78y10zq5u5zZ/gVRd+ovYFL6x1OwD+keJYOfjNrG4eH/isL8QyBfgkbWZmiXHwm5klxsFvZpYYB7+ZWWIc/GZmiXHwm5klxsFvZpYYB7+ZWWIc/GZmiXHwm5klxsFvZpYYB7+ZWWIc/GZmiXHwm5klxsFvZpaYUc/HL2lX4Dpgl7z9pRFxRlmbjwDvBbYBDwPviYj76l+uDZE08vzzKk+PiHGoxuw5YzpX/lW1LTNv9qzat2HPquZCLFuBoyNiUNIsoF/SjyNiTaHNTUBrRPxe0geBzwEnjkO9lhspwMdyYQuzeqj1IiyQfVCMZTkbu1G7eiIzmD+cld+irE1fRPw+f7gG2LeuVZqZWd1UdelFSTOA9cBBwPkRsXaE5p3Aj4dZz3JgOUBTUxOlUqmmYq06g4ODfm1tSvH7dWJVFfwRsR1YJGk+cLmkloi4rbydpJOBVuDPh1nPKmAVQGtra7g7Yny4q8emlKuu9Pt1gtU0qicitgAlYFn5PEnHAF3AcRGxtS7VmZlZ3Y0a/JL2yPf0kTQbOAa4o6zNYuAbZKH/0HgUamZm9VFNV89ewIV5P/8LgEsi4gpJZwHrImI18HlgDvCDfJjh/RFx3HgVbWZmYzdq8EfErcDiCtNPL9w/ps51mZnZOPEvd83MEuPgNzNLjIPfzCwxDn4zs8Q4+M3MEuPgNzNLjIPfzCwxDn4zs8Q4+M3MEuPgNzNLjIPfzCwxDn4zs8Q4+M3MEuPgNzNLjIPfzCwxDn4zs8Q4+M3MElPNNXd3lXS9pFsk3S7pzAptdpH0fUl3SVoraeF4FGtmZjuvmj3+rcDREXEYsAhYJunIsjadwKMRcRDwJeC8+pZpZmb1MmrwR2Ywfzgrv0VZs+OBC/P7lwJLlV913czMJpdRL7YOIGkGsB44CDg/ItaWNdkHeAAgIrZJegx4KfBIHWs1sylspH1BjdBHEFG+n2k7q6rgj4jtwCJJ84HLJbVExG2FJpX+os/7a0laDiwHaGpqolQq1V6xjWpwcNCvrU06fX19FacPDg4yZ86cYZfze7n+qgr+IRGxRVIJWAYUg38jsB+wUdJMYB6wucLyq4BVAK2trdHe3j62qm1EpVIJv7Y2Vfj9OvGqGdWzR76nj6TZwDHAHWXNVgOn5vdPAK4Nfz8zM5uUqtnj3wu4MO/nfwFwSURcIeksYF1ErAZ6gIsk3UW2p3/SuFVsZmY7ZdTgj4hbgcUVpp9euP8U8Nb6lmZmZuPBv9w1M0uMg9/MLDEOfjOzxDj4zcwS4+A3M0uMg9/MLDEOfjOzxDj4zcwS4+A3M0uMg9/MLDEOfjOzxDj4zcwS4+A3M0uMg9/MLDEOfjOzxDj4zcwS4+A3M0uMg9/MLDHVXGx9P0l9kgYk3S7ptApt5kn6N0m35G3ePT7lmpnZzqrmYuvbgJURcaOkucB6SVdHxIZCmw8BGyLizZL2AO6UdHFE/GE8ijYzs7EbdY8/Ih6MiBvz+48DA8A+5c2AuZIEzAE2k31gmJnZJFPNHv+zJC0EFgNry2Z9FVgNbALmAidGxDMVll8OLAdoamqiVCrVXLCNbnBw0K+tTRl+v048RUR1DaU5wM+A7oi4rGzeCcBRwEeAlwFXA4dFxO+GW19ra2usW7durHXbCEqlEu3t7Y0uw6wqfr+OH0nrI6K1fHpVo3okzQJ+CFxcHvq5dwOXReYu4B7gFTtTsJmZjY9qRvUI6AEGIuKLwzS7H1iat28CXg7cXa8izcysfqrp4z8KOAX4paSb82mfAvYHiIh/As4GLpD0S0DAxyPikXGo18zMdtKowR8R/WRhPlKbTcCx9SrKzMzGj3+5a2aWGAe/mVliHPxmZolx8JuZJcbBb2aWGAe/mVliHPxmZolx8JuZJcbBb2aWGAe/mVliHPxmZolx8JuZJcbBb2aWGAe/mVliHPxmZolx8JuZJcbBb2aWmGquubufpD5JA5Jul3TaMO3aJd2ct/lZ/Us1M7N6qOaau9uAlRFxo6S5wHpJV0fEhqEGkuYDXwOWRcT9kvYcp3rNzGwnjbrHHxEPRsSN+f3HgQFgn7Jmbwcui4j783YP1btQM5teent7aWlpYenSpbS0tNDb29vokpJRzR7/syQtBBYDa8tmHQLMklQC5gL/NyK+XWH55cBygKamJkqlUs0F2+gGBwf92tqkds0119DT08NHP/pRDjzwQO655x5WrlzJhg0bWLp0aaPLm/4ioqobMAdYD7ylwryvAmuA3YEFwK+AQ0Za3+GHHx42Pvr6+hpdgtmIDj300Lj22msj4rn367XXXhuHHnpoA6uafoB1USF/q9rjlzQL+CFwcURcVqHJRuCRiHgCeELSdcBhwP/bqU8lM5uWBgYGaGtr22FaW1sbAwMDDaooLdWM6hHQAwxExBeHafavwGslzZS0G3AE2bEAM7PnaW5upr+/f4dp/f39NDc3N6iitFSzx38UcArwS0k359M+BewPEBH/FBEDkq4CbgWeAb4ZEbeNR8FmNvV1dXXR2dlJT08P27dvp6+vj87OTrq7uxtdWhJGDf6I6AdURbvPA5+vR1FmNr11dHQAsGLFCgYGBmhubqa7u/vZ6Ta+ahrVY2ZWLx0dHXR0dFAqlWhvb290OUnxKRvMzBLj4DczS4yD38wsMQ5+M7PEOPjNzBKj7Fe9Ddiw9DBwX0M2Pv0tAB5pdBFmVfL7dfwcEBF7lE9sWPDb+JG0LiJaG12HWTX8fp147uoxM0uMg9/MLDEO/ulpVaMLMKuB368TzH38ZmaJ8R6/mVliHPzTiKRlku6UdJekTzS6HrORSPqWpIck+RTuE8zBP01ImgGcD7weeCXQIemVja3KbEQXAMsaXUSKHPzTx58Cd0XE3RHxB+B7wPENrslsWBFxHbC50XWkyME/fewDPFB4vDGfZma2Awf/9FHpKmkesmVmz+Pgnz42AvsVHu8LbGpQLWY2iTn4p48bgIMlHSjphcBJwOoG12Rmk5CDf5qIiG3Ah4GfAAPAJRFxe2OrMhuepF7gF8DLJW2U1NnomlLhX+6amSXGe/xmZolx8JuZJcbBb2aWGAe/mVliHPxmZolx8FuyJA3W0PYzkv5uvNZvNpEc/GZmiXHwmxVIerOktZJukvQfkpoKsw+TdK2kX0l6X2GZj0q6QdKtks5sQNlmNXHwm+2oHzgyIhaTndr6Y4V5rwbeCLwGOF3S3pKOBQ4mOy32IuBwSa+b4JrNajKz0QWYTTL7At+XtBfwQuCewrx/jYgngScl9ZGFfRtwLHBT3mYO2QfBdRNXslltHPxmO/oK8MWIWC2pHfhMYV75+U2C7HTY50bENyamPLOd564esx3NA36d3z+1bN7xknaV9FKgneyMqD8B3iNpDoCkfSTtOVHFmo2F9/gtZbtJ2lh4/EWyPfwfSPo1sAY4sDD/euBKYH/g7IjYBGyS1Az8QhLAIHAy8ND4l282Nj47p5lZYtzVY2aWGAe/mVliHPxmZolx8JuZJcbBb2aWGAe/mVliHPxmZolx8JuZJeb/A4+ck8M0GZyIAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "##################################################################################################################\n",
    "#\n",
    "# YouDo:\n",
    "# \n",
    "#    1) Generate boxplots of Tryptophan levels separated based on \n",
    "#        the \"Label\" value\n",
    "#\n",
    "#      Use whatever plotting library you like.   \n",
    "#      Hint 1: matplotlib has a boxplot tool\n",
    "#          help(plt.boxplot)\n",
    "#      Hint 2: pandas data frames have a built-in boxplot tool\n",
    "#          help(df.boxplot)\n",
    "#\n",
    "#######################################  BEGIN STUDENT CODE  #####################################################\n",
    "\n",
    "# Pandas version\n",
    "box = df.boxplot(column='Tryptophan', by = 'Label')\n",
    "\n",
    "# matplotlib version (instructor ver)\n",
    "s0 = df['Tryptophan'].loc[df['Label']==0]\n",
    "s1 = df['Tryptophan'].loc[df['Label']==1]\n",
    "plt.figure()\n",
    "darf = plt.boxplot([s0,s1], labels = ['Label_0', 'Label_1'])\n",
    "\n",
    "\n",
    "#######################################   END STUDENT CODE   #####################################################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Pyruvate  MethylSuccinate  Phosphoglyceric Acid  Glucose 1-phosphate  \\\n",
      "20   14.158545        15.624522             13.383600            11.923638   \n",
      "24   14.180287        15.325665             13.468363            12.046294   \n",
      "29   13.921329        16.156867             14.615061            13.051929   \n",
      "52   13.785605        15.939535             13.206615            12.342156   \n",
      "54   13.674234        15.743668             11.542008            11.807872   \n",
      "56   13.447093        15.595200             13.118302            12.002027   \n",
      "57   13.616625        15.406841             14.124967            12.499758   \n",
      "58   13.731086        15.449381             13.108050            11.948349   \n",
      "59   14.468678        15.438259             13.737836            12.204044   \n",
      "60   13.589152        15.895696             13.992094            12.260070   \n",
      "61   14.124827        15.667980             13.885415            12.434999   \n",
      "62   14.125212        15.848679             14.611221            12.768474   \n",
      "63   13.911841        15.770673             13.616139            12.200233   \n",
      "66   13.905019        16.145448             13.694255            12.297953   \n",
      "67   13.587752        15.693374             13.750024            12.287812   \n",
      "68   14.187646        15.470626             13.781765            12.357138   \n",
      "69   14.094374        15.391283             13.156052            12.299693   \n",
      "70   13.887737        15.651142             13.677427            12.106190   \n",
      "71   14.229909        15.692926             14.235475            12.749272   \n",
      "72   13.470893        15.776606             12.803612            11.874306   \n",
      "73   13.791416        15.660444             12.416035            11.807934   \n",
      "75   13.811276        15.877942             13.655066            12.005657   \n",
      "78   13.408338        15.825618             13.032500            12.104698   \n",
      "79   13.626238        15.683295             13.745549            12.419539   \n",
      "80   13.732388        15.614327             13.550485            12.214790   \n",
      "82   13.434239        15.599394             12.937875            11.949690   \n",
      "83   14.095026        15.480027             13.737106            12.276233   \n",
      "84   13.948679        15.707015             14.538745            12.775609   \n",
      "85   14.298264        15.355092             14.103120            12.766971   \n",
      "86   14.130082        15.892906             13.303000            12.037620   \n",
      "..         ...              ...                   ...                  ...   \n",
      "33   13.607465        15.742973                   NaN            11.700570   \n",
      "34   14.074076        15.550370             14.360856            12.789551   \n",
      "36   13.608710        15.620534             13.926261            11.985488   \n",
      "37   13.912395        15.710430             12.546500            11.658677   \n",
      "39   14.272681        15.398227             13.917771            12.557395   \n",
      "40   13.553682        15.332108             12.975258            12.172884   \n",
      "41   13.503647        15.798457             13.363521            12.329034   \n",
      "42   13.185622        15.649060             13.128470            12.195693   \n",
      "43   13.855509        15.827115             13.265267            11.844197   \n",
      "44   13.542743        15.697666             14.144507            12.567397   \n",
      "45   13.995393        15.763045             13.422012            12.303995   \n",
      "46   13.526921        15.583914             15.236153            13.343259   \n",
      "47   14.098327        15.572972             13.839735            12.908054   \n",
      "48   14.237930        15.678887             13.976978            12.430527   \n",
      "49   13.970157        16.027721             14.244315            13.328620   \n",
      "53   13.794663        15.445801             13.475055            12.279256   \n",
      "55   14.103663        15.568095             13.472755            12.202148   \n",
      "64   13.720616        15.587086             13.224181            11.946611   \n",
      "65   13.336426        15.462282             13.441044            12.612899   \n",
      "76   13.712649        15.710702             13.450497            12.085055   \n",
      "77   14.136347        15.646664             13.707184            12.397715   \n",
      "81   13.877760        15.865054             13.652736            11.859613   \n",
      "88   13.101075        15.563113             14.134044            12.751887   \n",
      "89   13.699883        15.633531             13.078100            11.848265   \n",
      "98   13.769740        15.617562             13.983691            12.404641   \n",
      "99   13.881991        15.634942             12.687261            11.649048   \n",
      "100  13.911391        15.808535             13.255651            12.072060   \n",
      "101  14.004190        15.578057             15.050097            12.955338   \n",
      "102  13.782098        15.628488             13.800509            12.324283   \n",
      "103  13.841570        15.697214             14.467861            12.549533   \n",
      "\n",
      "     Malonic Acid  Fumaric Acid  Alpha-Ketoglutaric Acid  Sarcosine  \\\n",
      "20      13.933560     12.787696                14.981440  11.739484   \n",
      "24      13.043221     12.536227                14.593418  11.833211   \n",
      "29      13.381985     12.419600                14.701978  12.232070   \n",
      "52      12.448068     12.211098                14.860913  11.949448   \n",
      "54      14.582068     11.635154                14.970451  11.457569   \n",
      "56      12.817962     12.065246                14.796780  11.831409   \n",
      "57      13.168693     12.235455                14.721283  11.691816   \n",
      "58      14.492458     12.008819                14.727796  11.733394   \n",
      "59      12.950250     12.466876                14.826535  12.037085   \n",
      "60      12.513580     12.256111                14.934873  11.807985   \n",
      "61      12.962366     12.152578                14.616602  11.767767   \n",
      "62      12.380620     12.660110                14.852526  12.026815   \n",
      "63      13.897249     12.203694                14.965501  11.984423   \n",
      "66      13.897956     12.847235                14.765581  11.922889   \n",
      "67      13.601792     12.299811                15.022294  11.774581   \n",
      "68      14.225409     14.940561                14.684765  11.810785   \n",
      "69      12.538918     12.572195                14.731909  11.818707   \n",
      "70      12.443795     12.495065                15.046157  11.758245   \n",
      "71      12.978876     11.558824                14.839368  11.590611   \n",
      "72      13.856436     11.851316                14.723356  11.897736   \n",
      "73      14.847563     12.695139                14.803905  11.960705   \n",
      "75      13.359262     12.686008                14.837980  12.442368   \n",
      "78      13.393705     12.255903                14.912439  12.352381   \n",
      "79      12.659798     12.046584                14.929399  12.618666   \n",
      "80      12.295163     11.849885                14.638246  12.493920   \n",
      "82      14.685660     12.535203                14.994927  12.625845   \n",
      "83      13.373069     12.241929                14.765373  12.465197   \n",
      "84      13.379634     12.260513                14.727783  12.237720   \n",
      "85      12.750475     11.651945                14.502873  12.194344   \n",
      "86      12.079807     12.701836                14.945113  12.122186   \n",
      "..            ...           ...                      ...        ...   \n",
      "33      13.821512     13.444548                14.891810  11.082123   \n",
      "34      12.534986     13.780358                14.695148  11.888787   \n",
      "36      13.138026     13.473093                14.805259  12.167358   \n",
      "37      12.798666     12.761682                14.730217  12.118298   \n",
      "39      13.113209     13.035004                14.700396  12.477438   \n",
      "40      13.984951     12.718311                14.723451  11.626854   \n",
      "41      12.712400     13.083894                14.792074  12.206497   \n",
      "42      14.377665     15.325704                14.478469  12.226616   \n",
      "43      15.126767     17.093881                15.067586  12.164017   \n",
      "44      12.971915     14.868657                14.950939  12.337996   \n",
      "45      13.350982     13.343193                14.944149  12.346532   \n",
      "46      14.652109     14.664571                14.872883  12.148834   \n",
      "47      12.830078     14.253377                14.688208  12.091903   \n",
      "48      12.674354     13.679608                14.850197  11.542836   \n",
      "49      13.067405     15.250113                14.968732  11.894993   \n",
      "53      13.653491     13.258665                14.796514  11.680741   \n",
      "55      12.950661     14.140104                14.573157  11.537200   \n",
      "64      12.845243     13.753623                14.738364  11.998374   \n",
      "65      14.526108     13.405522                14.843443  11.952914   \n",
      "76      13.107520     13.235522                14.892700  12.352521   \n",
      "77      12.885362     12.590245                14.732140  12.206637   \n",
      "81      12.580436     13.614601                14.953066  12.419737   \n",
      "88      14.329279     13.317395                14.881287  12.050647   \n",
      "89      14.006595     12.877403                14.893908  12.019573   \n",
      "98      14.108590     13.666401                14.984438  12.114248   \n",
      "99      12.839689     12.596961                14.787885  11.974167   \n",
      "100     12.829308     12.518926                14.914205  12.009783   \n",
      "101     12.021058     12.698247                14.838858  11.864216   \n",
      "102     13.557521     13.930284                15.010427  11.877001   \n",
      "103     13.157717     14.309882                14.518098  12.173317   \n",
      "\n",
      "     Cadaverine  5-Aminovaleric Acid  ...  3-Hydroxybenzoic acid  Succinate  \\\n",
      "20    10.921776            15.786042  ...               3.161390   2.165842   \n",
      "24    11.121865            16.069782  ...               2.355910   2.260621   \n",
      "29    11.025333            16.225353  ...               2.472365   2.295447   \n",
      "52    11.015657            15.957796  ...               1.844567   2.278589   \n",
      "54    11.103239            15.715103  ...               3.773262   2.136011   \n",
      "56    11.219337            16.001684  ...               2.165502   2.439154   \n",
      "57    11.189055            15.884567  ...               2.431075   2.396853   \n",
      "58    10.864286            15.935869  ...               3.595748   2.325542   \n",
      "59    11.032339            16.125586  ...               2.148865   2.286202   \n",
      "60    10.975466            15.808182  ...               1.805982   2.657117   \n",
      "61    10.930519            16.285400  ...               2.258414   2.491088   \n",
      "62    10.841767            16.208444  ...               1.683385   2.430159   \n",
      "63    11.260172            15.751961  ...               3.227728   2.456011   \n",
      "66    11.323538            16.015081  ...               3.111999   2.169131   \n",
      "67    10.902013            15.978375  ...               2.871030   2.530671   \n",
      "68    10.959663            15.914285  ...               2.047146   2.173930   \n",
      "69    11.256166            16.181429  ...               1.879761   2.061995   \n",
      "70    11.162599            15.731707  ...               1.847846   2.536259   \n",
      "71    11.228472            16.144300  ...               2.321552   2.765492   \n",
      "72    11.275317            15.809819  ...               3.178995   2.031551   \n",
      "73    11.263613            16.197134  ...               3.772191   2.185031   \n",
      "75    11.095155            15.957136  ...               2.656716   2.359158   \n",
      "78    11.293471            15.928972  ...               2.625084   2.334525   \n",
      "79    11.181892            16.190544  ...               2.050419   2.167668   \n",
      "80    11.128544            16.146602  ...               1.587524   2.198980   \n",
      "82    11.330293            15.975255  ...               3.662519   2.141213   \n",
      "83    11.278841            16.051996  ...               2.678659   2.204938   \n",
      "84    11.051486            16.122483  ...               2.675655   2.335937   \n",
      "85    11.072816            15.951453  ...               2.027413   2.646200   \n",
      "86    11.100334            15.753399  ...               1.347397   2.130309   \n",
      "..          ...                  ...  ...                    ...        ...   \n",
      "33    11.350869            16.444443  ...               3.029353   2.386811   \n",
      "34    11.228459            15.980659  ...               1.628613   2.728490   \n",
      "36    10.928987            16.056472  ...               2.296785   2.157262   \n",
      "37    10.966498            15.989482  ...               1.977932   2.284340   \n",
      "39    10.884966            16.229957  ...               2.358397   2.501465   \n",
      "40    11.235005            16.074448  ...               3.179794   1.858448   \n",
      "41    11.103387            15.918224  ...               1.908500   2.339160   \n",
      "42    11.094455            15.579402  ...               3.341021   1.845781   \n",
      "43    11.151177            16.083295  ...               2.673680   2.548548   \n",
      "44    11.316903            16.158574  ...               1.865180   2.135729   \n",
      "45    11.353562            16.046138  ...               2.551983   2.417319   \n",
      "46    11.120825            15.926145  ...               3.679456   2.722867   \n",
      "47    11.033918            16.048770  ...               1.250891   2.501105   \n",
      "48    11.096112            15.924515  ...               1.756122   2.449833   \n",
      "49    11.028224            16.062197  ...               1.745003   2.890356   \n",
      "53    11.125568            16.330321  ...               2.840020   2.767087   \n",
      "55    11.256968            16.064409  ...               1.777345   2.373036   \n",
      "64    11.070418            16.061408  ...               1.822230   2.099936   \n",
      "65    11.006261            15.951424  ...               3.595970   2.855985   \n",
      "76    11.340161            16.177415  ...               2.106706   2.409483   \n",
      "77    11.300098            15.987128  ...               2.070683   2.418890   \n",
      "81    11.253510            16.197836  ...               1.688147   2.160634   \n",
      "88    10.993768            15.754225  ...               3.425897   2.577070   \n",
      "89    10.943293            16.201180  ...               3.208075   2.329893   \n",
      "98    11.192342            15.924314  ...               3.288916   2.731427   \n",
      "99    11.090949            16.020527  ...               2.093669   2.238217   \n",
      "100   11.241416            15.898304  ...               2.099744   2.460060   \n",
      "101   11.361720            16.004943  ...               1.215855   2.716850   \n",
      "102   11.293042            15.663826  ...               2.567097   2.339410   \n",
      "103   11.058203            16.039076  ...               1.637428   2.147184   \n",
      "\n",
      "      Glucose  V127  V128  V129  V130  V131  V132  V133  \n",
      "20   8.310292   NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
      "24   8.423733   NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
      "29   8.357071   NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
      "52   8.414504   NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
      "54   8.193980   NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
      "56   8.346805   NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
      "57   8.139350   NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
      "58   8.359634   NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
      "59   8.402214   NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
      "60   8.319195   NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
      "61   8.240600   NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
      "62   8.369507   NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
      "63   8.212264   NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
      "66   8.390142   NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
      "67   8.470635   NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
      "68   8.382205   NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
      "69   8.421293   NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
      "70   8.222391   NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
      "71   8.528619   NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
      "72   8.339872   NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
      "73   8.260333   NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
      "75   8.223552   NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
      "78   8.273688   NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
      "79   8.489779   NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
      "80   8.301010   NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
      "82   8.289688   NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
      "83   8.636030   NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
      "84   8.484635   NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
      "85   8.288551   NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
      "86   8.263916   NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
      "..        ...   ...   ...   ...   ...   ...   ...   ...  \n",
      "33   8.446067   NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
      "34   8.435363   NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
      "36   8.497522   NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
      "37   8.449097   NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
      "39   8.431409   NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
      "40   8.375820   NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
      "41   8.278816   NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
      "42   8.399841   NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
      "43   8.398240   NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
      "44   8.237084   NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
      "45   8.467316   NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
      "46   8.486501   NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
      "47   8.518012   NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
      "48   8.363992   NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
      "49   8.400514   NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
      "53   8.474646   NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
      "55   8.395026   NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
      "64   8.476514   NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
      "65   8.215932   NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
      "76   8.380304   NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
      "77   8.449288   NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
      "81   8.381040   NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
      "88   8.330042   NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
      "89   8.449347   NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
      "98   8.495185   NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
      "99   8.352757   NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
      "100  8.268259   NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
      "101  8.382161   NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
      "102  8.362965   NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
      "103  8.379108   NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
      "\n",
      "[92 rows x 131 columns]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pyruvate</th>\n",
       "      <th>MethylSuccinate</th>\n",
       "      <th>Phosphoglyceric Acid</th>\n",
       "      <th>Glucose 1-phosphate</th>\n",
       "      <th>Malonic Acid</th>\n",
       "      <th>Fumaric Acid</th>\n",
       "      <th>Alpha-Ketoglutaric Acid</th>\n",
       "      <th>Sarcosine</th>\n",
       "      <th>Cadaverine</th>\n",
       "      <th>5-Aminovaleric Acid</th>\n",
       "      <th>...</th>\n",
       "      <th>3-Hydroxybenzoic acid</th>\n",
       "      <th>Succinate</th>\n",
       "      <th>Glucose</th>\n",
       "      <th>V127</th>\n",
       "      <th>V128</th>\n",
       "      <th>V129</th>\n",
       "      <th>V130</th>\n",
       "      <th>V131</th>\n",
       "      <th>V132</th>\n",
       "      <th>V133</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Pyruvate</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MethylSuccinate</th>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Phosphoglyceric Acid</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Glucose 1-phosphate</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Malonic Acid</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Fumaric Acid</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Alpha-Ketoglutaric Acid</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sarcosine</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Cadaverine</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5-Aminovaleric Acid</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Pipecolate</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Tyramine</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1-Methylhistidine</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>isoValeric Acid</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2-Hydroxyisovaleric Acid</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3-Methyl-2-Oxovaleric Acid</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2-Aminoisobutyric acid</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Creatinine</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Betaine</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Taurine</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1-Methylhistamine</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Pyroglutamic Acid</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Creatine</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4-Hydroxyproline</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Homocysteine</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Cysteamine</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Carnitine</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Glycerate</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>N-AcetylGlycine</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Guanidinoacetate</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Alanine</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Serine</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Proline</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Valine</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Threonine</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Leucine</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>iso-Leucine</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Asparagine</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Aspartic Acid</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Glutamine</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Lysine</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Glutamic acid</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Methionine</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Histidine</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Phenylalanine</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Arginine</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Tyrosine</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Tryptophan</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Cystine</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lactate</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3-Hydroxybenzoic acid</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Succinate</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Glucose</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V127</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V128</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V129</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V130</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V131</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V132</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V133</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>131 rows × 131 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                            Pyruvate  MethylSuccinate  Phosphoglyceric Acid  \\\n",
       "Pyruvate                        True            False                 False   \n",
       "MethylSuccinate                False             True                 False   \n",
       "Phosphoglyceric Acid           False            False                  True   \n",
       "Glucose 1-phosphate            False            False                 False   \n",
       "Malonic Acid                   False            False                 False   \n",
       "Fumaric Acid                   False            False                 False   \n",
       "Alpha-Ketoglutaric Acid        False            False                 False   \n",
       "Sarcosine                      False            False                 False   \n",
       "Cadaverine                     False            False                 False   \n",
       "5-Aminovaleric Acid            False            False                 False   \n",
       "Pipecolate                     False            False                 False   \n",
       "Tyramine                       False            False                 False   \n",
       "1-Methylhistidine              False            False                 False   \n",
       "isoValeric Acid                False            False                 False   \n",
       "2-Hydroxyisovaleric Acid       False            False                 False   \n",
       "3-Methyl-2-Oxovaleric Acid     False            False                 False   \n",
       "2-Aminoisobutyric acid         False            False                 False   \n",
       "Creatinine                     False            False                 False   \n",
       "Betaine                        False            False                 False   \n",
       "Taurine                        False            False                 False   \n",
       "1-Methylhistamine              False            False                 False   \n",
       "Pyroglutamic Acid              False            False                 False   \n",
       "Creatine                       False            False                 False   \n",
       "4-Hydroxyproline               False            False                 False   \n",
       "Homocysteine                   False            False                 False   \n",
       "Cysteamine                     False            False                 False   \n",
       "Carnitine                      False            False                 False   \n",
       "Glycerate                      False            False                 False   \n",
       "N-AcetylGlycine                False            False                 False   \n",
       "Guanidinoacetate               False            False                 False   \n",
       "...                              ...              ...                   ...   \n",
       "Alanine                        False            False                 False   \n",
       "Serine                         False            False                 False   \n",
       "Proline                        False            False                 False   \n",
       "Valine                         False            False                 False   \n",
       "Threonine                      False            False                 False   \n",
       "Leucine                        False            False                 False   \n",
       "iso-Leucine                    False            False                 False   \n",
       "Asparagine                     False            False                 False   \n",
       "Aspartic Acid                  False            False                 False   \n",
       "Glutamine                      False            False                 False   \n",
       "Lysine                         False            False                 False   \n",
       "Glutamic acid                  False            False                 False   \n",
       "Methionine                     False            False                 False   \n",
       "Histidine                      False            False                 False   \n",
       "Phenylalanine                  False            False                 False   \n",
       "Arginine                       False            False                 False   \n",
       "Tyrosine                       False            False                 False   \n",
       "Tryptophan                     False            False                 False   \n",
       "Cystine                        False            False                 False   \n",
       "lactate                        False            False                 False   \n",
       "3-Hydroxybenzoic acid          False            False                 False   \n",
       "Succinate                      False            False                 False   \n",
       "Glucose                        False            False                 False   \n",
       "V127                           False            False                 False   \n",
       "V128                           False            False                 False   \n",
       "V129                           False            False                 False   \n",
       "V130                           False            False                 False   \n",
       "V131                           False            False                 False   \n",
       "V132                           False            False                 False   \n",
       "V133                           False            False                 False   \n",
       "\n",
       "                            Glucose 1-phosphate  Malonic Acid  Fumaric Acid  \\\n",
       "Pyruvate                                  False         False         False   \n",
       "MethylSuccinate                           False         False         False   \n",
       "Phosphoglyceric Acid                      False         False         False   \n",
       "Glucose 1-phosphate                        True         False         False   \n",
       "Malonic Acid                              False          True         False   \n",
       "Fumaric Acid                              False         False          True   \n",
       "Alpha-Ketoglutaric Acid                   False         False         False   \n",
       "Sarcosine                                 False         False         False   \n",
       "Cadaverine                                False         False         False   \n",
       "5-Aminovaleric Acid                       False         False         False   \n",
       "Pipecolate                                False         False         False   \n",
       "Tyramine                                  False         False         False   \n",
       "1-Methylhistidine                         False         False         False   \n",
       "isoValeric Acid                           False         False         False   \n",
       "2-Hydroxyisovaleric Acid                  False         False         False   \n",
       "3-Methyl-2-Oxovaleric Acid                False         False         False   \n",
       "2-Aminoisobutyric acid                    False         False         False   \n",
       "Creatinine                                False         False         False   \n",
       "Betaine                                   False         False         False   \n",
       "Taurine                                   False         False         False   \n",
       "1-Methylhistamine                         False         False         False   \n",
       "Pyroglutamic Acid                         False         False         False   \n",
       "Creatine                                  False         False         False   \n",
       "4-Hydroxyproline                          False         False         False   \n",
       "Homocysteine                              False         False         False   \n",
       "Cysteamine                                False         False         False   \n",
       "Carnitine                                 False         False         False   \n",
       "Glycerate                                 False         False         False   \n",
       "N-AcetylGlycine                           False         False         False   \n",
       "Guanidinoacetate                          False         False         False   \n",
       "...                                         ...           ...           ...   \n",
       "Alanine                                   False         False         False   \n",
       "Serine                                    False         False         False   \n",
       "Proline                                   False         False         False   \n",
       "Valine                                    False         False         False   \n",
       "Threonine                                 False         False         False   \n",
       "Leucine                                   False         False         False   \n",
       "iso-Leucine                               False         False         False   \n",
       "Asparagine                                False         False         False   \n",
       "Aspartic Acid                             False         False         False   \n",
       "Glutamine                                 False         False         False   \n",
       "Lysine                                    False         False         False   \n",
       "Glutamic acid                             False         False         False   \n",
       "Methionine                                False         False         False   \n",
       "Histidine                                 False         False         False   \n",
       "Phenylalanine                             False         False         False   \n",
       "Arginine                                  False         False         False   \n",
       "Tyrosine                                  False         False         False   \n",
       "Tryptophan                                False         False         False   \n",
       "Cystine                                   False         False         False   \n",
       "lactate                                   False         False         False   \n",
       "3-Hydroxybenzoic acid                     False          True         False   \n",
       "Succinate                                 False         False         False   \n",
       "Glucose                                   False         False         False   \n",
       "V127                                      False         False         False   \n",
       "V128                                      False         False         False   \n",
       "V129                                      False         False         False   \n",
       "V130                                      False         False         False   \n",
       "V131                                      False         False         False   \n",
       "V132                                      False         False         False   \n",
       "V133                                      False         False         False   \n",
       "\n",
       "                            Alpha-Ketoglutaric Acid  Sarcosine  Cadaverine  \\\n",
       "Pyruvate                                      False      False       False   \n",
       "MethylSuccinate                               False      False       False   \n",
       "Phosphoglyceric Acid                          False      False       False   \n",
       "Glucose 1-phosphate                           False      False       False   \n",
       "Malonic Acid                                  False      False       False   \n",
       "Fumaric Acid                                  False      False       False   \n",
       "Alpha-Ketoglutaric Acid                        True      False       False   \n",
       "Sarcosine                                     False       True       False   \n",
       "Cadaverine                                    False      False        True   \n",
       "5-Aminovaleric Acid                           False      False       False   \n",
       "Pipecolate                                    False      False       False   \n",
       "Tyramine                                      False      False       False   \n",
       "1-Methylhistidine                             False      False       False   \n",
       "isoValeric Acid                               False      False       False   \n",
       "2-Hydroxyisovaleric Acid                      False      False       False   \n",
       "3-Methyl-2-Oxovaleric Acid                    False      False       False   \n",
       "2-Aminoisobutyric acid                        False      False       False   \n",
       "Creatinine                                    False      False       False   \n",
       "Betaine                                       False      False       False   \n",
       "Taurine                                       False      False       False   \n",
       "1-Methylhistamine                             False      False       False   \n",
       "Pyroglutamic Acid                             False      False       False   \n",
       "Creatine                                      False      False       False   \n",
       "4-Hydroxyproline                              False      False       False   \n",
       "Homocysteine                                  False      False       False   \n",
       "Cysteamine                                    False      False       False   \n",
       "Carnitine                                     False      False       False   \n",
       "Glycerate                                     False      False       False   \n",
       "N-AcetylGlycine                               False      False       False   \n",
       "Guanidinoacetate                              False      False       False   \n",
       "...                                             ...        ...         ...   \n",
       "Alanine                                       False      False       False   \n",
       "Serine                                        False      False       False   \n",
       "Proline                                       False      False       False   \n",
       "Valine                                        False      False       False   \n",
       "Threonine                                     False      False       False   \n",
       "Leucine                                       False      False       False   \n",
       "iso-Leucine                                   False      False       False   \n",
       "Asparagine                                    False      False       False   \n",
       "Aspartic Acid                                 False      False       False   \n",
       "Glutamine                                     False      False       False   \n",
       "Lysine                                        False      False       False   \n",
       "Glutamic acid                                 False      False       False   \n",
       "Methionine                                    False      False       False   \n",
       "Histidine                                     False      False       False   \n",
       "Phenylalanine                                 False      False       False   \n",
       "Arginine                                      False      False       False   \n",
       "Tyrosine                                      False      False       False   \n",
       "Tryptophan                                    False      False       False   \n",
       "Cystine                                       False      False       False   \n",
       "lactate                                       False      False       False   \n",
       "3-Hydroxybenzoic acid                         False      False       False   \n",
       "Succinate                                     False      False       False   \n",
       "Glucose                                       False      False       False   \n",
       "V127                                          False      False       False   \n",
       "V128                                          False      False       False   \n",
       "V129                                          False      False       False   \n",
       "V130                                          False      False       False   \n",
       "V131                                          False      False       False   \n",
       "V132                                          False      False       False   \n",
       "V133                                          False      False       False   \n",
       "\n",
       "                            5-Aminovaleric Acid  ...  3-Hydroxybenzoic acid  \\\n",
       "Pyruvate                                  False  ...                  False   \n",
       "MethylSuccinate                           False  ...                  False   \n",
       "Phosphoglyceric Acid                      False  ...                  False   \n",
       "Glucose 1-phosphate                       False  ...                  False   \n",
       "Malonic Acid                              False  ...                   True   \n",
       "Fumaric Acid                              False  ...                  False   \n",
       "Alpha-Ketoglutaric Acid                   False  ...                  False   \n",
       "Sarcosine                                 False  ...                  False   \n",
       "Cadaverine                                False  ...                  False   \n",
       "5-Aminovaleric Acid                        True  ...                  False   \n",
       "Pipecolate                                False  ...                  False   \n",
       "Tyramine                                  False  ...                  False   \n",
       "1-Methylhistidine                         False  ...                  False   \n",
       "isoValeric Acid                           False  ...                  False   \n",
       "2-Hydroxyisovaleric Acid                  False  ...                  False   \n",
       "3-Methyl-2-Oxovaleric Acid                False  ...                  False   \n",
       "2-Aminoisobutyric acid                    False  ...                  False   \n",
       "Creatinine                                False  ...                  False   \n",
       "Betaine                                   False  ...                  False   \n",
       "Taurine                                   False  ...                  False   \n",
       "1-Methylhistamine                         False  ...                  False   \n",
       "Pyroglutamic Acid                         False  ...                  False   \n",
       "Creatine                                  False  ...                  False   \n",
       "4-Hydroxyproline                          False  ...                  False   \n",
       "Homocysteine                              False  ...                  False   \n",
       "Cysteamine                                False  ...                  False   \n",
       "Carnitine                                 False  ...                  False   \n",
       "Glycerate                                 False  ...                  False   \n",
       "N-AcetylGlycine                           False  ...                  False   \n",
       "Guanidinoacetate                          False  ...                  False   \n",
       "...                                         ...  ...                    ...   \n",
       "Alanine                                   False  ...                  False   \n",
       "Serine                                    False  ...                  False   \n",
       "Proline                                   False  ...                  False   \n",
       "Valine                                     True  ...                  False   \n",
       "Threonine                                 False  ...                  False   \n",
       "Leucine                                   False  ...                  False   \n",
       "iso-Leucine                               False  ...                  False   \n",
       "Asparagine                                False  ...                  False   \n",
       "Aspartic Acid                             False  ...                  False   \n",
       "Glutamine                                 False  ...                  False   \n",
       "Lysine                                    False  ...                  False   \n",
       "Glutamic acid                             False  ...                  False   \n",
       "Methionine                                False  ...                  False   \n",
       "Histidine                                 False  ...                  False   \n",
       "Phenylalanine                             False  ...                  False   \n",
       "Arginine                                  False  ...                  False   \n",
       "Tyrosine                                  False  ...                  False   \n",
       "Tryptophan                                False  ...                  False   \n",
       "Cystine                                   False  ...                  False   \n",
       "lactate                                   False  ...                  False   \n",
       "3-Hydroxybenzoic acid                     False  ...                   True   \n",
       "Succinate                                 False  ...                  False   \n",
       "Glucose                                   False  ...                  False   \n",
       "V127                                      False  ...                  False   \n",
       "V128                                      False  ...                  False   \n",
       "V129                                      False  ...                  False   \n",
       "V130                                      False  ...                  False   \n",
       "V131                                      False  ...                  False   \n",
       "V132                                      False  ...                  False   \n",
       "V133                                      False  ...                  False   \n",
       "\n",
       "                            Succinate  Glucose   V127   V128   V129   V130  \\\n",
       "Pyruvate                        False    False  False  False  False  False   \n",
       "MethylSuccinate                 False    False  False  False  False  False   \n",
       "Phosphoglyceric Acid            False    False  False  False  False  False   \n",
       "Glucose 1-phosphate             False    False  False  False  False  False   \n",
       "Malonic Acid                    False    False  False  False  False  False   \n",
       "Fumaric Acid                    False    False  False  False  False  False   \n",
       "Alpha-Ketoglutaric Acid         False    False  False  False  False  False   \n",
       "Sarcosine                       False    False  False  False  False  False   \n",
       "Cadaverine                      False    False  False  False  False  False   \n",
       "5-Aminovaleric Acid             False    False  False  False  False  False   \n",
       "Pipecolate                      False    False  False  False  False  False   \n",
       "Tyramine                        False    False  False  False  False  False   \n",
       "1-Methylhistidine               False    False  False  False  False  False   \n",
       "isoValeric Acid                 False    False  False  False  False  False   \n",
       "2-Hydroxyisovaleric Acid        False    False  False  False  False  False   \n",
       "3-Methyl-2-Oxovaleric Acid      False    False  False  False  False  False   \n",
       "2-Aminoisobutyric acid          False    False  False  False  False  False   \n",
       "Creatinine                      False    False  False  False  False  False   \n",
       "Betaine                         False    False  False  False  False  False   \n",
       "Taurine                         False    False  False  False  False  False   \n",
       "1-Methylhistamine               False    False  False  False  False  False   \n",
       "Pyroglutamic Acid               False    False  False  False  False  False   \n",
       "Creatine                        False    False  False  False  False  False   \n",
       "4-Hydroxyproline                False    False  False  False  False  False   \n",
       "Homocysteine                    False    False  False  False  False  False   \n",
       "Cysteamine                      False    False  False  False  False  False   \n",
       "Carnitine                       False    False  False  False  False  False   \n",
       "Glycerate                       False    False  False  False  False  False   \n",
       "N-AcetylGlycine                 False    False  False  False  False  False   \n",
       "Guanidinoacetate                False    False  False  False  False  False   \n",
       "...                               ...      ...    ...    ...    ...    ...   \n",
       "Alanine                         False    False  False  False  False  False   \n",
       "Serine                          False    False  False  False  False  False   \n",
       "Proline                         False    False  False  False  False  False   \n",
       "Valine                          False    False  False  False  False  False   \n",
       "Threonine                       False    False  False  False  False  False   \n",
       "Leucine                         False    False  False  False  False  False   \n",
       "iso-Leucine                     False    False  False  False  False  False   \n",
       "Asparagine                      False    False  False  False  False  False   \n",
       "Aspartic Acid                   False    False  False  False  False  False   \n",
       "Glutamine                       False    False  False  False  False  False   \n",
       "Lysine                          False    False  False  False  False  False   \n",
       "Glutamic acid                   False    False  False  False  False  False   \n",
       "Methionine                      False    False  False  False  False  False   \n",
       "Histidine                       False    False  False  False  False  False   \n",
       "Phenylalanine                   False    False  False  False  False  False   \n",
       "Arginine                        False    False  False  False  False  False   \n",
       "Tyrosine                        False    False  False  False  False  False   \n",
       "Tryptophan                      False    False  False  False  False  False   \n",
       "Cystine                         False    False  False  False  False  False   \n",
       "lactate                         False    False  False  False  False  False   \n",
       "3-Hydroxybenzoic acid           False    False  False  False  False  False   \n",
       "Succinate                        True    False  False  False  False  False   \n",
       "Glucose                         False     True  False  False  False  False   \n",
       "V127                            False    False  False  False  False  False   \n",
       "V128                            False    False  False  False  False  False   \n",
       "V129                            False    False  False  False  False  False   \n",
       "V130                            False    False  False  False  False  False   \n",
       "V131                            False    False  False  False  False  False   \n",
       "V132                            False    False  False  False  False  False   \n",
       "V133                            False    False  False  False  False  False   \n",
       "\n",
       "                             V131   V132   V133  \n",
       "Pyruvate                    False  False  False  \n",
       "MethylSuccinate             False  False  False  \n",
       "Phosphoglyceric Acid        False  False  False  \n",
       "Glucose 1-phosphate         False  False  False  \n",
       "Malonic Acid                False  False  False  \n",
       "Fumaric Acid                False  False  False  \n",
       "Alpha-Ketoglutaric Acid     False  False  False  \n",
       "Sarcosine                   False  False  False  \n",
       "Cadaverine                  False  False  False  \n",
       "5-Aminovaleric Acid         False  False  False  \n",
       "Pipecolate                  False  False  False  \n",
       "Tyramine                    False  False  False  \n",
       "1-Methylhistidine           False  False  False  \n",
       "isoValeric Acid             False  False  False  \n",
       "2-Hydroxyisovaleric Acid    False  False  False  \n",
       "3-Methyl-2-Oxovaleric Acid  False  False  False  \n",
       "2-Aminoisobutyric acid      False  False  False  \n",
       "Creatinine                  False  False  False  \n",
       "Betaine                     False  False  False  \n",
       "Taurine                     False  False  False  \n",
       "1-Methylhistamine           False  False  False  \n",
       "Pyroglutamic Acid           False  False  False  \n",
       "Creatine                    False  False  False  \n",
       "4-Hydroxyproline            False  False  False  \n",
       "Homocysteine                False  False  False  \n",
       "Cysteamine                  False  False  False  \n",
       "Carnitine                   False  False  False  \n",
       "Glycerate                   False  False  False  \n",
       "N-AcetylGlycine             False  False  False  \n",
       "Guanidinoacetate            False  False  False  \n",
       "...                           ...    ...    ...  \n",
       "Alanine                     False  False  False  \n",
       "Serine                      False  False  False  \n",
       "Proline                     False  False  False  \n",
       "Valine                      False  False  False  \n",
       "Threonine                   False  False  False  \n",
       "Leucine                     False  False  False  \n",
       "iso-Leucine                 False  False  False  \n",
       "Asparagine                  False  False  False  \n",
       "Aspartic Acid               False  False  False  \n",
       "Glutamine                   False  False  False  \n",
       "Lysine                      False  False  False  \n",
       "Glutamic acid               False  False  False  \n",
       "Methionine                  False  False  False  \n",
       "Histidine                   False  False  False  \n",
       "Phenylalanine               False  False  False  \n",
       "Arginine                    False  False  False  \n",
       "Tyrosine                    False  False  False  \n",
       "Tryptophan                  False  False  False  \n",
       "Cystine                     False  False  False  \n",
       "lactate                     False  False  False  \n",
       "3-Hydroxybenzoic acid       False  False  False  \n",
       "Succinate                   False  False  False  \n",
       "Glucose                     False  False  False  \n",
       "V127                        False  False  False  \n",
       "V128                        False  False  False  \n",
       "V129                        False  False  False  \n",
       "V130                        False  False  False  \n",
       "V131                        False  False  False  \n",
       "V132                        False  False  False  \n",
       "V133                        False  False  False  \n",
       "\n",
       "[131 rows x 131 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAC7CAYAAACend6FAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjAsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+17YcXAAAgAElEQVR4nOy9d5hdZ3Xv/3l3O73MmV410qhYlixLcpGNsbEhxkAgtBgDNxBuSCB2SJyYhDQSctNuyk0hv1wMJHBDKgRCSQBjg21sY9xkSZYl2ZY0kkaaXk8vu72/P9aZMyZPTIllkM35Ps88enTO7L3f/e531rv2Wt/1XUprTRtttNFGGy8sGD/oAbTRRhtttHH20TbubbTRRhsvQLSNextttNHGCxBt495GG2208QJE27i30UYbbbwA0TbubbTRRhsvQDxnxl0p9Qql1FNKqeNKqV97rq7TRhvfT7TXdRvPF6jngueulDKBo8C1wCTwCPAWrfWRs36xNtr4PqG9rtt4PuG58twvBY5rrU9orV3gk8Brn6NrtdHG9wvtdd3G8wbPlXEfBM487f+Tzc/aaOP5jPa6buN5A+s5Oq/6Lz77lviPUupdwLsADMu5aNf5ikML3fKdBsMFbYJOB+i6ifJofRfENcpVmA35vx+T78w6hBYYPvhxMBugm9tXGNGYdYU2QIVgNjReQoapDTlGJQPCqokKIHTkOKsC2gLDg8AGq6YJIs3jTDk2suQSxmz8qMLwaZ1ThRCaMhlarY3FdMGshwRRA5SMWYXNiQkBQ65leHJ/pitTFziqNZNhRO7X8OU7P6rkeraMy6zLNUHOZ3jN4yx5Ooa3Nk4UBBFQAdjltfujeQ/o5rkUBM25VsHaOQxf7jPbXQJgZSkl86flGOWDETSPS/v4DQsVwFjnHCdnewmSzZsPDCKLAcawjxea+HULMyITGlYttJJniiHnDO21+TxvQ+9/seQEjz766KLWuvsZf+G7x3dc1/Cta9vEvGjnDoujB+Nn4fJttPGtqFPB1Y3/al0+Z8Z9Ehh+2v+HgOmn/4LW+qPARwEiw8O6/LpbqN74IQD+vRLnr971Zha3R2nkAA3pU/I3VBhTRJfAS4gRiyyDn5RzxuY0oYMYbQW7b3ic/f98AQCZkz5L2yy8pKbzkMaPKBYuF4uTOWIRWwypdRl4CYgtampdMl+RvKawSaNNiM0ZaAV2Ra5X75JzaQWhpbCrIbVOseB+XFHt1wS9LvEnImhDNgkQw+sUNbUexS+85QsM2Ct8aeVCAKaqWaY/NYofU/zY2+/js5++kkhejvNjkJoMWdlsUB/wMUsmkWUZZ3oixIsrlneGRGdNgogmMy7HVfsUyofsiYDl/1EmenuaevP+YvOawmaILiisGhiexo+trRU3DfV1LumDDtUB2SBBjHtsQVMegfSJ5ubX3BD7fXDTitJYQPaIQWUAIvm1c/pNOxf68NJXnGD2/61vnXPlRyvYjyUJoqAtTWy2uZE2N9jKkMZLB2z8lEfjN2RiVr7Wz94//qVnWosopSae8cvvDd9xXcO3ru20yumOx19GfuYAANcN7DxLQ2mjDXhI3/mM3z1XYZlHgE1KqfVKKQd4M/Dvz9G12mjj+4X2um7jeYPnxHPXWvtKqfcAtwMm8HGt9eFn+n3DF6/t3yvi0v1Yosqfdtg4RQkR1Ho1jcyqJy3HJCc1jawiUgyxak1vUkMQUdgVCbmcn5zmcGF7c1ASWjBchVPw0VmT6LTcvuFpqr0GTl6Ok3PIYV5CkTqlqPZpzAZEljWNDrmeVVG4KQltRFe0hCCaEYbQguiiourYWDX5ndUwgpvVZI+HhKaJoTRP1AfZnhAHcKKcAw1+AuKGi+mBm5Hj0idD/IhCaYjOWK1w0uo9aENhVg3sEtR7QwxfxhmaCjOESq9BfSKFmVStkI0fUxiuFs878q2hrCDSDCHFfLTpYFXlDUAuKNeOLCkJ6fhr0YnierCqYFYNgogikl8L4fjxZpjGl3vsjZaYanr8oaOwrBBtybPEV60wEKG8GTgrCqNhUetWxCwZjFl/ppV1dvG9ruunY9Vjv336QNt7b+P7gueECvm9IhPp1VtueB/JKReARofNfX/9ETZ85t3EZkwS0xqnLFbTqoeYtZD8RodqnyJ9UlMaFUtlNsQAWFVwSpqOo3UmXhmV4yoKbULijIRDgijEZ+XeS1fWSCbqGF/qwI9L3HzVWJVGYf1/1Ji7NE72mI+bNljeJtdb/9kiizvTFLZA7hCUhxTVETE43Q+YGL6muN4gMa1RIVT65bh6b0j/NzRe3CC6EhA/XcLPNsdZaDDx6iy1EY8NnwyZuSKCm5ax9DyqqXcoUpMBxXUW9U4JqwA0spJPSE5q/DgUN0DPXvlOaYi/e4rxwwOs/7zPydfaWL1VAFKJOsZnO0mdcZm/KELPow1Ov0J2ofi0Qb1bkx6H2FKIH1Ms7ZB7MFzZrEZub3DqNQ7KUySm5LsgArVezabfPcRT/3cTm/8/j8qwbNyF9SaxeY0RaKo9Br2PVBl/o9z78FcDnLzL4oVx2UhLIdVu2WlSZwKCqGJls8nwV0vc+plbueG3fwWQDePA/73lGdeXUupRrfXF38OSPGtIq5zeo172LZ+1DXwb3w7fy/p4SN9JUS9/X2Pu3xO8jEMjY1DvkD9yp6jZ8Jl3c+LHP8LFv3UjVkNT6TMBiC0qSJlUBxSx3UsUrByxC5cB0F/L0fPySa7qPs49772c/FiUYFDcup7PO5QHTVauqdN9W4StP3+Y+06MAdDxtRiV/jg9Ux7L59nEZkLclMzXdS/bx/5Hd/K6d9zDP91xFc76EteuOwbA/sd20f3QCksXpVl6hUvi4ThWWjaozgMN3D+rULh3qBWL9ppGenDrHNPdWcb6Fpj/txHqHVmSk3JcrT/BxmtPsPDhURo5g3e9+ct8c0XGObl3E51H6hRGo5ReVAWtqPeJIVauQd83YWWrgT6/RP+/xFsJ2MULFHqhA50MmN0TQUd8Io9KosL3kphRjVNwCe0IjQ6LV139KAAHlwfZ032Ke2Y2MpNPkr071prPa7Y8SS2wOTK+DdVfw69aRA7brWfqZmH6nRfQebfm+Jsi5A7RerYq1HhxhZeE5a0xkhMyzkqfojxkoUKodyoaObP1phBZbmAdOkn655LMLq/jTb/zK0SaG35p1Pxvr70fBK4b2Nk28G08I87WujgnPPdE17Ae+Zt3Eh6U+INVEa8wNq/Z+3u3susPbqLaJ+Psfiwkv9Gk3h0ycF+INhT1jHh3ucNlli5IElsJmXoZZI6YqECOu+h/HuTU+zazeEGMyoCm83HN/CVyfdNVxGYVxfN8YlMWdglqzev1PhQyu8eg/8EAo6FxMyZmQ75b2mbS8VRAdNnHcENmL48RNu1MbF5jurB8AWhTowKF3y0GfN2/GthVn+kXxYjPamq9ishKkxETUVg1TWhKmCJzwm8lYktDFk5BY/ia+Ushecqg4+gqjQgmr7HQtiY6Z5CY0cQWJWGcH7Oo92iyT8HKVrDKikhBDltNbmoDzBr0Plyh1i+brJs0qHcqkpMB9Q4DpWHpUrG20Smbwa/XWdgVpf++ImeuTVPrl+uNfCXk9KsMzLJBGNXEJw06D8s4p6+ycPISbqkPeGQfs/n9X/o4AO//y58iOR0w+eqAaKqBc3+K4ha5XuaIhdnQVIbArCtCG+rDMp+ZxxwOfvDbJlTPKc99FW0D38azxbfz3M8J457KDuneX7ulxcTwUprcIbAamlqnwf7f/BAX/PlNQJN2ZwmLpZGTGPBqnLt4nk9un4lVg0aHorgxIH1crK1d1tQ7FaEF3Qc8at0Wjaxcr9EBdqkZs3eg0albcVzlK+JzwiCp9WhyR3TLq48vhCxvNbFLkB0XNk7vIw0AlrdEcEripZZGmkHspz2Crsd9CustClt9EqcsGp3NEEoAyQlY2e1jVEzSxwycknxX7VeoQFg6iUmF4WlSU2L88hts6t2ayIrMiX7aO5mT12RONpi+Ikr6VIhd+9ZnPnmtJjZtYVXBqujWOO0KOGUJjUQKmvnXNEjf1wwf1WRjyD3ZYPpFUUxXPgMobA5IHzObHriEikrr1m4+iGnMmkIbGqeksJv3V++S59Po8enca9LolPsFYe242ZCB+zSGp6nlTILo2j08n8IyT0fbwLfxbHDOG/dE57De8JO3EF1uxo8ziuR0QKXPpNqncQqKx28RmuSlv3EjhU1geAq7JMk0sy7H1XoVkWVJLKamfErDVsuTrndprKrCKUCtF1ITmsqgzIldhkYWckdCVrYYmC44hSZ/PKbQlsTxQxvsoqbe3UzuLmucsia0FCqQDWA12Rpd0gQOBFFFvVuDhmSz/MWqNTeRDkViOsSphOTHxBpnTgTUswZWXeNHFfUu1dq8rKoW/r8F5SGIzynssozT8DSldQaGB5EVLZtA5+r9SRLYdCE9EdBIGy3uPECt08CuyBhND6q9T1srGuyKpp5T9Oz3WN5qtz6PLWpMV6MC2YSDJoWyNBZgrxjYZdWsKYDYotxEYCusukYbUO010CYtpnjoyJwanlAp02cCisPyAKPLslFWew3ic2FrkweoDBgc/vZUyHPWuIMYeGjTJNv43vHtjHtbFbKNNtpo4wWIcyKh6sfE+y6MrdEdrXpIbFGRmNOUhkwu/Y0bAXj4D29lw+feTcdjBm5Gkd/hgS2u38BXxFO3awELuyzSJ9a808E7Fjl5QzdWXZM+iXiPZpP50YCRr1SYuyyBUwC7qglevQJA5hMZpq6Bzv0GW3/mMA/es42Rr0jMZvKaKI2aYvCuErNXpIjPhkQK4qEubzOJz2jctFRvalvT86AEuoOEw8Qr4yitseqahZ0WiSkZa2gr7Kpm9kddEgejOAVN31eFJnn0Zwewy4rhr5apDCTx4xKeAkidbhDaUQJHPNr0jM/SRbJ3RxZMzDp0Pe5y6q2a7IMOtpBlMF1N6kyN+V+qU5pMo6MBmz4u8fEgZlEacnDKIZkTAUpDI9ec63s9Tr1R4SxYuL0+6/81wE2Jl937YB371Byn3rGBxLSmsFHxr+/9MwDe+Ne/wjvfcRt7C6Mc/ehWFi8JGf2chJZO/rhJZVNIZMrGrEHx7UWi/5YFQL95kVItQvBkilqvYvBen8I6eYsob/ae3QL8AaNNk3x+41x98zonjLsKYeUSj8xjTuszsxZCyiS/URgThU3y+YbPvZsTr/8IFx28ERVCbFIMAcD8bk18RhFbhOgCuClFJC/GduL13biZkPKQgVMQul6sSYU0Api5IkG9R9P/zYByv0npmBgVM6fIPCFhl71TI+QOaSr9Mk43G2K4BvUeIWPXug2iS03KZkVCOUFM4xSFO1/amAYgNtcge0zCJkagic3pVmhChZryoAlFMVymC2E63jynxNNrfVFi8yKF4DelAsy6T3lQYZcld1DvMOkcFRZRdaqL7PGAao9F6oCBlwKvmTcwfLDLJtUns0RqChWalIdlWVj1ECPQ1LqErx5b9FthruXzHJJHRY7AmbHx4wHTV8t3sekEob2BRmeIXTFQIVx3m4RNzN6Qv9r7Uija6MsCkuMWxfWyCX35FX/GW/7sl4m+cp75453ohSTFq8VwG091oh1NaklRGZQ5XpWISD2xxtJ5PqPNonl+4rt5Xj+I53puxNy7hvXQz9+CIeQH4WrHoDqg8FIh2ScUlaFmEnNaEn6PfuBWLnn/jVT7pIgGRIrAKWmqvYrEjMgCJKebLI0X26ROSzzZLkn8fNULjSwrrKrEslUoHPdaT/N6c5qVHSGJUyZOQRNE18JbXlK48oYnJf6ZkwErm8T6ZU6EFDYYeClNZFnYHabkWuU8MUUj0yzGyqlWjN90xWCHDmil8JOQnBRjtrhD0bNPU+kzqA5onLwiMb32/JyyMHu0AUNf98mPidGrdcumFymG5Dcr7LIiMy6ZSsPXFEfWirm0oVBhM98Ql3HHFuQeQ4tWnsJsrCW3nYImNekzd6lcL3VSZCBaydDUWm4giCjCiBSUWVU572oOI3s8oJEyMAKJuRuuJr4g917LSR6iOCqbRXxOt96SCmMmh//o+Rtz/89oG/g2vluc8zF3rSAzHooQlykFOdU+4bEP3BfSyIlBtkvgZhReUnHJ+2/kkd+/FbMB9o489o489W7N4iUBkcuXSE66hDbc8fGPcMfHP0L/Az7VPoV3fpXux+qUz3cJ4lpEyDQUN4fknnSpdyFiXrYmtDWN64rkDhjUd1UpjkHhAg/z2kXMaxfJjId0PFWj2qdw05pyv0l0SRNd0sQWPBrba2SfkuRqYkbjJ6QqM38eFLYENM6r0ehQhKYYddOVJOrSZT6NrMLwNQPXnWbh1XUWXl2n85DGrgQkpwL8REhlvcfiLs3iLk1pWLGyyRTRNQ+q3Rb1Lk29S6NCRX6Xx9xLAuyijHVpu8nSdpPiOqEYJmcC6t0Ks6FxfmwB58cWqIyEmJevsPhij/KQVMZ6aY2X1lS2NTCvWSL3pE/h6hqTP+HRfcCn+4CP6WrKw/Jc3ZRsEOV18uOUNLGFELO5kaOg85BP5yGf4oiJ6Wn8KETyIUFEUekzqPSJYU+fqlMb8klOSbK6OGpSfJ5x3L8brHrw3wm3Tx/4rn6vjR9OnBOee2R4WF902c1UeuUPNVIM0UpR2ATdB0IKo2bL683v8IhN2qhAvMeD7/0Q2z8oNEm7LKEKbQrDo9ZlYFWb96eFnWL40HHUY/4im1RTjMyuhkTyPstbIyRmhdO9SpO0S+JZawX1zXXUkkPfg3LK2csgedrA8KGRgUZnSOpUs6JyMmB5iymqkXl5U+h+TGL187uilDb7pJ+yiM+FqBBqXc34+EqIH5frBVHxeBNzEpOeusqi44hsgFZNY3qaRlqOW2W0dBx1qXfaxOZdJq8RrmBsHhKzAaUhkaiMLa7lBgJHYdVDFrfbqBDGXj3O3IfXt55Ntc9oed1WTbe8cT+mKGxuCT+y7os1qgNyvUZGkTnhMr8rQnY8oLDepLRV3qA6H7JYujhAuYq++xUrWww6j8hJF3YaGFvKJG5PYnjCye96TK6d32xImCsCXlLT82hIbFHO+Z6P/itvGNv/jOvr+ea5r6LtwbfxnXDOUyHjPcN6+D23YDWTfNEFTWVIEbl4GevzHbiZNS700hUeySMOXko8dv1glkM3C01y5/++ifLlVXpzRWqf76XzcJ3j75ANY+wfQk69xqFr6yLGJ7qYvUJjdYuxdQ4kaHRoUqegPCKUxRtu+hoAH37garIHbIKXr1CtRkgl6nhBkzt/Zwa7oknM+px5m8/IJ8xWKKRnb5np3/SJfj4rRU8K6h1iiGs9EG6ukEzU4bYcWik6n5Cx1Lod5l9Xx887DN6lyL7nNON3i7G1yzBwV4HyhiQLb6rh1S0iJ8WgakMUG6sbXAaGlrH+uouVLRJuKV/QwHAC1OkYQTLEKhskT8lcx5Y1blLhZkQnJz6nKV0jwjpeIYKKBWjPIHHMITGlmb9KNhoVCXj9tgPc9tnLiFy6TOWJDuyCrLHugx6lIUvCMRVNfo+LduXezaJJkAxR0QBrxqHzcaGSgsgzxGc1+fNobt5rPHerIonuysvLqIMp4rO6tXHntyiOvv/5yXP/TjhXk3VtnBs458MybbTRRhttnF2cE557onNYb3vVL7Y8OKUlQWnWoPehEkd/Kkb8tHih2eMh87sV8RkpDorPqJaK4YFf/xC7fv8m3CxkjwXUOg1KzQhDxxNQGlUYjbXqytic3HtlUBHENLlDQo8srVNkj0nYYmG3YuR2l9PXOcSnZXzldfLd2KdrTF6ToN4TkjlqYNZ1q/rTKYUsb7HIngiILHugJBwDksi1S4rseIAfEepjPdv0bF1NZdCgkdN0HIb8ZojNNz3bQU33fk1hzCC6qLGrmvzmpvKjBYkphR9fS3S62WbCeEkRnwtZOV+x7ktV5i5JYNXWirTiCyGRlQDDC5l5cQQ3K/cXnzIwXJmr+JymOAbJ0zKfjaxICAx+vc742wzSB51WsVVhh0fHPovoiqY0bOAU9ZoqZFRheqKwGVrijadOyJvCxKuTpE8IiygxExJaqhUeS04FNDIGVk0TXQmY320zcJ/QpPKbojz6ty9Mz30V7RBNG/8VznnhMJCYs11dY1QEDvS8fJKl6iC5fRJ2AOGMx2cUTklTPC8gfcJk5Ufkj3zX79/E/vd/iD9e2sQX/uBleGlFkJD3+uiKlLovvblK7AsJun56giOPrQNg8J6Q6SsViVmPyZc6dBwJKY6KsX3p1fs58sAORnZPMekNoscqvGHz4wDc/+Aehu6ucPytUQavP8nc348y+zIJW2z8u4Br3ryfuz95CcawiRFAtV/uoe+COWaXMsRevsjMHcPEFvS3MEse+IU/5xU330y12+AnX3sXD6+MArD0wVHMekhiSrF4cYiOhih7tYUTVHDwejxyvUXce7tINltU5K+sUTU1qUSd2eUcXLlC+ZBQPUNLY3gGyakG87tixGY1r3rNwwCcrHQSNX18bfDIoTF6v2Ew/3LJhF46dorxlS4qR3Ooqsa7skjk60L17LnXYuV8cDMGXQc9zrzFp+urq7IFa1XItR5NbN6gPJACRNtGK01oQWG9QeisyflGlz3i0wErv1mj+O+dmHWYfrFQUK3vk+TvDxIvFJrkC+Eeni/4b4dllFLDSqm7lVJPKKUOK6Vubn6eU0p9VSl1rPlvx3c+mei0eAmFlxDGhlWFq7qPE1sJm3roCm0o7FpIbDGk2qvoGCygTejNFenNFXGz8MdLm/jVzmNEFz2cgkZHQnQkxKpIEc4F/dNECiFZp4Z2QrQTYpcDtKUxaz7aYK3NHHBN5kkAbhr5OqEJl607xfUdD3N9x8OiOa4UOhqQi1QIbVBlC1W2MOs+G2IL0i0qr4X+mPUJsj5bsvNsH5pmR3YKlCRIY0sBsaUAqwb/WBwjPtsgUtTsSRzn0o5TXNpxCq0gOl/Dj0lR1OjoPHbMw455ZDoqxGcU8Y4aVw6coPOIR6SoiRSbyVArINRNwa26TXxaEZ9WZI43uepPTWNVNaGjuL7jEa7veIQLM1NsT02zLTVDJFcjsBXkbcjbXJEdpzcpbfV0LKArVcGqaKyKJoiAVW0WpC3WsZwAs6HludZFvtlPiHyCVdNEl+UnNq/RlkgNxBY1keW1JRJ5Ygp7roBj+WgT4vMhiWlNYlrYNWcTZ3Vtn0V8tyyacxltw/79w387LKOU6gf6tdb7lFIp4FHgdcA7gGWt9R8ppX4N6NBa/+q3O1e8d1j3/eovkphq7jVNPZP0SZdTr7dIP2WSnhCPeGGXRXRBvk9OutR6bBrpZsu4pRBtKqKLHnf9/cfY+Uc3tdQWy8OKzAlpRedmFOmJgPwGSYwGUeh4KmBujyL7hOjDVNaJKz36BZ/ZyyIM3VmhuCGG6Wqye2cBOPH2QawadBz1qWeEhljPrfVXTU4KvdMpyb9zF8n1evaFKK2pZ0yCSLPnaZMN5CUUkbymuEF0cHofroCScy6fF6PRqUhMhVR7DRKzIWGzyja27LOy2ZY+o+5qWEbO6RREX8aqagqbDBJn1sTBgqhsptU+uZ7havpvl4rY0oW9oNe4/4X1tlBFEdXL9Gmf2cssskc15SGj1QCk+6BHYZ2NH5fagciKboVXVsNFSos6ZXxWU9ogx6VOQGrSZ3aPjZcJsQsGjW557okJC6VF26eRUzh5qPfIcekTmkf+7uyFZc7m2j5bYZmno+39trGK7wtbRin1BeCvmz9Xa61nmn8kX9dab/l2x8b6h/WW1/8SF75DRL/PT05zx89dRX4sKpS7LQGZo2IYnaKoMlpVTWiLcereL2GZpW1RvLQUBAURxYFf+xBX/OLPAmA2RJzLrmhSZ3wKG+yW3G3HUz61nJy/1qOILmpSk17znA5KiyqhWZeq09UQShCTz7wkpE+FVPqNtXbJhoSQjGYjaW2sCV25GaSBRwCv+fl7WPISvDUn/MpPruzhrn++lMR0yE/81pf4m1tfgydRC9InQxoZRXGjJjYn8fBab1OaeG9IPWNQGlUkz2i8pGpJEwcRiWGXhg2sOqhgTfXSamiWz5cchlX9T/1TszL28vkNuu5zKGyCzLHmdykR9qpnjBY/f7XTVGVY4vzagsx4QHGd8P9loYAflbxKaCpe+s4HuedDe5rzqeh6/RmWPj2E0lDrVq3OW1rJvKtQ48cVPfvqTN4kzyh2b4rH/uq5K2J6Nmv7uTDu0DbwbQiec+OulBoF7gW2A6e11tmnfbeitf62r6+JrmG9/n/e0kq6OQVNfisEg3Uy90cJnbW2cNEVUSGMLfrc8fGPcPkH3sPS5XKgtWgTJCQM03OvhdXQ3P+XHwbg4t++kUZOUV7v03evwfwesAvypuB2BBiuovOgwk0r/OhaS7zqOpGfXb4wJHHaxEtp3HXiZg/8h0368DKzV3ehlSQYW02iH6xz4g02/feKXgwayoNyPW1K6XzgaFITyJtKMxbtxRVLe3yy+21iyyHLb6jQWJLYcvaQRWrKx00a5DcrDE/hpVY1aTROwcBoiGccWRGFTJBK01qfxuvyyB5wUL7GTzTlBzyIz4UkT9eYeXGCjmM+Uz/enM9TUdxBF2om8TMW2eMBK5ubm+CgjwoVQ1/TLG63cLfWSDws43RKmuUdms4DiqUdmsSkJFVBag06j3jUsyaVfqOlLCljkeKvhQujaEu0cBZ2yIT2PVjFmc5z/Kf6yRyThPtqK0RtwMG/fG6M+7Nd2/9d4/7dGO82TbKN55QKqZRKAv8G/KLWuvg9HPcupdRepdRev155tsNoo42zjrOxtj0az90A22jj2+BZee5KKRv4InC71vrPm589xff46prKDOncn97MyJeaH2iJreeOBGx93yEOfGQHxSalceyfF5l4fTd+TNP/gE+53yI7Ln9AxdEI0ZUAqxIwd2mEzsM+1W7xNPf+7q1cddO78KMGS9sVXQc1btN7LW6Uc7sdAbkDJn5iTeslMeuzdL5N1+Mu0akybm+ipSZZWG/jxyXkEZv3mPhRm8Rp2S9VCNlxj/KAJXo0IZQ2CLNl8J4QsxZgBBovaU1YYoIAACAASURBVBFZbFAaXWV+aEw3pDxgESlo0kfyVEeFhbJ8viV67qbEsbNHaxi+nDO/OUF+M8RnFYnZADdh4FTkO62kq5Jd1SRmG5QHIxJCohnvrwEK0qd90VtvvkVoSxLZkWWP0kiEwIZGUyM+uqjp2rvC8oUdZE7UWNkcY+liuV501qTeIw07ihsDeh9UZJ6S5OvJN6TJPinhoIXdig3/Vsb600UAznxhPYNfWeDEW7podAX032Mw+2IZS+6AgR8XnX9twMJFBokzzUblNX3WqZBna20/V2GZp6MdovnhxXMSllFKKeATSILpF5/2+Z8CS09LOuW01u/7dueK9Q3rTW++Za3lm4IwIkJg3Y/VmL001opXhxFwMyEdR1SLC1/rF6Nilww6Hw+aHZUU5QGj1ZGo46jPvR/6KNs/eBNOUeMl1sSsVi4MMCsGqZNGU7BLmnsApCYkzFAaMfBjkDmuWx2A/GYy0ikJAyS0RA8GJFSAgnKfSWI+JDRplctX+hy0IUJfizssoQA2JVLsCmROepy+PsRYcEhMS6s9gFrOJFIKqWcNQgu85FqcO3MixKqHNNImVj1kbo8iSMq8DN2hqHZL+b6bUnQcbXD6Ogl3OAVFakKaeVT7pf3gqtJkbCXAixt4CYXhSen/agI3vhCQ32iSmNEsXBYwcJdBecBo3buXhOyxkPk90vh6do88iEZ3QPqohVXRuBkFCroOyrxMXW1JlauSzcOqC2USZKOxyxLbN3yNl1rbgLVxdjsxnc21/f0w7tA28D+seK547lcAbwMeV0qt8rN+A/gj4F+VUu8ETgPXf6cTGT54CfATzdirq0ic0axcU0eFsabyYHPAdWFmVHvB21rFfjhO11bx/Er39bD05ioX9E9z8uObsSua5Z2S/UydNtj+wZs4dPOHeOk7fpqTbwIz3yyMOmRRHAvl90claVjeKW8D7tY66osZzMtWqEynqF9YoiMuCdzyp/uJLYUsbTWpb/LovcNuCVkN/eE3adwxSvDlAdyEGL1TrxL5yrCngTI0mUwV92gHtjJITci9l0YVXFsi+5UO0JB54xSnxnsByO2D1Bcfo/LO3ZRHNHqoRrgiRrrRqbAqFsa2IoMdBbKfHMKPyv3N/ngVNREjXFfDOB2jPOrQeXB19qUjVWY8oNZj4EcV1vXzAMyX4kQcnyA0qMwnyO03yW+XjSYyukxtPkNyysGoG+RvKBH/krxhGJ50uypsNEgfhYnXBnTsk6sFUZmfwiZwipA8rSmMyjjNmiK2oMlv1YCBVYPqasL4kRCrGjL9dpfIvgTJM2FLKjg8+9UaZ21tf7/wQuHBt3H2cE5UqCY7hvXY225ptWJzCgELO21Sp0OWtykG7vOZvUxc1PRJ3fLmuh+r48fNVpMISdCJKFZ+o0XHUx6NjHy3vE2RmJJQyV1/97dc9r6fxYs36XmOJDzrnc1NRNFKAEYKIQs7DXr2hVT6DOLzYSssEzjiCa/SGKP5kOI6uZ425G1h9lKTICbnyhxrjntvkeLGFF5CkZr0qOesFp1TG03Z4j6D7LiPHzXW2sn1i5xv1yGP4rBFNK8prhMDF10UOWLD06hQkqirbJnUpM/CDpv0KaFkGj7Um+0A3Yyi42hTatfXwu7x18TWal0GqdM++U02nYcanH6503wOECloAlskl0W5s8mpr8p5Ow81WNnskJgLsSuyyVZ6bUxXE13xqfZYRAohK5us5jhDtCHXD02FUwworJfnnpgLKA+a2GXhyxfGjFYVcWLG5a67fv0Z19cLoUL1u0XbwP9w4ZyvUA1tSE+s0RF1Vpofb/35wzz5wW3Uuq2WZ2vVpUjGTSnG324QO2lT7xbDEZuFrp+eIOvUCP/PFgobbIobxQD0fVNTHDE4+Sa47H0/y4N/8mE2/pN0dxq+02N+l03vXo/Cehs3I/LCANt+9SBH/mgH69/3BA/ct43aS6sMdMhrxMpnBkmf9uHmBeYKKar7UzQ6ZJwjt7tEf3MG66uj9O4NUCHMXib317gOXLeK71o0DkTJnApwmtdDQfKdUxh/M8jcxSYDe6bRTaqQ8bE+nHJAPWtS3KiplQzCZheqercmOaEobNbovgYd90Txm9rz87ttaiMe1Y2a9EGH4nk+0Rmrdb1qt0Fm3GVxRwSrqun4ceG5L5QTDKSLLNfilI514hQdDHHcqb+8SMkzyd6WIP/SGuFKhL571+Qj3LRiaXuEzEmf+d0W8dmm6mWhWYE6amNXNW5KGDMApWGjqaIpVNbQXmv0HVnxSU5UOPqeCOl9EZwSLdmJ0y+PPJvl94JC24NvYxXnhHHXBsxdYtLoaUoFTFvEZzX3nRhDXwKpUwo30/xdUxGb1TRyGqNokTql8c8X0nbssThHHluHdkKSG0wwnkZ3TGjx+vIWXlyx8Z9u5Pj/uBWA3eM34hSg3G+hDcg9ETB5nRice27bhXE+TB7aQjyvqC5HObEgyc9Yp0JpCz7Tjx6C7sc8pq6SKQ1iJuPzXaioZnaPjMVPNaUCjqQJI2AM1vDjML/LINXUbFEBjB8ZIDFiAJpTk11ET4rxinZCfM6lcr7NwTf/JTs++QskJ5pGMy9SuDsuHace2KyUR1rNT4obNZdsPcHxf9zMyoU+kXkLZ5X7oUQJ081E8OOQHQ84dXCgOddwNBuDkk1k2cCLgzcsc+0uxlG+8OqdJ+JisMfklIkpqUEobfIx6yb1Ph9Uc14iilr3amMSAxXQornG57UknteDnwyxKorogny3eGGE6KKDHatQGXDoezgkv7EpEb3y31t3L1Q8vZL1+Wjk25vT2UFbFbKNNtpo4wWIcyLmnsoO6Z7330JicrWwRlO6skb2azEKmyXRtuppGg2kSUVGOgOlTwUtDy6IQNehALscMHupQ9fjPpMvW226YeDmArKHLFCQPe61Enn7fvtWdj3yZgqnM1hlA6eg8MU5JzMuXX/iiwGRFQ83Zbda7UVWfArrHSKFkPQTBaavzbUqMbWCaD6gOGLhJSVUsdrrNTEbkj5RobwuTmlY2vdZ9WYRU0IRKWhmL4fuR4WKaHhr361KFfhRSVyq5uPzY9IsfOBOE7MRMvkyRcdh2bsDR958Wq3xTFqtCYOoJjYvypD5jQaD99UoD8iX2pScg5P3MOs+c3tS5LdJXKb7IZPYYoBVC5jfHSG0aUk9aFMqTflAJ3/2jx/mvW+7kcKYUIzy11VJ3R1HhfLGkDpJi0LZ+w1FfM5j4lU2qVOGNBgZlmcbXdTUO0X1suugz9RbPNQZOWfnIc3Df//eZ1xfP0wx9/+Mthf8wsY5H3NXniaypFoJ1WqvQTJRp9IfJzYrYZtGsy5w5CsVZq5IYFWlNV73AZ9GswmG0jB9pUJbJv33BtRyJoa7tnmZFYPiWEh63GB+l91i4Ox65M3sv+ST7L7tRiqDYLjgNKtl569xGfiyxcJOi85DCi9usHithCY2fERj1TTFUZPyUA7DBTfZ5IGvhCxcaGFVIXUmRAWwcJGcs3heiOnGKY6YhI4Y0NUkLSHMXQpO3kCFIXMvDlBRCVcNfsEitKA0bGLVNKX1awJdoQOqYZAfMwhiCmdFmm+AGGl3a42gbjLyOYOpawxU0EwKR0MwDAY/c4ro8hDVHoel10rXFL9ukzgcodFh0rs3JLTAaMhcL7zIJ3PIpvfhOrVemyDno8dXmTshuxMrzE4qrn/oXfRlbDLjMmf5iTi5J+qEjkG1N0Ji3qM8bzefO3Q8skxstp/EjGzSbjMhPvTp08xfO0IjB07BI5Ou0Miv1gasKWO28a1ox+B/eHFOeO7x3mG96W/fgb4zB0hru9CG1JTPxGsU6acsEtPyB1wZMKj2axKTityTLiubpZwehGWSmPUwaz4n3hCj47DCaOrAzF8WkD0s2jKlEUXvXo9yv+xtyztDsocN9v32rVz0OzdSHlmjZW78lwqzl6foe6DE7OUp4gshqQkxVDMvilMZCtn4yRoqCBm/IUHvQ3K90pBB30NVpq6KYwTiLffsW20cCm7GIjFVp9ofodolBTogyeWe/S5TL7FJnYTuvQWUJzcx/tYcZlXRddin2mnSta9IdSQBQLWnGX8uSOLYdIVrDtLEO3XaxfRCCqNR/BitxtP1DoPc4SqnXxkXKuK8pvOwVAzXu6MoX5N4cp7GaCdewuLMK2WcnY8axOcD8pssIivCYJm9RsbZ9YBFcYMUVBW2BmQPGyLBABQ3BmhbY6+YGBvL9P5DjNOvknFaJZPOxzTz13qYsw7pE+C9SsRl/L0d1EZdcg/buFmFVaElfpacCrj/3375GdfXD7Pnvoq2gX9h4pxvs5dKD+mBX34vxtMqta1mYY8fF096NYzgFCAz4VMasqh3QeehgPnd4k12PAn5LeLpp49DvWtNf8RwRU+lMiAbg+GvsS2qAxrlS3u9R3/nVrZ+9CYCZ1WzBbr3a/KbDJJnpGGF2yFGbOO/eBQ2RAkdCZnEFkJKI3LSzid8ZveYZJ8Ur5oQ8uc3e7aWmhowHdK0A72WFGzkwI9rEmdEeKu8LsRw5dn1fzMgtBW1nIGbVjRymkjzuZYuaJB4MkK9OyRxxsAIaBX5FDYqtCHFV7EFRb1Tk31KrhdERMkyuqSpdyt69nlMvKbJenEVVtnArCtCR/RqyiOyKZgNReoEZI+7LOyMUN7okz0om2ViPmD2ckVywqCREy3+uBBwKK+TZ1PrUa0xlkfku96HA6rdpjQH98BL05L9NevSG7ewCQbvDciPWRS3SIio6xHzBd+s42ygbeBfeDjnjXt0aFhvuuGWVsXoqp56bEGUFqsDa0YsuiRUutXOzKtdh0CMefpESBBR1HNKpGabfG6jycqwy6Iq6MeFFQOwssXEcKVDkuErnnjXhzjvb6TptjY0iWmJW9sV6X60GnopjUJ0UeLziWlNrVe1qi3nLrGxS2BVNdEVjTZoNQCJLWpiSyF+VFEaMUhMa6q9ck4/AdmnQsqDouAYWQnxkqtl9lBap4jNaZyy9B5NnZYdcfrFIrZl1kRqOL/ZaCkqEoqnXlwnHr1dXutbatU0tR6FWRcZ5dhiSLVbxlkekYIubUBxAy1GD8icRwpSuWuXRDUztrCWG/DSTRnjUHIDqzmFeqdqFa3FFiC6HBI4a28t2oBqv8Iuiq57oSkNEZ9R0onqgGb5PENkjCtN1k0DHvlE27h/N2gb+BcWzv2YewDlkZDovBgVuyKetpsSNUOzvuaFBq9eoXQsi10UjrdTVPzE278KwBf+4GUtA1odDOh6PKDWI3HgVUNR3tmg8+sR7BItumNkTmLsfkJj1uC8v7mJJ39Gmm7v/r0bRQZ4AVaursNChNSJpjGKhFSGNZs/lufkmzrIPhW2CoCGvl5j6soYdnmt+Gg14RifD5i5XIqbMsckOZo5KRtNLWdQHDWobnLJ7nMorTMYvEdi4Gd+JM7IbSVK6xPMvDTAWrFwShKvTk5qVs6D+qBPZbOm5x67ZRgbgx7VBZvoMpQ2+iQmLBJTq3UDIYFjkH9RA+0apJ5wWptsdEnRyMnzSZ+U/MLs1TLO+ITNyk4fsySc9sS0al2v72FpcVhcL+co7ZIG3QB6NkrP1gUiZsD03n4ieah1NQuqOjSJSWBbidJylFrearXuy+9yUXbIbNQBIyQ+p/BScly989msvh8uPN9pkm1892hTIdtoo402XoA4Jzx304Weh6GRXXutrwzAdS/bx2N/uJPFC8w1auInMpg5hTY1jeuK2Hek+fADVwNg7Va89Or9XJN5ko/+3BtY3BGhuq5J3XtME9oW7tY6kYLNtl89yD237ZJzjmvmr3HZfKvL+PVJVKDZ/XtSvbrvt27lZW97J9f8xf38vzuvJj5a5Hdf/XkA/uTX30Z8psHp/2WSiixRKXexcLFM6fAdip6rppl5uJ/McS1Kl1dJyOaqnQf4yvhWRnIFZhaGyJzQLUEup6y5/Gf28chf7SaIaj7wjn/m8A1DABT/5EpqAzGqvQbOoqJj1wJTfVLdFUm4JO9OUUgYxAbKeAmbjidlPhfiFkF/g7HLp5j49Bhcu0zpYUleK22iQtjyf6qcekMOsw7/+1f+FoAv5ncSMXyW3QT3nRwjeW8c5Yo/cO0bH2bf4jD6Iz0U316ga0+Z6McGAVg638TNSuw/M+FTXm/R+ZC8QVkNjX64m6URg0RFwjPJKXHPaw3p5hS5N0WyshaqAhj7VJXQMSn8TpHg093UeiT5C3xLg5E2vjNWPfZ2iOaFjXMi5h7vGdYb33oLpVH5I0+dMuh5tEplIML8RQbpk6JVAlAZDsg8YVLYFpA7YFDrlpZrALmnGrhpMa5L20zsCi29k0ZWkZwO8WKK0ij07A9ZOl8YJvFZTaQQUh4wiS+EzebNcr2BbzS48x8+xhU3v5vZyxXZJxXZcWG9FNY7uGlFajKkkVIkZ3zqHXLO4gYDJw+NDvBSGm1quh+VcSanXcqDDuUhA7sI+YtdYifWaISDXxd1x2qfyAnHZ2VTOPV6i+S46KtU+4T73/2YMHf8mElhg00jA04JUpMBhaaImV3WGD4kZnymrrFY/7kq1QHhiDsFn+J6h5Wtmo7DCj+u6D7QFEYbiqA0RBc9CmMOhrvWxCQxF1DtMqHZ1s90dUsfJ3PCpZGz1wS99Fq+oeNYwNL5JpEVSE1JjYKTb4aB8priqNH6fxBV9N8rD/f0q7IEEYjNgdKa7DGX2csly64CeOL3n7tOTM8G51rM/T+jbeCf3zjnY+4oSE4HFMdkjNU+zdylcV73jnu4+wNXUBgVvjhA534DFWgSp0wK11SxD8cJXi5Uk9OjWUZ2T3HTyNf5yE++njMvT7Q01Nd9MWBhl4N52Qo9n0iz/n1PMHlIpLi7DikWdlqMfLnE6VemcPK0yt6v+Yv7ueLmd3P/Bz/C2Kd+ltwNk7yuX2KW//S/fpSOp+pM3BSSStbw/66D+T1imDZ8rs5lf/EIn/vUlXQdkJ6hq/Hqvi1TzC13srlzgQMHN+BM260iLbtsEL/5DMGHh/Fjijf+2lc5UhY5gPpfbCM5WWVpW4z6+gbRXIUTu0QnWRk+nV+zKF5Wp6dnhfBDPS3NlvmrPZQZslixsEpw8vVxkqdlrr24Qb1TsfkTeWau6iC04Uc/fDcAt81upyNaZUN8kf84tR31jSyVYZnPLT95jKV6guV/GqbyuiKVlRjrPtPcgAccvDhUBmHwPo+TN0DH3iaXvdsgdVo2p1qnQb1Tt3SDqt0G5fU+nY/KphHaMH21FDiMfDmPcn3m/hi8u7todFhEmwnc4sYfvIPyfEWbB//CxTnhuUeHhvWm62/BF8o2ZkP6mk69xGToLp/pKy1iC2I4LnzTIfZOjRC7I0VxDIK+BobdVAd8OE69WxOa0Pm4SAOvJuSGblvm5PU5Gt0+HY+blNaDk2965/fXqHU7lAZNnJIwW1aubjYZXYi0mDnjN3yYDV/9KSLj4vVqE+IzGqsGS7s0uYOq1WbPCKA0AiqUStrQ1qz7snjE87vjFHa5oKHzAZvKkGptXmZNqJl+MiQ6Z1Ib8ei/Szzw+YtFWTIxF7C4zUI/TQo5PRFQ6xLJ3uKmgJHbQyZeK9/FTtvEFjRuWlEZkiKuxKxMjOFrDFdz5jrRUTfqiujimpSumw1xCgaRZdGZn3nRmjxvdM8S9Yc6MS4qEL0t3fLUVQDdjxY5/YoMg/dUOf4TDpdsHwdg8q830XPjSc4UM/h3dVHv1Ix+UfiqZ345JAgM9HhCaJdjPslxOWllJEAFQuk0XMXol1yqvTLZM9cETPzMM8uqtz3374y2gX9+4jlts3c2oIGuxxtoJXTFyLLGTRs460u4GZPcEY1dlJ8H79lG+vNJgqjCz/moJYdUok4qIcZYj1W4/CWHMV2NVZWQiJfSuL0JMsc16YES8fkQa32Z6jqP6jqRFPDihtAFx8CuajHqCxHio0WyTyo27Jhiw1d/ihPXfpzci2bJvWiWSF5CC6X1ip4tC/hxRRCVn679JUZfdIboEsTmhAVy+roYp6+LUdjlcs+P/CUDg8t4SYVdgsR0KIVaCpwtRYZv18RnNX3Dy8xcHTJzdUhi0qB7X5lGyqQ+EMD2ErXLytQuKzO3x6CeUxQudEkMl7CLPj3fsOj5hkVtvcvypR613hDthBQ3BvgRhR9ReDFFecCk61GDyKKJWVMYl61gXLYCO4t0nb9IY3ON/HafRsbEcBWGq6hvamAaGtOFymKc5Z0hXQdrdB2sSQPyLSka3SFhxKRjoMCBb2zmwDc2Y1dCnvz6GMtzabLHfcyGYmlbgqVtCeJfSeEuR+V5lzWpY9J5ynChc5/B0J0h0YEKhqvwEhbVbkNom067QvXZ4uksmmeLs3WeNp4dzkYPVVMptV8p9cXm/9crpR5SSh1TSn1KKeV8p3M4ZU1hgyMx8oo0UV7epnjpumOYDaFE1rvlZ+QrDVSz6jTXV6DvQfACEy8wKa8Lee3mx3lP351k986iAnDXNXDXNdCmIohCR7yGNpXI9gYKAjHGi9fWSU3UcTsC3KQidcIgdcLgD7Z/nuy4y+v6DxAZj3LFwTdw/47Pcv+Oz9J1sIFd9qmPuMRtD6uiKW7xpbhGKTKRmhQOLYYk5kIaXQGNroBXXnCI1x14J7YR4iVFO97whA9ulzVvHDsgnYeqmg+e90l+/SVf5Ndf8kXRrQlDvARoK6QzVcEwNIahWX/JGZyCJpGrsa5jhcUdMexKiF0JUXWTXE+Rvu3zZA9bdI0tU9ioKGxUFDcYhI6i+2unsUuQOQ4PXvwJHrz4E7x1815ePXSI153/GMm+stQPjLjUR1zefdG9dCfKdBz1wdL0jy0wvzsuPxfZLO5QJE8ZhJaiUIgzeLfH4N0ebspg8N4GnQ/aLF4g6p8Ss28m1h8w8WPChTdrUsjkpaHnjgmSh+fJJGp0HtIsn29hVTVWVZN74DsusR/Y2n4+YdXAP1vjfC6/AZyN+3u+4Gx47jcDTzzt/38M/IXWehOwArzzLFyjjTZ+EGiv7Taet3i2DbKHkF6TfwDcArwGWAD6tNa+Uupy4He01td9u/MMbMvq3Cs/QCPXLKypKIbvKFAaS7Gy2SD3ZEAtJ/tQZVDhZkNicwaZ8ZC5PZAel++691coD8dQGpa3GthliUUD1DoNAkdh1aUYx00q6s1mz/0PNjAaAbOXx+l7sMbEK6OEERlL3wOaetbAKWmWz5eqz66DUhV65z98jIt/+0bSp1ymrnZInqFVANTIGhietPlLH7VaTa0BcofKTF+dwqpI3N7wNPVmIY9TgMRMgDYVc5fCuq/4RKbLAKxcmMVNKbSh8BOQOh2iwma3pVNVJl6ZIrYgzBhCKI+sqmxC5xEJxRg/Of//s/fm0bbdVZ3v57f63e+z9+m7e25/c5ObPqQjkNCoIIKFBQgKyNMXIPpEkAcMS58lZVnPN5TSKitB1GEzUANaWBagCJLQBEhIctPde3P75vTt7tvV/d4fc599ogNRyQ25wTPH2OM0a+/V/PZvzd9cc37n90vr06P9sTd9jeFDbTeYHYWf1ez4nFxfZbfbV3XSRk/wu8fHY/pgRJpf/PAf8Zt3/TiGH7N0s9QiEuvSeJZ82yLxb4+wfJOJ1ex1GJc09Rnhts8fNajPQGphq5CutOTs/UKMXTX6XO+bIiGGr7j3pz7CB/7dT7Jyi8BA28Nw8pcuLlrmYs3tF0rO/R/bdg7+hWHPZc79t4APAJtJzyJQ0Vr3bkXmgYl/bieVtQxhQqhbi0c0iXXN+tVZsscqDJyIKF1m4jSk5d5qg7dqkFzWpOc7pGcNoQVoaubvSJGea5Na6GC1wexA9miJ7NFSX3YusRHjZxTZuVBoANY11Z0O9R0ezcmY6i4Pb12cmTYgudTtdWB2SS5p8qdDVKxRseb6/+fdPPLhe1i71qVwTNMcV6QXfNILPn4OBh9vMPYVA6+k++RaZldT3yUaoEOPt4TTZinCrov6kzYhueyzfrUic87AXWmiPQvtWdR2GYQpRWdQOnZTy12yT1fJPl2lPpMkc0GjTeFlMX36aa7EmkZFGrcWkf2QR3oxIjsbkp0NcatCP8CBBiqWQjBKXsMPlhj6ZomBUz6DT3XYuMImUY5JlGPcWkSnYPAff+0drF3lYFW7dEZiOiMx1d2QWglo/ck4rSETu6GIHeHYiW1FMBBhl4UfZ/TBmNRSRGopwmopEiuyeOePGkKbsKRILinivS1e9qrHmPxik59+73tYeHmOwSNtBo+0+2ibi2wXZW6/UO1i5uC37fmx7xgKqZR6DbCqtX5UKXX75r+/xVu/5Z2nlLoTuBPASQ2w809mKb1YmnW0AbW9sHFdll3/M8Sum30ulIn76nSGE3QGTJZuSWK1IbUs99v6NSan3+KhvYipz4SU9lss3z4IQGopRivYuMwktaThPWvwl2OAUO7WZkz23NukdEWSMAH7/qCHr/4Vk/wnYi7cFZP/HKxeZ9HpkWft/DOfQ791F0/93N3s/8N3Y3Zg4XaJXrNnNafeniRzymDgVAAalm+S4fZ3dHHPeNRmBMYYVAys1havzum32Izfr+nm4Myb8zj7BCc5cG+Mn+px22tYviFBbEt3l7YguagJk8IBH7n08eKtMUVz3KI7HDL25Szl/Ua/NyBMgdWE0T/yaBc1nYLi7DtkmzVfIChEWBUT0xeMeTcr8cDGlRpvTZ6M6jMmJ9+R5sDvCH70/JtGqE3ZWF1Ncj3G9A0S6z1h7fU2A8c9ugWDzKkalcszmD2yzPypmOoug8HDBrlzHZJrTh/tNPDHDk9kr6Z5gyB9/LymdEDGenNuXCy7mHPbI3lRz+27adswyRe2PRuc+63Aa5VSrwY8IItEO3mllNWLcCaBxW/1Ya31x4CPAXgTU7px1Xj/JvXKGuuIYuMHfAxfkT8T9p3K8q1CUh5b4Gc1kaOYe6s4jtzXbSbecI6C2+T4Vc+tbQAAIABJREFUVy8nez6mXeyRda36hCmTzt6A1JLDSjWDlrWE4a9XaUwWUFFMkFKkFjXn3jgAQMbdoJtRZNJtNq5JMLx/laQtTUXzt09QOBaz/w/fzYl33MMt730Xqzf0BrYDBw7OsfLEDmJboZXqNwApU1O8ZZlq28P52xxWW2M3e5TGIyaHDs5S/+sp/JTF6DXLzC/LuThF4bwPUgbVveAXQjB7/sXQqNChPRFhD7UZ+HSS5lgP0niwQbCSADumdNCkO+HjbUgt0C1LtB+kDFojiuaOiPFh8fzNnEOtnsAqtulUPApHTZZv6Y3Z3nWqUwmWsmniHoPmyu3DMmYXROe2usdg5yc2qPwXi/aXe+M5a9IeNOjmFX4qS2xDekHSQCvXe1gtWWxLBzxKV0Uk52WKZs8pkei7tUHzQoqhR3VfI7Y1821m6XdmF21uZ1Xh+ccaPwvbdvAvXLsoOPdedPN+rfVrlFJ/AfxPrfW9SqmPAk9qre/+dp9PDk3p3W97H5nZHgxGQXmfidWSR3ltiBIPCBNge8ggvRjRGDOp74zZ8Tlx7rUpu4cpFwbFTkGUiwBa4zFjD2jChOTdSwdMhp4QJ13eZ2P4UN8VM3gYGlMG6bmes50wKB4JCBMG3Zzwrls9NsLYgea4IDvyJ2O+/l8/ykvfeScAlV0W+bMhtSkLsyu55CDVI7oalJRR4YQQbOXOCC89CHtje1D+V5tx8CoxtelNrVBNY0oR5GJ2/E1A5BnUJ8T5qQjstia5HBA7BpU9dp9sTdSbNLWdBtOfKbFy6wDd/CYLpZCXRZ6MV5DRfUUspQXF4zS05N5d1adJ7gwqYYHUQilQ2W30x7q9u4uq2WTOGkQJSSFtRuC5cwHLNzhgCMQxe16idZDagFvVNCcUsa1JLqi+wHl6ziD0oHA8InIU1V1SiwFYv9LkxC8/Nx2qz3Zuv1Bz7v/Yth38pWnf7Q7VDwL3KqV+FXgM+IN/7gNWO8Yt6X6UrWJoTYdYWR/nqSSj3+yyeq20mrvVGG8jprzXxNvQZM4bVHbbvW2a5ZeHqIZF5pwBmn5TUWrWwAhDajMmVhu6A7ovZl04KgXWkYegsttg6Imgz+64dr1FasFk9UZN/pg8May/SJzKyAMGk/d3WbjdY/UGeOk77+TLv/sxAG5+/7tYvNXEqUJyRfD7QU8ZyR+MyJw0WbvaYPhwRH3awW5tjUXpRRFeycKpaxZeHUEsxzMPO2QuCHTw/A/JNVs9vvrYgvSsQX1amq6cqsZuiWOMHMXyHRF2SbF8W4HqwQhv2eyPdXI1BANaIzaDT2qWb5XPaS+mMFpl9aSIhEz/XZvTP9FbTCxNeMbFroERCczU25BzcR9xiVzh3ffTBvVpg4ETcg2VXTbDhwNiV8niPGUQ97j6U0sy5kOPhZQOWIQpyJ2UeZu9EJA8vcHJ/5gj87UEQU735RU3U0zfBftXz+3vFdtmk3zh2UVx7lrrLwFf6v1+FnjRxdjvtm3b823bc3vbXqh2SdAPpIpTet8b39tn94stQXgUH6+wdn2eyFV9sYfGtEisZWZjEmsBft4iNSdhb3s00RdzXnxJBhXD0OPSubp+hUd2NiTx199k6edvYfCJLlFCIr92wcT0Nc0xg/zZkLWrLCa/JFQBQdJi9Xqb8Qc6VPa4DD5WByXnufKiDH5OuNStjqYxZvTl677xGx/llW96B4u3JUBLamTkEUkDBSmD1EKb2Dap7vboFFVfaSqxohl8ssHZ16dxS4qxB5rUZ6RoqmL4g1//CG/78M+T2IhwSwFmS6qRjZ1patMm+bMhZjumutPui56MPVClui9D9lyLxnQS048p75V13a4LgqhdNBh+tMHqdWkGn+xpqKZt7EoXf8DFKXep70zhlTfBIhC5Bq1Bk9gW9aXK7q1YIX8mJEgIIiZ/ymfulfIINXRY8v6ZC5ogrQjS0pEM0C1It25mXgjYurmtOoXVgsqVITs/FdMatugUDMxuj3CsFPONT2zL7H237LlK0Wynfv71dskrMSXGpvTBV7+XygH521tX0o7/jkUafyxos6DHO6O03OiNSUX3ijbukQTmjUIcph8Y4I4ffZhdiTU++eEfoD5l0Nwh6YDpv42JHYVx1yr6vw+Tet88Z1YFSTPyZx5rV1lMfbHF+dck8NZEqQlg+CWLVD49zmt+8qv8xadfzMwtc+Rccfxn/mQfg483OPX2JAcOztH8yCSLt8qCMfPZDl/4xB9y1a/fJXQGQO1l4jQtO8IwNDeOX+CrXzoEMWQuyPE6BcXLf+Rhjv/sQRZemuKm1z3J8YoUKoM/HyF3tsP6lQn8l1VpraT6YFYVKVRXkd1dYW9xjYX/trfP9dJ8Y5UoMui0HOK6DQZCzoWki6p7YervAzpFiyClmPzxswAYKibUJiNena/Pz6C+maM5I879uivOcvjcNOnHPVoTMfZ0k/yn5EtKLfmsXeXRGdRMfsnn7JsM8k+Il9YGfVZLt6JpjUlXLEBki/yeV5Jcvp9VWD2Kn/wpH2+pwcqvatqPFhl9KGD5Rtlndzji/Lu3nft307Yd8aVhLwjnvudH30f9GkFNqLKNuyHqRWFaU3xCiqggcm5RQm7+/Alx8umeqpBbi6nsMtEmZOZiWiMG6Xlx7t2cQWwJjWynqLHaisjrRX5rEjnHtsjlhQn6BcDGtDTOGAH9vHLk9bbNhIx9xaA5KlDG2FL4ud5FKcmHP/HBu3n96VfimSFn7xEWys6AgRFqKgdjio8ZlK7QeGtyfckVTacoTyedYTC6UhwGQIvo9Y7PVjj/ujyGL+8HyMwH1CdtWqOCKd+U+ANpACpdoRn7GizeHmNXzP6iEOQi9v1hm+reFI1JQ7Rle1PF6kBtV4xbMkguaYYfWOPEu2VBNDqK5IqiNaqlCUnTj6RFhFxI25yaFLHNnpM2A019GoxA+Nibk0JCtmmxq4kcSKyqHjWD/N/b0LTGFI1dIVgxybNbnf8qhmO/tk35+922bQf//NslT/lrBBKRJ5+W3ITVlugucqCWE0cRpnsIFROcmsLwRd/UrWw5FbcUYEyZuGsapx7TKRjEtlx35CnSixF+yqA5qRl5JGL5Roleg7QsBs1xg9gRKOYmjDB3WlO6XDP4uKa6V7pOU0s9RkXfInI0A6cCYlvRKpokV+SaWiMKu6V5/elX8qk9XyDSMS9t7wMgvxHQGrFITjRQj2Zxaka/AzNIi2N3azFK9xqSuvTP0zcVYc4ltmWB2kSo2LUeS2LvySaxqvuRe+yAWzJAR3jLFqa/JVvYzVtsHEoTpAUJ45Y0bq1XUDVAGyZOTRPb0DhQwK71CpxnATTeOlidiPqUSWpl63tojbhoU3iC7MbWgmj68l1v/rRaiiC7tch20mA3JKqPHUXunAxMZbdFN69xV03ckoVd39LHdavPf4Dyb9G2i6yXtl0Szh0kJ70Js9PGVuSsot6i1Lt/ta0ll6zpwx777SVKOixV/Iz9bd73sSwMm6ZitlIavZb32JT3bXanbh5Xm7pP2wu9Ls7eOW+KdWsl1L6bUe+m0/XMkEjHmGqLfnjzfXrzp6HRxtbia0SC348thdnRmEFPXNo10JZGK3HEaiv9jepGGJGcz+Z1buarN8dz87xV/Ixx6f29OY7/YOzj3lgqtvo0N4dl83QNaSJ6plC50s/YZ6/esFlT0Ib8rqLNawclpQhBNilpyNI9PvfN3ofY6X1fWq5981rgH36v27Zt2yZ2aTh3BX5GnALITd0ck4guHPJxKi7pOdk2/GCV+p4stR0mjSnF+AMdNg5KWNgc9WiNaaJ8iLNs4ZahMSFeuj0aU3w64PyrXXKnFMs3mYQZ8ViJJZO162DqCz5z32fhrht9Hpi1lwSM3GexfHvE3j8OmP3+BOVDsm3wEaETWL7JIrYhuaj6cMeRRwIW3u5z9p79vLS9DxXDV//H7wKw83M/RfYpRWslhX8ZpOYUqWXxVrUdJrU9MVOf15QOWNR3i4oTwOjXYkYe7nDmDR6xFxGmFdlT4tlO/1iGxAq0JyKUr3DLisyC7HPu+xTajal0bMGrJ0CF4jStDiR+dBnzT0ak+Hmmy5kf63lLDfaGQeRp4kRMyYswexDKxqsbJL6YxoiEe6Z2bRf9mHhwbdg0pmPMjiKxpujmYfrzUm9YuyrJzKdbbBxK0CkqMrOa1dtklRr+WwOnGrLwEpf2sEKFsHKDfH8jD0coDavXGFhtyc23xuT7y566JJir/03aMyX7nvn3tj3/tn1XbNu2bdu2fQ/aJeHcjUCk1syOdG76eU1nJGb02mV2fFJO0WrLK0o5JFa6OFWNn9WsXuPRHhZmwG5BM3pohZcdeprhwzF+rpeeMWHiyzHNUYd4uMvQIzXsy2rYZQO7LFwlUUqiQLtukFjXpJci0ksRr7v6cdKLPlftn2X12iTdyYBX3fAkr7rhSQpHGhg9zvhoqkNnUCLn9kREkDKw7IjOgIHVirHaMTs/91Ps/NxPce4Hfl843w2IvBivFBMkDYKkkIxN7l+lmzdQGq67/hS3XHeCW647gd2IaY67aEvjbIgUXWtC05rQ2Dua5M5GmHUDPRDQHlWoUKNCUS5CQeuqNkOPh3RGQsKU8Mr4GZi7MEjxG8t084rFWz1ef+2jvP7aR7nx8jOoXU1yu8uoUKFbJk5N4dQU04UyzTuaZGYDare1wdAYkaRLygcUZkdROKbJzEVErqY57tIcd8nOhbTGXFQvfRZbUHzQovigRWQr1q90KR6JSKxqzK4UVhOrisyJKsnFNsFAzMAJHz+rGTysGDys6A48n7N320Ai9m2ysUvLLom0TJBWDDzdIPLSAORPx2gTFofy7GyFDD6laI5IOuDCq5LkT0GUUMReRH2fxs5JxdF5PMXyRo7BRBOlNalF3Re6NtsROmugDE1tTwbfb/U7I7Nnm5h+Ej9nYXQhsRGzdLMc73NnLiM/4bBSKtLoKSj98ONC4x3cniE9F+Oe8Sjeskz5aILMSflcaqFBxdAYoaY1YqEVZJ+Sc9np38m5136MPfe/g8LnPWGZ3KSI6WpWKhnG10LQFk+vjdC6kAXAvcZg4v4Wg48kKB/o1R8mejCUM2law4I2UVbMzF+V2bha9EdjN8Zomui2wfohE6MbM9Br3VcxGE8q1m8dQ1vQHg/5m7OXA+B3LbKZNpWyQBx3/4X0AACcPDHO0I4yKzekiSoGVsMke17SK07NJLERsvASi71/XKL9dlgPigBM/32HtasTtMdC2d+VHgMnJeneGLeoz8RkzyP1k2Ar/1+6ZoBEKURbmoXbHca+FvZDk9UXf8dTb9ueYRcD/bLNRXPp2CUSuUP5YJr2sKI9LG3pQdJg9+gai7ckqO606A4ougNStOwUFd0cqLxP9oRFOtUhneqQPxOxc3Sd3ek1OjkTFUHkaCJHY0QapxGTy7UIUorQt2CiDRNtGjuSVHeapBY6+ANCSBUlNFFCM1Go0pg02FtcAw1vPvY2sl6XrNftNVN1MAKotj0KJyK6RU23qIltkxvHL1A5GNN6bY3Wa2rU9obU9ooD3HP/Ozh9xx/SnBDVKbca4VYjugMGO4c2iDxRMbp+bA5nookz0SS5pEEpvEqEnm4ztXONuGETN2zsqoydNd5ibLBKezJN5Erx0hluoYY7aC+me6iFGunQGjJpDZk0JkxWrjNlnwbkjlkUM02KmSa2EzKcbjAyXIVswNrVHrWrutSu6jKzd4VSNYVTBatmYu1sYLUirFZE4ckqld02sa1pT+corWX7EX9zzMVqgtExqOx2CZOwcchm45BNkFG4ZYP6pCkUFBOyOBqhxitHmJ2Yyw/M9au564ds1g/Z2BuXRIzygreL5ZC3I/hLwy4JnLs3PqUvf/V7+cn3/28ADKX50w+8htqUhdXWrL8oYuSBXnqmI466kzPpDiiSqzHdXE/ooiai2PR0WF/701/m0//9pQCkViLWrrbwczHTnw9Zv8LpNyrZTXGC+dPCHS9yd7Ktmwe3BNWDEYXHjX5XpRxPECt+TnDYSkNyTSLiTt6kcgByJ7cQJ5XL5HORFzP4qEFzQnH0Z+7mroWbuDV7CoCjrQk+8/EX45Y1r/u5+/nze19GmJLvaOxrIdpQLL5FOHc2sfcgxGraUNT2xiQXDBq7Qka+JmNWukKRPS2wxCCrSc9BuycOkljV2P9+legTwwRphVvZIvKKkpr0eai+uEPiSAL75hLmZyQH0ikoksu6h2iRfedP9/hj9piESY3ZUdgt2Z6ZlRC8XRT+/TAB2lTEryyT+AtpDjB9zdqPtEl/KYU2eyRlPVSOXdeESdVH4OTPhqy8VRoA0p9Pc/hj7/sn59c2zv35se0I/rm3Sx7nDoILH7el0/TpzgTJ2TqdgTzNcUXqvIXTlIh37WqLxIom8hSxuek45drsVkxiTfchihtBqg/5c9e7mG0LWxl0Cha58xGr14gTSy0KIVlrSMjGUouasOc0c2c1y68IcRZtmpPSHr/ZOBRbIrQRVAystsaphtSnpbmmU1QQa0pXaJyagTY0qblNNSKFioV7/a6Fm7h74kE+uCI3QTe2cCoareDJ2gRGIIIVAOW9Nm5F4z2WxM9rwoTG7PZw54dDWkPydcYO5I4LBh/AritQmuLRgLWfauEcy/Rhi3ZLU/vKCHoCMnOC/snMyueClOSz9YaLU9XUTg7gFXoom7aMd3PUJLkaYze3YIsDp6TWUNulSc+L428N90jhek9e2hQ45JXDS5wye+kjG1w3JEoo7IamPbjVoNYaVTgVoSwIk5D8Yo0dRVmBV73Ms5p72/bc2HaK5vm1S8a5O3X4bPkqAK5ILRLmPdLzPmHCobabPm9JakH3uyFNX9MeNCgelbxzdUYKdcnViNawyVsKD/Jw+zoA6jMJ6Vy9oOlmFU4dMrNybKsjnOZhUuGWZaHJnZMoNDYVibMOTk2i5ORKjNHDZVf2GUSuCDXbzZjyvi12x8gVSoFOodf1aWzBHYOkFEtHHo649Q2n+ODK1fz6iDzG/tr6fuyWYPlfUXya+bm9+OkeBa8N3bwitiA1B14Fqrt6UnqhJkgpvDVFZjamMWH0uzsTq5pEKaY+ZWF9NUfo6n5jlIq3OmAjB/yMSXK1d+2WgVfqcb40IHNe4dR0/3hOLaIxYWJ1Y1DSqQvQGdRkzwo0VMWikrXZoRp5shjbFTnfhBlgtXvsla5iKl9hpZkjtmRO+Nne9fnQHlIUTkSU9pu0pzPMzkktIPePMPjbdunYdqPT82eXRM5927Zt27Zt2y6uPSvnrpTKK6X+Uil1XCn1tFLqZqVUQSn1BaXUqd7Pfx6oZkgkvtDKs9DK89mVK7CqXSLXIHIVKoLc2Yjc2Qi3qvEqMaYvhU+3HNMecmgPOfK/BLSLJkFKcW/5RoKkIkgKq6TdhPrMVi5XRfIKUopuRvK53YJABNsFg3ZBBCi6xRhtgNmWSDxIKYKUwqnKfiJX0DzJtZjEeiivFZGsS65IlGy1ew1KO8xeSkJTn7I42pqgG1v82vp+fm19P78weIJuXpFYj7ivdIDIUVvHq2vyZ0K0Kftrjhp9qGdjzCZMQZDWdLMGkSvNSmFCZPbccoCKoL4nwq3HuDV5JdYDmhMauyHMlunFqA/LDBMKqyUUAWECantjIk+i79awgdmNMTvSMdscNRg4ETBwImDkkYj2kMAv5b1SBPYqEbEJyVVBQ5m+5on1cToDBp0BA6cec269iNKQWolR0VY9yCtpaXZLy1OPU/YZHy0zPlrGqV/8utFFm9vbtg2TfJ7sWRVUlVJ/DHxVa/37SikHSAK/AJS01v+vUupDwIDW+oPfbj+p4pTe/db3YQSbHAOi9LPnlWdZ/PhOYnOLX8VuaZqjBnZDs3FTSPGbFtU7JK/gPZ7kGz/7ET5e280f/pfX0h5W1A9IDmXH/5LPV++s434yT+b/WODMsXEAxr6qWHkRTH8+ZP4Om+KTmtqMrHt3vP5RnvzVq0i+Z4GzD03j7K/xI7tlkv79r76Y5LLP6bfYHDo4y8Kf7aT0Ijne/o922XfPce7/xA2CDok05cslfzC5f5WVSoadQxssfnYHTkX3mSO7ecXhX7qHl77zTlqDJm/++b/jSxvCSbPxOztwyyHNcYe1F8XoVIiTlOP5LRtn3sEfC5iYLFH/3ChWW/ZZvtlHmTHZbJvwgQL65irxYSliRp4UWItPtli6LYVd17z6nQ8AcKQ6zpDXoB66PPTUHkYeMFh/teRXfmDv0zy0sgPz3gJr10Nqpor1d5I7dyua9asUTlUxeCRk6cc6FD4j1evQFafv56Sb11vdKgrHNiSXNY3pLZWoTS6b8a+1MboR7V+p0/zUKGFCCNJAitpHfuPiEoddrLl9MQuq3wv56++Fa7iU7DlhhVRKZYEngF36GTtRSp0AbtdaLymlxoAvaa33f7t9pQem9MgvvJfEyuZNDe2pgNH7hdO7fG3I6P2CH195lQ81m/xR0eH0NjSlqyVHbNdMhh+OSS53mXtFkvzJmM6AOGkz0JRu9ck/5BAmFemFmPq0bGuNxTgVg9gRabcwCY2DwpM+fH9P8airWbpNM/V3Gt2rVISewfrViqHDkn9WYUx3QDauXWsw8+kWa1encGsxkaP6uexu3iCxFhJ5Bu2iiVZbXCmJ9QgUfPl3P8ahj9yFV9IUH5Pk+eyrc/h5zcRXItYPWSSXNamlsDdmBrVpE7ciknb1aZHjAyg+oWgPKdLzMX5WnmQ2ET92U+oIK7doUnMibZhelJNJzTZpTSSx6xHeiSXm3zBDZ6hHhfBgxOyrYOzLBqk7F2h9dJzyvp4cYFXk9CLHYOkWg8n7Q9Z+sse5P58hMVmncyGDESgGjtF30qUrBUVU2S/NS34Wgh5FRJSOUcmQoS+42O2YhVfoPm0xmouKlrmYc/vfMlrmn7JtB3/x7Lly7lcjIsDHgKuAR4H3AAta6/wz3lfWWn/bx1d3ckrve8P7+KF3fBWApOHzxZ97Md2CRWNcdFILT/b0R4s9tEZLinqbbI8AQcIgSIFb07SGDe6889P86Yd/UM4jhpUbFNkzkF6OWLl+i21q4Lg4xCApRVX1jAKd2dV086pPA5xc3oqyV6+HzDkDq6NRoaRHNlME9WmhGI5dEeCIrS1BDqUhuRwTW4rX/vz9PFmb4BXFpwG4r3SAM7+/n05R8dT77uamD7yrLwTt1DVBUlHdJ70B2TNb5GCZuYDaDpv6TkjPQnMcnHqvGNkVlk2noVm5EUYeog93dCua2m6NU5bCr90AtyID0B0wMNua0jUxY1+GlRsV+RO9eRRDciNi46BFYk2TXI36AiCmD1ZbUzmgyR9XBGnVlzu02sJuafhyDVP/7hzrv79DvtuCQfrVy1S+NNonEXOq8jk/K+ObntNU98HkfSGr/6csGOrh3EWl/L2Yc3vbuX9r23bwF8eeKyikBVwL/F9a64eUUr8NfOhf+mGl1J3AnQBOcgAjhE/9xW0AmAFEt8KdP/o3/NUHvw8VWnSERhynKhqi7WHF+PfPUfrTKfI/I7CXU1+d4e2vu48bU6f59bf+OL8X/RCd19cB2PnhgMZEgdyPLND9gzHGb1zk/LzsVJ90WHlxxIGPNjj11iyDh6G+Q5zfL//En/GRD7+ZH/nQF7j76y8jd2uJ3z5wLwAfeue7cVeanHlzntFrlol/b1g0T4G9vxcw9V/P8OBfXym0vR1Nfbc4/uuuP8XTayNcPzbHn9/7MowA5uf2AqJ3+uYP/B1//pHv56YPvIsH/7+P9mGSX/wfN1N4uk3kJqi9pM3gbavMlsW3RGaE9b8GiKY6DN+4yvqfT/epkNev1aRnqviA+/AA7beU0N8sAEL6lZ5VjN1X4sIPF3Dqmt/8T6L5/Pf1K8hZLU62Rvl84QCFv0tQ/n5Jgb3l8ocJYpOv/+KNdH+mRDsycT4tXaiRq2iNKnInILUUMfvDMfnD4t2ttiaxJouvn4O5v9pJ2pfFxOxqOn85godGGwKh3Fy8Zv66jGp1KfxxidN3H6A+ZeF9Ptu7hn/prPsX20Wb2x4X/+S+F2wbJvnc27Nx7vPAvNb6od7ff4ncACtKqbFnPLqufqsPa60/hkRHZDMTOnshpNY7HT8Hflbz9fJutCURa9ST4Bv9wiJxNknl8hxnVgaZXAk5c/9OABIl+GZ5Rg6gJKfb3RCJutZMgtzZkPNnRhixQGuFd05CaSOIUV6ECiIMX1IWE1+WqPDomyZJLgcca4wzdp/J0u1FHp+RSNNdbKA9C2dfjfnlAdLTZl/Muj6T4HhlGKst9QIz2GJ3dIyI1oUsD4YzxClNckn14Y5BSvGljX0UH6uxfl32H8AkX3n8amo7PZQG+0SSs1aRblUS1l6+g5tWsOZywh8jm1QMPN1LhQwlCSZNUp6Pt6BZK6Xweg8uQUYi+/jIcRK33kxqOeBodxKAxW6O2XaBuNdJFNugZmU8n54e5eGnd7F/tcPiY8PsufkCrVnJ469fadMtxgycEElBb95i9CslALqjKbxHz5LbM8nGFcIj76dk/4VjHaq7PfInW7RHhJagske+o/iJpzHzOR48t4uZBZ+lm7z+XNrEwl9Eu3hzWxWe/y7BS9S2YZLPrX3HaBmt9TIwp5TazDm+HHmM/d/A23v/ezvw18/qDLdt277Ltj23t+17wZ4tWuZq4PcBBzgLvANZMD4JTAOzwBu01qVvt5/U0JTe86b39R+v0wuxEGlpaA8ZhElpPweoz4DVVIRJTfGIZuOQsAYCtIc0o9+M0Qo6eQO7pekUZP0KU+CtSbu8n5NGn3axV2z1RSR69XqD0QcjOgMmtZ2yz9zpmNVbNKNfUazeAKl5A29DzkXFmtoug4ETMe2iSMltinwnNmJCT9HNK0xfmpJSyz3myUbM2jUWySVNejGkvNfG9HtRfV26RMv7TPKnY/yMYuC4pEK+8Mk/4ub3v4uclQZ5AAAgAElEQVTWsEFs9ygHes9eZlcoDjYbuzLzEc0Ro79PEII2I5BUzCbJV5hQ2K2Y+TtM0rMG7SEZV4Dkik9lj0tqOaI5YpJaiSgdkANmL0ijWOSK4MbgkYClm3vF5xaMfrNDp2hjBJryPqvfEZuZjantFOGS1IJQSaQWBfFT3ekITDSG7Hmf6m6nj5Kq7gZvQ+FUhYbAbmoys1L0XrrZ5fh/uuhomYsyt7dz7v8y207RfGf2nNEPaK0fB77VTfOvms0qBKU1mXlxKqEriJXisQ6R4xG5IqsHIsGmQhg+rLGbEQPHLApPStWtti+D2YnxVttUd2eJbcgsiBOrzli49ZjMZ55g7j3X4jQikiuy0/J+l9gCs6WIbUV9h2L6byVX3x5PkD5jkZ5v0c0nGDpch1ic9OoNWawW+ClFYj2mdJlJ5oJcwx/8+kf42bf+NAsvSeCbCm1pRh4WGGFz3GXi/hYohZ8TSoFuvicZV4kwOxF+3iBIKgpPt6ntlBTEze9/F9/4jY9y4wffTWa+i7PapDUjeefYVlR2W4x+o0Vy1WLjcrdfxBz5RoXWVAavrLGbId28zfoh+eqNAIpHYzJnDQonuqxd5ZI5J239sWMycLwtBeB5n/qeDAOnZDw38/mRq2gPCerHW5fj+VloDzkYgaY6YzFxX5Uzb5Tz1AsKbUHupCyuKlbElpxoa1yRWNGkViKaYw6xrehIaQCvBI3pmOnPRwRpg9oOE7Mri0nu7MVvUb1Yc/t70Z4LR7ydg7/4dkkQhyXGpvSB172X1mivzV7D0OMh7YKJ6Ws2rlRkzst7B59o0h71qE1bpBciujkDtyo3d3m/SWpBGpmcumb9Ghjs9U20RgycqkTQufMBnbxJc1wi28SaNBzlz4a0Bk3REu3pcrZGDMyO/J5ajehmTALpekcbis4gpJY0RgClQ5qhR2RbmFCECcXo1yqEORetFOd+WJyYtjSDjxh4lYilN/p4jyX7ka02ITMrLIhLt5ikL2wpVIUJgX4+9Ov3cOVv3EVqKe6353eKiux50Y1VkcbPKSbukwVq6cUZBo/4bFzuUHg6wCl1WH2R8LG4lZirfvYJDt9zNU5Tegg2+Xi6RZj4cof5O1yspuKK1x5n9ZelvlG6zCW9FNHNShSuDajvkHOZue0CF76yAxUIuqgzqCickEUhSBg4jRg/LVBWs7tFtha5iiAhwuhmoIls1R8XIxSaCLsVy3deifv6uJ28waN/sE0c9r1g2w7+X2fPCRTyYlpidEpPvue9+EW5yb0lC28D6re00Cse2dMGrdHNVIgisQrNKU2Yihn5uqL0w1I4TH8xRflQjLY1mZMW2oQgI5/LnBfulPpOyJyD2h7Nkz/63wB48a++R/jDfUnZpJZjll8q5+KsWxhdRWdnl8Rpl854hLZkMUmftnGqPVz5Tpi8z+f8D0k0OXF/zMbbmsRP5ohtQX8Ehd4+N0xiE/R0G/epJGFak+rJCFptWL8hJrFoEnmacE8b+4Tkq6wWjDzSYfUajyfffze7P/kuMmdkgbKbknLa/7bj1AKP0j07+jquK7dqbrnmBEc/fpDqZRFWwyCxvLWQtoc0VksReTD6UMDsqzbFZUHlfOK6jbNhkp6H+h1NAIKWA75BYl4kBrWxNY9yp6UZq7EzZuCIYuOGEG9JxsVbh/aw7mu1mu2teZlY643ljHTaWk1phNq09GLM+ms6GGcSDD6lqU/1yMhieOo3L25a5mLZ8+XcX8iFym0H/y+3b+fcLwluGSOA/HEw6yZmXSLnThHQivR5AyPQJFcUyRWF3RQ4nVNRaCfGDDRBxyLoWNgtjfZiZnavkFqOe1hqhREo8ifbBGmFnuzgVTR23eDKe3+WK+/9WYwAmhMweLhGtyCYdKtsYZUtBq5Zw6lBvtBEW+AOt5jYscHEjg0yszGFEx26eYVfiIi8reF0SwGtlRSGD05N4a0rlC+vIBsT5iPGi1XJgSc0XkVe3QGFToUklzXZM7BvbBV9eR19eR1vXeOsNkktxez+5Ls488aPUt8dU98dU90LaPjG0T0cPTeOEWmSywHJ5QB3zeSh8zNUL4vQyYhoskOQhSDbgyauKgZORcS2JkwaFHeWKe4sYw22GRuqkh2v4w+HODWNcTKFcTKFk/JRyZDMrCZ2Nf5IyPDhmOHDMVZHE2QhuWCQWokgUiQXNclFjRFo8ielVpCeA39AoK2mL5F5p6BIrCgGjopjz8zHZOZjiscCEusS/Xsbivqk1AfaQ7pP5LZtW7bZ8v9CtG2qgotjlwQrpDYgUQpxSxLdSTONpjNqi0qPQZ/O1q1oQleJytKISTercM9JTrqyT6PsmMVSjrypaI9sFRyNMBbu8bJDbYdBbGvSFzZpaCVybU2ncEuKzGwHpy7nsjCaY9cTHc5ekyRThXakWK9JXqYYa6yNNrGdAFNTn7CwJLDFbPlg2CRXdD+C3hSzbk2IgtLc2SESHphd1Wd31CY4yYDUkkFjwmK2PNCHO6YtaM1k8bOKzBnVd/AAB++5CxVpnDWLyNNU9hjYja1GLeNsgigXk33KobEjJra3CtS5k5rkUpfUfJKNy03CWq+yPZdgOTKJI0Vi3iZMaIJ0j+64ZWOtOqhIi9i2ArPT42wviPKUW9H4GYPkvIFbk21+RuHUYwpHRIxEm5rCcXHa7aJFmIbC8VBSNFmTyu6eQPajEYljS4TtMeyGxgilMCwn853Nu227dG0bJvns7ZKI3Ldt27Zt27bt4tolEbmbXc3GZTbZCxKCGYGmssdE+QaoiMouG68k2zKzXcxOSPmyNO66gd0UxkagX3xLej6JkovziMHibRLdVfalyJ2N6RYV3rqmM6RxK/J+PyskVK1hk/qhLmHSI91D7rgpnzDhoYyQ7IWI9miKHTdIglydh9plOXk6MDQq2jqHxs40KoLMfIBd81HdiNM/JkVMe0cTzqTxqgq3rMkeDjFCOV5jzGZtl02YMsjMBURmhJfv9MYpLQiSoqggmW3FwXvuAuDYu+/mml+9C6sJkStiH97GFo2A01T4QzEDJ2Oakya503KeWkl+XCtJi3jrit2vmAdgPp9n9UwRFSjau7qYHRe3l95rp6TwmT9epzGVwx+GjcvlaUdpSbOoSCL1MKlJrG7CHV2KT3UwfJflm2xmPtNl4zJ5MsldCEisQewqWkMGyeUtVSh3o0N37whGzaJwtMXiS1MMnJTrC91vmXLcthe4bUbs2zn478wuiYJqanBK733jezHk6RxtiDBD/nTM6vVKujw7cgOnFjWNCUXx6YjyXhOnJnlqEB6S5oR0fGoLnIqm0UNwxJZm5OGI1WstEqtyjE2HWr46RHUNCk8ZdAqKKAGhJ9vSswoMwdnHjsjLOT0kTXtYkbmgiS35PXthi6gscqE5GVM4omiNKCEG6w117mxEa9igPazw1qQYGqS2SNP8rMYtixqREQg+HURGsDOoBCWTUaDFgcrxFI/94t0c+q27BJ6ZERQPQHNMxsO/rIV1KknkaaxeITNyNIkVRWIjxitHVHbb/X2GKXH+sQWF40IFXN3b+84WILkWU580ccua+rTqQxIjR7F2c0TumEX1Sp/Br9k4Ddm2cpPk0zfVsrp5RWco7u3TwC3JNVpN8CoxzbEeVr+iwYDaLlGWCjK6v6ibXTj5i9tome9l23bw39oueZk9FUN7VNEdFK9itgwGjmrKlxloO8ZdNPoRceQIudXyjQZKa0Ye8Tn75h5B1gmHYDgg2N/G/FqO+ozqKwB564pu1sS4vIaxkKG6T3Pli84AkPid3VR2C6Sysl/jrRl0Jnr4+JSBt2xSu6lD5qEEzb0+FKSpKPGFLEFGMPntiYjRh0Lq09IuXzgeYr60Rmu9AFocWXuip69aNIhdjTXepLYo+W1vrUc/kNYEwwHZczbVvRBNdWBN9jn8TRj9Rou1a1Nc9uan+cbRPThrMjBWEw791l089XN3sxQ2+OH/8H/3x7d7RZtspkV4pEDsaGJb485vzgdFe1iTWobGmEX2Qsjym2XQgraNnQhwnIjF4QyFJ+nn3Bsva9E4mSJ/UoqgnZGIKCneNncK8kcs2sOaHX+lmH2DT/5BgYGmLyhaI9AtaHkKUOCt9xA/vQW0W9R0ByDyDHyBxzNwIhJ4aQYSyybJJSgflG2bbKLb9r1r2zn4f71dGjl3Dd1ChLds4i2bfdpdfbCOt2KgQnDLGrfcg9BpibyJoVO0GZ8sMT5ZwgihMFLjlTMnMH1hEPTz8kotR1idmImBquCyR7t0IptOZGN2Y6KEpjZtkJozSM/HYGgwNInxBk4dpobL1PZGpIstZgolZgqlfuHX7IA91CZ2ehhxDWY7Zm9xjdihH7VvomWiUR+GugzlGiQXDLw1g8xsTGY2JjWvmJgsoWJhd9w7vkqcC4lzIe1BgyhhoSJNLfDAjok8TeRpuoUYqwVLYYMxK01qJUDFWl5KE0QmsSui1XEmIrUSk1qJ8UoxTl3hVsIeHbDBUL7BUL5BZqBFIdsin2yjnRgVQ5yKiFMRl48tEUz5aBMpXKcjwqQWYeyupIXMtsLoSlSuLYW2FJ1iT2rP1rR2hLhlETMxuxBkRJBEG+BUFUoL5W+QibEboYxjJuh3sQ4+rhl8XBOmnse5u23bdonaJZWWaU7I30E+xugYjH1dGla6AyKODJBaDekMmHjliNaQRfZ8lyAt0WvpMvlZPBZQ3iv5300hiOSSpnRIkz8hzUVWW9SHAFZu1jhlE2+j9xQxBLkzmyLRPVWgWGB8di1k/UohzzI7AuNrjUqqpJs3UOHWeCZKgsVOrG7tC+QpZeavyrQn01z4QUXuuEUouyRywalJl2dsazKzQkUMkJ2NqE+ZhEnInosxIqlNgKRPANLzmtRKwH1/9Ptc9yvvBgS/HyaFSM1qSnPSdd93DIC83eb+T12HU4NuHrLnZcwBogR4G5rYBLeuKe8z+iyNqUUteqoKOgMGYUoalgBqM4rOeEj+KYvqgYjJv98ak+aopNLcasTGFTbpOeG6B2lcsjqa+qSJn5G8/Wafgl2TBq4gLemq9ZtDlCMLx457Db78Nx/4J+fXdlrme8u2UzRbduk3MY1N6T0/+r5+V6gRbmGX7WZMZa9F3EP1NWciijNlgs8P0hnUoFVfCLpb0KQvCJ97+YBBYkX388eJkgg85I5awvXibIk9R54U/mJHov3OoKI7INsGjkvXZmJds/rikOEHLOymOJX1K03spuSD/Zzqcb3LtvqkyO51c5Iaim1IL8nJqFDTGraIXMm3R456RheqLDDVfZrEiiF497OSBtq4IkGQVox/pU5tT4rkckDpoKRsvI24L0moYnGCj/7yPQBc85/v6nPeb57XJq+OijX5Uz7LN7tEnkT2g0/1eGdcA6sTYzcigrSJ3YhYuV6Ol1rSdAYURgj1Gc3wo7r/HQUphelLPt3sCKNnfVdPAGTOlJTMuvDmKw2pJRmzzlvKRPcXhZO/ArU9MYbfS7kFECY1qTkp5KYXtvLxcPGVmC6WbTv358a2UzRil3wT07Zt27Zt27ZdXLskCqqxJYILm5ztsakYOBWzfkjhrVs4Fd1XMXLXTFoLg4RDuk9FUL1BqAPTT7pUbpMoN304QfQMiJwuw+TnFcv/vsXEn9qsXmtT2yPhcva0CE0nlzXVPQq3BN0JeXRYS1rkTmpWbw9InHVYfamP6kiI6i2JeEhrTBEfbJD421Q/xTD2QJXFX4pxPpsndqRGMPd9vSjUN4jdGGe4RXwmjV1X/dRNZ1DR2d1l6D6HzoAIbbSHpOiaXI4Z+UaFhZcP0Ngd4fYKrSBwRxVK8VQpTf4LCa75zwKTfOw/3M2+P3o3UVKjrYjGjGLyPrm+IGUy/woHb1VSH+0RzfzLt5qD3LKFnzdwNwzAoj0pn2vu19grgqxJLilKl8l3AeCVY2o7DFozAcVvWtRua+OeSPT3qQJoXdXGPZkQiUFfYozuQ0WUB35OoyLVTzWB5OCTC4r6rhgVCVpqU+dWb/Vqbdu/EduGSf7zdklE7m5Vk1oSqToVihJT8p0L6L1NOsOa3Lku3QFFd0BSHAMnI5JLiu54QGo5wnDklVyJUabGdkPshia1FNMe1bRHNX7aoDVkoC4kWLvSpj0dcP11p7j+ulP4Oehc2SYz66MNTXItxlqzsdZs4iEfIwRlxiTWeqma6TID02WKxyJSy5HwxqwkMLua5Tsilu+IqO7LEEUGpSs0rVFFZ1Ch3RjtxkTZCBUqwtAkexoSq5pEKSZRihl/oIMyY9pDkltOz1QJb64R3iy5p9aUkIDdcs0Jwn0tYksTW1LU1RZkMy28hC8KS4sR6cWIfX/0bk7+xD2gwV01yeypcOH1mguv16y8rktyUTFwMhSRkq+E5HdUyO+oYAx1GLp5CYpdIk8z9rUu7kAHd6CDl+kSjnfJnYbWeEywq402e45WQ5DVjH7ZlL6CJxM4NakleCWhkkgcTWDXhO2xOaVpTmkGj4YUTkSk5yRNlT1jkL6gSF9QFI8EgoPfXSN/HGrvrKEN+q9t+7dp21QF/7Q9q9tCKfVepdRRpdQRpdSfK6U8pdROpdRDSqlTSqlP9JTj/1kzu5r82Yj82Qi7rjlzdJyg5ZA/AYu3en3+keHHfMKE6rez1ydN1GwCNZugfFCRSXVwnRCrJcXMYDAgGAywWxqrBfGONtnzgoY5/fF9nP74PswuRB0TM4iJPKjtMPBKCq+kuHJmgdRSiG5a+FmFWbZI2CEJOyR0FUao6Q6H6GREbaeBXbKwSxbZcy06LYexr0HhWETxaET6lE36lI2RCNGGJmrYdAcU6YWQ1qBBa9Cgssslm22Tno9xGhoNpDyflOcTpBUq1mxc7nD04weJNlyCXEyQi2nsCfAva9E4UqB9NkuQ2RrbKKnZ/Yl3ceZNH2XwSESkFfnHHPKPObhHE2gDkvNC87tyg03W65L1uuQyLYLYIJ9v4g+G1HY4RGfTRGfTaA3miovVEf3VmbENrLbGamvWroMgH6FicEtS9yge8yke8wk9RfZCiFOVgrZWkDspr27GpDUo7J2pRS1NWArRUq34JObqNOcymL6mfrxAckWTXNF435ZR/Tuzizm3t+25tW0H/63t2QhkTwAPAAe11m2l1CeBvwFeDXxKa32vUuqjwBNa63u+3b4y+0b1zA9+kPAlwsveuZBh5jMByze6BFlN4YjG6vGHL74mJPO4S7egsWu9gt4eKdbt+bMOyzeniG0pkm7yiwMMPtVm/fIEzWnZX7toUDsghcPsSYv8qZB20aQ9IkXJ+h7Zlj9q0ZjWoCHyNNqJyR+VbFbyNctkP+RRPZCldFCx8y/LLN8mBOTJtZjF22UB8pbl/ZvC20OPh6wfsugeamGdTmBeXsP6qkjU1fdEpC6YuBVN+TKNu2H0ZeS0AfkzHcKExYUfNNDJiOxT4l8GTgYs32gTO1IUdZ/h8Gr7I9xVk8EjEV/9nd/l8t+5q8+Pb3alqNnNy/UVn9J0c70u1GHpLrVaMiapHuIIwFuTNNDEqy+w9olpYksxeERSYvaRC5RetQ//jeX/n733DpPsrO78P++Nlau6qnOa6ZnRBOUEkkAICcRikgkLLMYJHABp7cfGxou9ttfGrG1+Nni9DhLsz8YgjGFZGxsbEEgCJYRGYRRGM5qkCd3TM527crrp/f1xblfL+yOKQQyiz/PcR5qqvnXfe+ut8573nO/5flFfKFI+d71pKn1S0RrVeKMepfscKi9p4+yXtFNmVrNykcapGYSONC1lj8s4O0Xp5m1c2+Tnz7ufO9/2fE68N07LHM5w5L+euSamMzm3Nwqqz579KKZovp9NTBaQVEr5QAqYA14CvDV+/+PA7wPf8gegl20iBxJflo4VM6M4/lob7QYkT1rY7YhOQX7Ihd0OflZEO7yCpu+gpj0s7y08Lw0vKuN1bHK3p4kcel2hjTGXvsNdGpsdjABqOwPcRbn9yIRT1xnkjkCnpMkdg/R0/GhetsrUB1yOvz5F7qhBbZvGfsUSAK1/G8aYCinvMOiOeSy8sI/qubLQZG8X52SvmtKKH9GDO868EoyuKJNmToLzZJbAjfP/X4XyT1Xx784z9ICm/dZVllYFQ1nY49It2CTn2liNDGHBo7FJVozmuIkKpUHJ7w9A2fQdlPcamxWJy1apXqo4769uZP8v3cSWz75TBpPzGf2cTXoOli6RDl31yhX5HgKL/lSbk6eLpA+55I93aL5GVoXuiE1Qc1j+1CTVnRo92CFIyQ0mtuygshOytxYlei8bZGIRk24fuKsKrRxq2yBzf4rmeNxJO6oY2KOpT4LVULSHI+qbZd72PyZoIMPQ/P0tL6PxCwHOIUm2p09/q9n1jO2MzO0Ne/ZsQ/Dj39szdu5a61NKqQ8icmNt4DZgD1DRWsdEAswCY9/2sxRkZkV+be3f1lALd08GS4LBngyd3ZJml/zRkJXzTdxqSOaE/MhVpGnsK5A+rUBJHrpbXGvrVyxd4lDaC50+RWLO6kEo/TSoUJFaijA9gyCpSMfRcv3BIq3RkMyMIrUY4ZYNquUBAJIIbNOpQGLFoVuAxLyMpXwOlPZoalul6IpG2BMBFVn0HQxpDaRo90tBeU1OThsQPZInykB1i4F+sNgTs86dkIjfmciSnAe/leyxO+afgtoWhTurSC/A1K8+yf4laeEc/6rPdCFD4VEHnYAtn30nx97wEQDuahu85953QgRORUk6a0+/jNOHJatAAigcCaluTsAeceDJDnHqR2N0Fc6+ZE9+sDGhiNyI0DXo9EPfwagHk0wuyT1mZjWNcQO3KsVTgMgBtxrR7hh0C2DXjd4OozWoyMyFhMcypOqazHCDZty+ah4/sx2qZ3Jub9izaxudrOv2jHPuSqk+4LXAFDAKpIFXfIM//YZ5H6XUO5RSDyulHg46zWc6jA3bsDNuZ3Ju+3S/fwPdsA37Fva9FFSvB45rrZe01j7wWeAFQEEptbYjGAe+4aZZa/2/tNaXa60vd+wUfkrggMlFjVODbLoTN678+/NMT7i8jUDjloWkKrmqSa5qgqTIspmeJkworK40Qxl+zJMeq/p4edUr1KGIW/iF9Ct0RcDC6kRYHRHqdqpSADQCaeeX68t1gqR0Z6pQE6QlLRLZGrsuxUI/H9ItCNJH6Zg6QUmaRluClHErGrsph1sOCBMauyl86FYn/pYMkdkzfJHGU1qgm2sIFa2EBAwgtKXzdI1+wE+bmIkQLxfvEHI+d7UN7mobXJuMsNoapaXRyvR0j0JBRUK4trarcJoRfk7j50T1KbECZkeeq58RuGroyvOO3AirJfzvpqexW3KAjHdNAN1pRPhZ2QUYAaIfmxNlJ21A5MrRY/505Pn3Z5rr3+2ZR8ucsblt436jP9mw76OtCZX8qBdZv5ec+wxwpVIqhWxdXwo8DNwJvBH4NPCzwOe+3QcFSUVkQ32L/NvwNLnPljATmshRzL5Mkzso+/rsyTZ2w2R1h4PZESfcHIoLnEsRhm/g5YVBcfVcRekJyTu3Bg1Sc5rmmKLvcEhrwKAxKddLziswDIr7W8y+JI0RQuisS7jVphw6JUVin8bvN3DLa+MUZI3VBG0q8kc0qcVYKzRjsPB8g+1/12blggwqguRb5gE4Od2PsVfRHtAUX7FI7Z6hXvpp8XKH/FPg1CKWLhNWSiN2rnYrorQ/Ysf79nHXly8muajIH45z2QVFckEk7MKkwZ2fvYyhI3Li7PUOmQeSPV6e0c/ZkopBFrLdf/JhLv7AjaRPCy2AXZd793Ow6UttTl+dYulig//nLZ/gQ+/9SQBqkyaJFeGbcargZ9a/T7MrUoJ+RlgvG6MmyWX5HiJbNFO7OQOrpenmjB4baHo+pLLNIXdcqB56iw3Q7jfoZg36HxNkVfPvR0kOxOmYM99lfcbm9ob94OxHPQf/PdEPKKXeB/wnIAAeBX4ByUN+GijGr/2U1vpb7k0TYxN67N3v7jEjGj6U9nVxqh6zL82KQ4jpa/1XV2gdLJA+JTnYylYpWAL0HfSxWwHOodMc/8WtuKsw8IikfBafJ0XJ9JxQ12ZOeSxdKlFVdiai7+4THL1hitRpCBOK+qXCjLjjgy0Ov63A9o9XOfQLOfr3GAzcMQPA0XdMws4Gwx9L4KcNOkWD7Kx4qtqERf64T7vfot0vnCxrbfal++dZfuEIiUpIu2TSGBP5QBAisuL+Fk+9NUFi3mTT51aJ9h2U633wSrLHDKy2YOf7joSk5uTRagX1zYk4sg+obHPoCnAHqwl9hwNSsw2mX12geCDsFaiVFkrhx37zJqb+7RfJPGUz8XeH5MSBIn5/BqvWoTWRpTZp9fZ6I7ctMPuaITr9muxx6H+swcmXCf4ydyKi+FiZA/85T2mPSXtQYV4hK2K0u4/8tfOcni2SmHEk+o+RULVtQu3cGfNBg53rkr1Tvrdbf+eD/MyRN1P/8ASVbQal/QGVrbKoawP2/8mZpR84U3N7Ay3zg7ez3cF/L+M767llUkMTevKdv9YTzwhdKTJGNth1etzsAKuXRLhLJskFTWdAmprWUCgjX++ycr6L1ZK0jF0XnhiAviMB81eYqBD6DmhaQwZ+TCerAhh+0GPhMgdtSbpj5QrpxEzMOqRPaoKUEkjhsoldXx+niiB9StMakt3HGrHWplvrVM9JU58QLhSUMFtCLI5hiVNKrMguI4wR01ZH0xxdLyxHNr3mqdBVlPZ36ZRsVs4TqcB0TN1rdTTphYDGiODxg6Tg2wESi6oXWYdJec2pxEVMW0jAll4QcPw1/y+X/7cbCBOqd/3WmFyjvjli8CGYe7EsUKmTFl5BaHsjR3jlE8tx0bsp3DaNCUS+cCLoFU0zx0wamyJ0KiR5wgYlFM4gSKUwAVEqQnmKKB/0oJ6Ny9q4B5J0SxHuqoFTpUcH7Fbh8b/Y4JbZsG9uZ7uDf6Z21nPLRJHCVMAAACAASURBVJY4ZsOX7bhT0wzu6dJ3KGTowSZWU2P60rmqE9Icow0RXd766qO9Zpa5q10hqkooDE9y8H1HAvqOBIS2ovSEwByNEKyWZuDRgIFHA8bubdPNmyQXNYOP+CSXI7IHHLIHHBJLwqgY2WB0FJGpSSzL4eU0flb4zJubQvyspu+wqB0tXpbBTwsOPzWvSS5oige6FA90RYxkNMBdlfy51RaYX+gqyb3XpeHKbkB2RpM/7ssuYECzdJFLc9hg+AGfwUciOgPQGZDUVmWrTWopJD0XkTsRYXYUZkcoBYb2iBZt6QlN+pQsGMklTe5ERGopIPOUzeX/7QYe/oObMdsasy2duvlD4uTHvyp86sP3GgzfKxTFhgfdPhErGburidPQOA1NeaeiuL+N2VGMf7VBYZ+FdiK0E9F3JEC7EWbVZODxAMOHoQeaDD0gW5ewEDB2BxT3KlTL7D1r+2gy7nmQekl6PmTi9joTt9cJry//gGfwhp3t9qOYgz8rIvdsYVyf8+Zf6ykOaUNa2l957R4e/aNLaQ4ZvfdG7mvSmEjS6VM4P76E+kQ/K69tARDOp3jdix7kTX0P8f6XvoHTrxij9gJJZo9/2qY1YGK9eRHzb/tx3zXHib2jAAw+BCuvbTH153D4bQmKj5hSbAX++Df+hj99+0/xqg/fyd984pUYV5bZffnHAfjxn3oXKDj2dhgdrND812Gqu+KGqk91SP3xPEf/bSv547Igzb5Sot43XLqHLx47j1K2ycrXh8nO6B7TpJ8y+A/v/hp3/8EL8JOKD73/JvZ3xwH42O//ONnjTRauyFLbHlKaKlOJxawvmpjl6Ke303yRcLG3/nWY7GlJEc2+VFHYVCGX6FL/xxF4zQqtGO6Ilt3R+C2HmHvLDsy25uH3C3T7vy5cCEArcrj9xA4St+VYvUTu78arv8IdC7vofmiE6f+omRhfofaFEbmHDHSLEZkZg/zxgJnXaLb877j2MWSTnvOoTrk0R2PIafys1wrlXlYJrYCp6MSppambj0A+w/yfOSQ/UaC8w+xJIYYOPPrhDSWmM2HPdRjhc+3+zvrIfcM2bMM2bMPOrJ0drJC26uV5QXLZqdMGe1fH8DIGdpOn5YwtrE5EkLKoHu5nEPCrUhjNnDI43izxZesC6hcN4RXAOiFqHdoKSZZDFusp+jUsNdI9NkFtQtCx6QyYKE/RmITEiozn85WLaYy73Dp/vtQBApM/W5WItrLVZXD3KtZskWbeQWmNTsSizRkbQ0VYHdmJqIgeKvpUp4DXtVjWaaKU6KdGlqyzQVKxrzpKeqbJwpU57qifz+muUBOkFjwix6RbAhRUGwk4KQWH2UKBIC3SePWEC0nhY5cHDLVGEqW0FDcDCxXz5atIUDEMFNEKnKbuRex/NLSXNx69nlbgEEWG8K/HjVh7qps4fHSE/iELexE6w1bv/rQJYTHAKzt4WWNd6BWILEWQMOWzQvmug1g/Nj0XEZkKFUBzUpE9oaEUz4v+AlEmAXg0xkxSc5p2jJaxWt/lhNuwb2rPlYj2m9mPEpvkWRG5G77Gamm8nBTJlIbOgOaKgRN0SkrElWNcdH1cHEZkQ2ZzldawgUqGqGSI4UHCDLBVCFpQN96YhzfmoQ2RkHMdkasbzdWg4EHBw61GpPe7qEBjNQzyR9bH5hoBSkNfooVXiMilOr331rhi/GJIrZ7E8KE4XKU4XMWudAm0SW1LRHPEpDFmYq9Y2CsWj86Ok8u22VQqkzkh97yGV3crmoFEg9ZYCrOtyVstgsgkiEwq22QRG7u7g8p7DBYa6PEOerzD4tESWoGd9Ek5PomVday+WzYYLVVxLJHC60u1paBriQDGxO0t/P4MrTGNn1xfZN949Hr+cesdbMmsoDUMfWUOq9TBKnXI222ygw3BotuaRtvFrQhmP3Vak9vrYISQmelgNExagzatQRuro+mUTNoDCqsD7SHVY3ZUkXDIRI7g5zv9qscm6ZfSmKdXuGTwFKErXcpruPr24LMwSTfsOWU/Cjn4s8K5R5YQU3U2eXJc2CJ3FO6e20ZmVjDpa00+TiPCTyuSS5raalqYBX0D7RsEKQi0ga8lMmyc24W2CW0Td9XHTyvCyCA7E7DaTkHdhrqNU/Hp9mnSBxcxO+JsVCjHqpcmseyzJbWMUzVYWslS9lOU/RRBStEZy2JVTBw3wGloqoeLVA8X8fpchhJ1QXbUNFZT9/RO08ku1UqKmudSvbpDeziiuo34UNQDF7sesnpJxOHWMBGKCEV6PkRpmL3OJarb1DsuUaiIQoXypYHLcUIsQ9r97UaI3QjxChGnFgt4gYnVUpw8Xew9e7upOH11CqvWIT2r8DOKVuTIETj80qkr+KuxB4gixfz1I+iZFHomRTu0adYTUrz2FZ2W06Nl7pZE0q9blCYqe6wpkXoEzWEDs6t7O5nuVJfC0ZDC0ZBOQdEeXlemCp3178Fs++h8hvtmptbVm+IFf61HYMM27Lux57qDPzvSMpYcub0Ce9OmQ3IlZK6SId8n/COVrbIO5Y+FhK4iMhWpI8IMlj4i5yUXNQ/t28re4ijDgab/XofWkESi9UkTw4fmYhrrHIv6kRLuqnym2QkYejiiu7lE5GhqWyB3XHIM9x7fSnarw7+dOB93FdoNm9tmdsp5BhhehOlBp5JARWC25HpOucvXZzeTmpNmHCKIkhLqV2YKACyoPKknEzH9rTyLIKl54IltnHtolpG7J7ituLP3nApDJqlZD6uZQiuTmpkhOSvYy/aWLsX7LE4PZmk6aUbrGj8jeSd3xSBsJlhtWaRCSB9yKRxZT5UsXWzQmshS3xwx/tWQ20/skO8lMji2VGL70R0cvuYWLr7vxt44d09vxjmaJEiJUPVcwSKxGjcqmQovr2iPhaAU3XKC9GmBg3uZBKl5j9Bx6fYZZB91aUgdFruhSS4IYihIi2yhjmeo15cguVjFm+/DtCAz28HLJePrfddTbsM2DHhuNzqdFWiZ5MiE3vH6d1Ofkn9bLUXhiEToAJVrO4x/Un7lRixAXR+zUVr+rVX8dzsFwx7aIoLdnNAMPCIOp5szeo6icMSjPun0hKcxZHEpPBVQ3mHh1NbFs0UHVAjIigd8unmzp/CUWgxYOd/GakJyWYSec8cljGyOJahtNpj8l0UaOyVSnn29OFTdMtn6fwKWLk5gXLtK/XAf2RPymbVzIgYelo7M5nhE8Qm1Lko9HxImFOPvPsKRW3bg1ATuCdApiTMMnThCHlOM3C8Ode4FLiP3daltclAa8sc7QgKGUAq8548/yfv+x8+QXIkIkqqneqUiScXMXz9C6Coe+62buP4nf07GOelihLKTqmyxsBsxVQLgVkNqmyQHH7qQmw57otuRLeIskanwsgojlP8HcOrSnJVcEqqCdknhxv0NrUHZAaUXAwxf082bNIfFq1sdzSMf2UDLbNgztx9WB3/WNzFlihO6/w9/BWdZfqyGB0FWE451iDoWfXusXtGtW9RxykHh5zT9j2rmXyJOs/iwRfmFXajYuMsClWuOyHmmD1YDKucHWDUTIwB/Iu7ubFkYXQNtawgUxX2KlcsFRqg8g74nDCrnaqyGwvAUnUlpiS3db5MsR3RzBsuXaPofUSy9SCqVU5+B4683MLoGdk3GsNao5NQUVgtqF3Upfd2hU1SkT6+LdVeua2MeT5I9AdXr2qgZiVDdZcHtW82Q2V/wMQ6n8TNynrtqEDnC5RKlQ9x5u1dobG7xcfs6IrJha9zNddgjRVo/pxl6MKIxalI5N2D4XoP5uFFJBQqr1EHPpLAbitGvdbjjkx8F4NWHX8GBRzeROWHQGtOEgx6ZJ9bEsyPmr4nIHbKkYUlJgRygNR7irphEpsYbEnx9Y7M8a3fJou9gxMILNHbVwO+LSAwJ/t29KyfY+zfPs/rVEbQBnUEZ58DD8OAnfv2bzq8N575h34n9MMIkN6CQG7ZhG7ZhP2J2Vjh3rcCqG73iGYZE79fvOEjilC0Fs7h4NnqPT2pOiTTeoEjuKTdEucK++PytJ3j3S79EclGaYdpjAe2xgMSyJrUUMrR5ldxxsHbV0B0T3TEZeMAkSoWU9hiYXSlMpqZtUtM2P/6CPaQXQq543iHMtqK7pcu7rribd11xt4hwhJqVCzWD5yzT6VcoS6MsiaYvO/8YRkeROyaKQtbWBtbWBuMvnaG2PWDz+DKdosJqS3rJCDR+SvFj5xxgeHcIEbz1vIe4+OrDXHz1YXLTIWZXs7rLxW856B1Noj6fqM+nPeGTPgXWWItLdp0gfVp0adNzGmyNUmBuaZBYMvC6NmYnZnQsK2qTJiO3LZA6aaENaVC68eqv8PxLjnDd1sNcdc1+gvMa1CZdXn34Fbz68Cv4/PZbccabmF1h1Nw0toyfFm781pAwlHk5GLofMpuqFA8FFA8FpE+a5I5q7KYS5skQnLKJUzZ7hGxm28AIFEbLQCmNUpqRryxR2t+k6dlCTFaF5IJBcsGgPXhWTOMN+yG35xqb5FlTUE3PKlQoTtEIoLYF2qHN2F0dVne5JGPekhP/UZE5LH9TKDaobi/y+vPky/jisSs5Wu6n4bvkZgLChEH9nFjJ5+EyM68q0l7MM1zV1H0TFYhTSC6H5PfZpBYDQsfErWnKF0uq4JHlCVr9JiudNIkrVkgbmruXz5Fxhpr6hEViCaoTSXJLmuCopCZCN+CR45NkFxRrAPDkVwSsf+I6xcCmMieX+ijOa+xWhFOT1FJ6LuKBl21i+RUwcVuIH5k8dEDoMguDkrbKzIVUzzPwLQtrMS5CG8KK2Tic5rH2JEM1TSt2evaCTdeMMBdc7ACCmtPTWE2sQGIlYvY1Q3gFTTmluGNhFwCHj46QHWzQrCdwjiYxwogDj24CYNfST3PghZ/g8i/fwE+/+Gt88rZryC3KfZoebP10yNKFNslln7mVNJ23SEI+d3uapZd66EDhnnJwqprik/Ks515oETkGdhW0DWEmxLpH0kfTrxOHXqua9MXcNf17JQV26pqzYhpv2HPEnitF1rMi5z58blEPXf+7NMfi/PGKYujhDpVtLn5WMfBYh9aQOLGVC1TMmQLFgwGnX2hi18WBj97XoTkqf1feqcgeh2RZnGY3awqPeAzB0wY9CbfRr3Wxmj7zV2UoPdFl8XKXTr/kc0e+rmkXpYGnOaIwPWFYBHjvh27h9//o7aSWQuZeYNL3pOC0QcjAun3C61LcFyssxTn37IzPwvMcnKpQ7gZJ1WuoMjvCz2L4UN1qMPC4T2JRHOPpF2VJLUrhtjmqyM7o3oJYOFhn/oV53LJwvJueptMX57lHNPmnpPCYe8dJlj812Xv2Zkf+dvlihVNRdPs0Y/fI/TWHJJIPE4ogJeRr1U3iSM2u8Ko//Ac3c/1bfw6lNfPPl9pAal6TKId0b1wl/cE8069wSC7GRegFTW1K0S2FFPca1LZBQUgv8TOKTj+YbaGfcCoGZlykDVIauyEw1U++43/wm2/+BVYukMWyPah48o82iMM27MzaD4ODP+sLqqmBCX3hy34VLxsXHl1FfSpCjXTY+mcBMy/P0XdYnPTpl4c4czaZk1C9to0xkyS9S/bzleUMqmWikyFDdwlaY026b/zOFkd+2sHoGPTtUyLMfEB4WcwOtIciivsU7X5Jk6wtNMlzKyQ+W6D5OtHkay6nIE67TH0ali5y8LMi1DH0AL1o2fA01R2azLRBei4kshTdQoyIeVGbsOJg1UyyJyBR1lhdWUy0Upx+sWLLBacof3oc+w2LrDwqXTruqsIIhV2yNqWIXI2KudDthvC0dIqyoKRPKeqbZZypOSXOsmyQXNZUt4s0HojQhlMV3Hl7SFJS3XEpGNuLNpGtMXxF/2OaxphBe3it8Bvx0y/+Gvf9yhXc8Q8fZfvHbmBwj9xDu9/AqWuSiz6Vcxz8rEJfKeLn5r15art8jIZJ7qhwumdPSQR+4jUm7oqJU4bsbMji5UZPYGWN/bM1GTD5BVi8zKJ4QK63eKni6H/ZQMts2Jm3s93Bf08FVaXUR5VSi0qpfU97raiUul0pdST+b1/8ulJK/YVS6iml1F6l1KXfyQDNrqbTp3rKOlZTo3xF0LI4+bIcpgftkkG7ZDD1GRjaE/Z40zd9vk3zQB/NA33k9jokRppMTKzQGDcIEgpvVxtvV5vy9iSjXzVITNTpFhVR2e3h6yNb2uXNrqaxLcDLQfq0In1aMZhtYHqaZjlJ4tYcyjcYHV1ldHQVw4sYu6OKV4iIbMHitwegPQDpxRB7sildtRMm7QGDyhUelSs8osDAaphYWxt0+xShowgSBkHCoLbJJL25SuvDo6QWQ4LQZNtV02y7apr+fT7JxSju6NT4xQBvSI7mFp/6pKK+NcQf9gnSMLhHM7hH4+VBD3UZuW6WyFLowW7s1EVBqTkmfOwqhMkvN5kYX2FifIXChcs4mxsEY13mro2wG4KKCQc9Nu2a55O3XYPSmu0fu4HDb7uZIKEIErKrmn+ZT3vAJkgq/DQMfjjJ4IeT2HXN1n8IKT0uC4lbj5i70mHuSoeRe2D4fg/Dh06fQekJTbdPjpF7q0zcXgMj3ukYsHK+YuV8xfhdwbeZYT+4ub1hP9z2w5yD/7aRu1LqGqAB3KK1Pj9+7U+AVa31B5RSvwn0aa3fq5R6JfDLwCuBK4D/qbW+4tsNIl2a0O+561L+9a9f3HstdAXjvPS8iPxhs4f1zs6EnL4W3BWTgccCQsfoNTjZDaEusJqCl3bLuodJr28PyR4xhTvdhcSqphqf55bjiHeTT+ExG9PTVLfJ9fr3aryMInsyoLbZon9vm8VLJeL3M9AZithx0xIL1w7SHlQ90W3hSY/wsopERWhxWwNxVB+K2LXVCumULCJLSRES6DvkU9ts0SlK0dipa7Iz4rzmr7RJLEOnX5z74CMRZkei15XzbDKnIlbPVwQpTf+j6zWMbsFAm5IC6jvcYf7562LWa5j2TlHw5U5D0+6P13wtdAjdPkViNUIbqsfn4qelaczPKvLHQoKEYveffhiALf/8TpxlKZAavtAD9B2UcS5dqrBaCrsOL/2JB3n09y5l8XJJ9USmZuqzNZ76DRvLCrH2ZGnuFLhqbq+Ll4f+vSGtfgMvpxh4TN7zsxb3/dN7vun8+maR+7Mxtzci9+eGna0wye8pctda3wOs/l8vvxb4ePz/Hwde97TXb9FiuxHNyZFnNuwN27Dvr23M7Q17LtszhRkMaa3nALTWc0qpNeqmMeDk0/5uNn5t7lt9mNLw2Y9dixN3n9amBBkBMPmliJkfo7cMDe3ukDydJn1a4yeF530gRk1Mvw4G77EIXRjY6zP3Aovc0ViNaN5EGxJJqlCumT4VS/dlFd1SRP/9Fm49pD5mMvyg5PhXzjUZ3t2lOeqgQvByNp4AOEgtaLy84sR/GiI7rWlv7eI+HIfCNqTnPLTh4K76IoxtyPajvFPh1EyKsw3mryjQdySk0y9jiU7IjkWbknoIXcXyhXKe1RKSsc0vmmb145OEjqJdtHrPMHQU+SOiYVo5R5ESyVYSZeFhWboMBj87TWLLDhoT65KGZlek8QBWd6keV4s25ZphQigFEpUQI0YYtYYMTA/seYn0zY5E7ADHXv8RdvztDdhNCBLC7uhW5XkOP2CgFZi+5vZ/ej6ZPs3Qg/L9lXfYVHdmyd5nxIXliCDlxs86Am3grvpow6ZTMoli1su1TuYzZGd0bm/Yc8N+GNkkzzSG7Bv9yr5h3kcp9Q7gHQBOskBjU0TmhPxYrRa0hzReAZYKBrlDJu3BWMLtxAKRvYXIgdpWxdjdHpVtAkPpewTK5wp9QejYuCuwemFcqDRFnq89BOlTCi+33tY/9KqTXJou87XmBYQphVORHD+AV4joFm38FAw+VKO6I9vrCjWnYeKrPrUJWyT3anYvzVE4GrB0UYIgBa0hV4jMJmUsZgeSKwFz1xQIUoJqyR2T81Z3WIJYudenMWrRGpaFB2Dqcx7tAYdpexNOARo5hY6fuOnB0lUhhX0WXlbRGQ3IzggEp7bJwM9pwnTI6iu2U9kJkRvzwLgRzorJ+JfLLFxdZPLLbY6/TqgJwmKAXXcIXfDyCi9vUT03zm+rkK2fDvHTFl5oMP8yH3dGbn7H397AoZ+/mct/7wY2v/EoM5/eQmrPNAAHf3uKHR+t4xcStEcU+WOK6TfI88wchMITFZ76mT7suiLxZESQkXEW9ldJrGaY+Q8u/Xs1QUYze61M38kve99sPp5Je0ZzO0Hq+zmmDXuW7YcJJvkdoWWUUpuBzz8tL3kIuDaObEaAu7TWO5RSH4n//1P/9999q8/PbB/Wgzf8BpETO82Wwbb/vo/TP38BrWFN5uS6Bqg2oD0cUXpcHFu3b/03l52NqG2OhZ8DwJC8MICXUySXIqrbDEbuE63V6i5xVNs+6eHMrnL6VePCRZ6AbiFGyywoUksRq7sEgtkdWF+Eiq86ReuWUZTWVLcZOGXIT8tn+kmD1XMVdkPFsoDg5eUzi09qVi5QRLbGXTEIMprU6fg+lOSziwdD5q5W5A9Jzh5ARRoVym6iOxCROmX0uFdUuAYl1Jhtyf2vLV61XT7DdwtTZuetZbi12KtFWC3BjNe3BTirJmZnvW7gZaU+0C1qglxIfr+1TuSVA6cCyZcvkvqzPO0Bu1c3sJsypoffdzPX3PAO2r9YoXOPKD8ZnkAzg2KA0TAxugqrE+fxMxFW06A72WXT2Aoz80Uoxzh+JyJ9wiJIa8699ilaLylz4nefL/OlCwf++zODQn6/5/ZGzv25aWeLg/9WOfdnGrn/K/CzwAfi/37uaa//klLq00jRqfrtJj+AWrSIbE3hSXEOoas49NfnULpT09is6ZTWUwXp0xq7aQDilApHQ5rDcl59wqB/r4+73OHUS7IUjoTMvTCW7rMi6lsUucNQ3u6QPx5gdiSy/dDff5g3PfAO/CURwLYa62Id+ekAP2Uwdq+P2QqIXJPIisWlnxoiGoDUcsTU/15h5rUDeJn1VMH4XR7l7UKFi4bk0pqYdcQ5H1+lPZlndZdBZlYcN8jCkj8ecuoak8JBSM+F+OmYvdLXVDdbpOY1qQVFeiEQMQzAyyqqF3ps+meF0Y048VqLwn55r/SgRWNc4a5q1BeK4uRjlb3WCCSWoLTHpD0kmqedgUT8mQaZmY6MTSnmrrJojUskPXQ/JJd9eCJPeYdDkJQCMEgqZvMbj3LNDe/gnpv/Fz/22p+mskPOq7++Tv6LORJVg/krof9xzdKr4wanexOUDrQ5+kaH0w+PMPJoRHnHWtHbIHQFDrr6gc1Mf2QTVlwUzk5/uxn2XdkZndsb9ty0H4YI/jtBy3wKuBboBxaA3wP+BfgMMAnMAG/SWq8qpRTwV8CPAS3g7Vrrh7/dINzJCT319l8jjCN3t6IYvafOU2/OkFxQJFZ1T0O1U5JGJKcikamKoHGheJW++x0qV3ewnJDMV9P/Lhebno9wqyHTr4XxWw2WLzTpDMeR+z/4eHmb+oRJa0SaqOqXxJ6qYTF0n2L++gB8g77RKtWqbLWt6QR2Q5E5GdF+UxXzy4XeTmJ4d5djbzHIHrQxAslfd4Xpl9DV5M5bYXUpR+aAg+GvR7utQUVwQYPRW1zKO2xq53kkZiV6tZow9tUqs9fnaW7xIVSkZmV9DlKa3FFYebGkKMb/2aS6Rd5rXt4msTeJCqE5IeIda+gV09M0Rk28rGDIC/ssKhetyTSB0TCxx5p0ywlG7zCYu1ruL7OpSn0ljblqk1wSuOOaZU9owgSYr1qh9D6XL33uE5x7842ANDjVtkA40cE6kUCFqlf7aI0qQlcTutKIpdMBqin3kJw3sNpQO9/DqFts/7sKcy8Wts3Ihn0f/O4j92djbm9E7s9t+0E7+O8pctda/8Q3eev/N2O1rBT/+bsbnjhot0wvVWD40JxIUdwHqUWf+Stt+g6JM/rMr3+Il9/6btKzJo1N0B0JwDN65/XfnsDsamqbhQJ3LTWRP1TnxGtz9D0CdtMnNW+AktuvbjXJH+0QbbFIndYorTEcKQCWHnCobVb0PWyz7ScP89jXtjN5pzi/xUuFHje5HNC+uw8VafoOyXknX+ZQeFwi9jAhkMPJ24SmsTnqsuyXyNQU2ZmI1uB6J+bA3oDudIqln6+jHsxTeMRh+B4BdMy8psjRN+cYuT8gTNqkTmvcmjyX5KJPp2RR2O2gLQUE1LfIWNxDScwulJ706PbbZKZ1jwPd1DFd8fVV1GyOykV+T8waoDVooqI06dNdyjsSvfRR4c4snbd0cA866KuqjH44Sbskz9OthqT2THMyu43Kjohzb76RJ2+4CYDz/vJGfuf1n+Huyk4e2X0hjXFN4SlZkOov8fErCayKiIpkt1XxvzAg13v9KZYbadK7C2gDmlM5nLosCsvXdb/bKQc8O3P7u7EftKPYsO/eno6DP9u+uw3GpQ3bsA3bsOegnRX0A5nihN7x9z9D905JBEe2ROFOTbbwhqfIHZNxtgcV7aGI7DFpcQ9dqOyMc6/HDKy2xupoqlsMMqfWRZS9gsYrhlh1k+ITkr9eowpoXdWE6RRBSgp6dl31uGWGd0s+vDVgkChr7Gb07/Lcka0Yur9KY3MGpx5S2SJbheRKRHPYxOroHpfNWq4+dzLAqfk0R1zaRQMM6dIFURUyfFi9UJM7amDXNdmTEtm2hmy0qTB8TadkYPi6p+Pa7ld0+yMy0wadktQmvHy8W4ukaStIKFCyi0gurX/vkS3P1fCEP2bt/iJLYXU0zWEDq6mJHKicKxdMnzRxVzSmL0Ihdl1TEQEnhh+IOH0NZI+ZBC+qkvxSjna/jGX/L9/Ejr+9AaeqiK6qoh7M09y8hsCB8dsUzbdVqEwXMDoGUUqu5y7JVsOuEzeiKbp9cppbhsf/YoNbZsN+sPaD2Hmd9XzuQRJWaHfIYwAAHlxJREFUjvWJBmpKKICTiyKa7VQUYVIcttXR/Pzbvkg04BG5oLRm5fKQMBMRZiJMf41VUuMVNEECSk/6lJ70KRyE3GFL6II9SUu0BzTtAU32zhSbvtBB25rcU8I4ObhricFdS9Lmfq6J1YbBG46zcLlFasEnteBT3yziH15fgna/wfzzHHLTAbnpgNVzTeyGpj4JjUmobYX+vS3697YIEopT1yRpjIo2bOjQa91PlDUoSI7X8TNC9pXYc4zEnmPUpgxqW0QwvD0omrKpxZDUYsj4nQ369q/nvp2apluUw8trgpQidzKgNSqfvyZKHdmQWgrJXztPY1PEzKsEn5+e80guBYSOFHX7Dndwqxp3RfhfUnOa1Rd51KYUtV0+hae6WC3pPtUKdny0TmtEk/xijtoWeO/Pfob3/uxnejDJ97/zFtqzWYK0Jn/AIn/Awi22mb1e49oBZqnLzuedwGwamE2DrdceZ8d1RzF8aEzI/WVm5Kic/8zoBzZsw86knW1UBWcFV6o2wK6urzNGIHS6fkoRJsFsK7QRw+uqm6FmCwyyYKA8DTn5cZtdk25e0R4yUVqjNHQKsRRbV2M1oXoOlPYHVDfbveupCKGaLZu0BxVhAlwzbmKaNHDL0BpWnKzl6Y55VKekwBlkI5wlk27REFIwA6K4bpCdFhUowyfmfYeVC4Q1UYXQHgkwOoY4QxPsyjpk08spOtNZUp4QkeW3jffGmT8c4WUMtAGr5yuK+9aIylwp2ha1qC1VQ9xlucfWRW38ZpL6uIk36qGVQ2Y2hlBqkSBcmC1CKsSsmlSn3N712gMC5QwdF20oIlPOa44pdCDsjkbDpLbJwq7L8zR9jV9IEBQDElWD8oTH3RXRgnWqin9pZnhdusF7G4ogKTUJgO5yEgwIQpNwKcFiPkNky/WenBnBskPsNHQHQowDRm/ntWEbdrbY2YSiOSvSMunShN79qObNHxGptMgVNIyfgfakj7Ng9aCJbkWzdGVI32PitBNloasFyB+R7tbUgqJThMSy7AQAuv0ad0Ucd+64bPXXGCMjW1gSo4vq2A9nsVrQHJXnkj6lyMyFtEvSDVt4KmD5grhwWNZ4OcX4bVVWLs7F3DJynjZjbhctDlQrSV+AOM2R3R0qW11CV4qyRgxQMT1Na0QRJAX77uUhc1L3zvNyCiPQtEYgc3L9vNqUFJC7BUVrU0DmmNVjjAzSYNdEgBoFtW2QiGGZaMG6twcVKoSBxwOWLo67XkNEF1XLgksElYv93nnuvE3mpKRIIguu+5kHAbj9n55PeyREBfKZZkeRmo/TY9dVac9msRuKQ2+/mUv+8EZqVwrONepYbLslwPj9JardBAszRc7bKU2hx74yRZDUJBdVTzs2ikOTRFnz0Mc2WCE37OyxZ8vBn/WUv+7EhN71mncz/hPHARhK1Jn+tXNY3ZUkdBVBOlZoAhpbQtInTNxVcVTNUUXhqVjzMxJyLqut0QZc+84HuPcvhdupuL/BqeuyJBc0ydUQL2v0Gp5ScyK8rA1FY9TEqcv5IHh1L6/olHTcqi9OFKC6DYZ3i5C3n1K4VU19Qk7UlsATtSEds9qA7EycV7ckXx+kwLqizAWDcyRNcZqPL4/if2EAu6kZfPsJTv7zVC+vnihHdPNyf60hhden0XEkvfnzHnNXJkDJouNWNK3htdoAJFZlgWlc1SJzfwq3Kuc5DdGA9dOSwzb89cav0BUa4O5Ul+yjLlZnPY+vQnCqmto2yB8SdkezI+d1+kxMX7N0saL/cU15p0HoxjDXsiJIa4IkZE8oHv3tm7jsfTfIs84oXvLWB7n7b5+PNqG+JSJ3JO4byMr12oOK7kDI9o83Of4b8l5wKsXxX93QUN2ws8ueDQd/1jv3TN+4HvjrGyndEbe9O1IkzUwb/NEvf5Tf/dDbCZKxotLeLrUph05JUdoXECYUfkp+5IWDDeauzpJY1VR2Qu4I2G25v9I7pml+YJzqZosgpUgua+oxv0pnNMRdFNy7s2qSP0KvAJg5JcyOblWTnW6xcl4aM+527/Yp0nMhKEVySZxrFNMPpE5rrA7UJxRBVqN88GJcfWm3ReZ0wMoFNulTEdpUWG3x4J0+2SE4dY3V1pgeeDFePzftUZ90SK6ELJ9vYXpQPCiLQnWTTW1bRGLZwOxCYlljxUXadsmgOaHJHxbBET+nyUzHXaFZicqdqvQSDD3QpLwr1bs/bUDhaEhjxARNL7/tlE1GvhbQGrLInPaZu9JZ5/950Gf6DZrErIN/Tpv83Yke3PHYm0zyByzCBLQuaFP4WoI9v3czABd/4EZyMwEnX6mxc11SX8/06A76HzAJE5ImSqxCbWtElJX3cvsdnvjQRkF1w84++37DJM/6guqGbdiGbdiGnVk7KyL3xOiEHv7dX2Fgd5xGyCvyR32awxbB09IFAM3NAV/8sT/nF9/zbmqTJt2i7snsBWnhVUkuSodj5ChSC5LPaYybtIfk/eKBiPqE0cuP2w1Na8igNaoZuS+kNWhS3yzXS84r0gsRrQGD7nU1Ul/K9iL38k5JuRSORNQnDaEYiLVeW8OKxJLwylsd2Y0U4wan0FZ4GYWfVWROhUS26qWBnHrE6k6L1nhI7oiJ2dUUn5QOp/L2BK1RRfFASKdgYHU02pB7T64ErO60sevCsZ6ei+i+RRSqOg+U6N8fiNSgIamsNToHI4D0vKgeqfgZF46svSdQy05B/l5FUN0W1z7K0OnXpOYV1R0hI/fA8oVyE4lVJWik4YjcU4r2sCLc2QDAryRwi20pntpaOnjj6z72mzdx3l/eSOqFy6xM96HyHpEntRXVNtFWROaITZCS2kB93IrnyzPnlvl+23cbuZ8txbgNO7P2/fpevx/cMmfUtKPZ/ndNjr0nZoW0Ipw9Jo1xi8zpkPaASXIhRmmcE/ETH3oPdlaTWozwr2qS+nzMKqZAK422IDsbYP/6PPr90uGY37fK9OsGSC7pHgyQOJedWvDpe2iV1SuGaA2IA0zPxmNTUNts0JgKcEKDlctCBu+PHY4GpwbVLQZWC+zW+kJp1wU7HrmaTkbG5lQljbB8oUt9c4RbVixd2cZ1AyYKFQCOL5fIfj5DasHAecs8nX8corpV0lXahOSCUB2rCJrDBkF868klcFcFi56ei7A6mtadJRlnArQhyJTKTs3AHo1bXWOoDKlsczA7Cj+tCQsBxgG5v8hUdAYU7WFNckFYNNdoC0AQRp1+EU5xah5TnxUHXt2ZpfBEheNvKlI60GZ6UwK/Ivcwfpti9npBxWy7JeC8P3+UR94vokbn/eWN7P/lm7jq19/FEFDdlqR/75pQiYHZNVEagowmudBh6RK5+WB765lMu7PSNhz7c9N+ECiasyJyTw5P6Jf/42s4dOs5gBQjE0sSNa5e6UPXoP9BcTj1KShcvoT+9ABBQqLJ2lb5nMSyICgSq5rGuCI1t67E1C1pYXUsRQzfLzS79U1yXpjQJOcNmud3Se9zxWGeJ7g+954sZlfGUtsijI5r+f/GhFAC9z9i4FYjli8yGXh0HXPdHDapTYm+qbbo0fOW9oUYAdTHTQxfEyYVVnMdmtgaFjUlp6pILGuKT4rzWj03RXohpF00qW0VxspczEKpLSifY9Et6XhxUng5uZ6XF2ZNp6qpTymMboyCQdgdc8c1iy8KUJ7B2B3rOrAqEFHv0FHYTRHEXr4sXhTaBnZVYQRCYWz4UL9axpm9L0VjkyYY8FANC+UrDC+GiJ67imsHBKFJMdli8Z8mewic/tEq1ieL3P+hD/PyA69msZGh0YzrML6B5QYkHsrQLWiyM+vfe+qUYu//fG5E7hv23LYz7eDP/py7grlmTjhYEhKhml2B/SWyXUoPmz38t9lWLD5Vwssp7JZeF98IJWKNLInK/XwkEERLjm5/SGJZKHYjUwkKJyO84NkTBvnjIeZ8TOJVg85qgs5qokfoJSgU1RPxBvCKEYUDBvnjHVqDBpGtWd1psbrTotMnBcjkoiJ9WpM+qTE74gi7OdGDtZsiymE3YqIsUzpbQc4LnZjieChBeyhB7oRHpyBIFD+jMTs6hlIqGsMmVhOIhPjMy4JbiY9VQeaIoIc8s25BjsjSGH4M2fQUnYLR29k0JkXUO0zKLsRuaeyqgV2N8fk2tEYjsrMhpqdFGs8KMTuSKts0toJVjyX+4ganynSB5XKWlaNFqt2EYPxzXexcl5VpaTl9+YFX8+Vdn6daSaG1kiMwUEfT0pCV1phdsBoKq7Ge0tqwDTvb7dlsdDorfhZaweLBAbSl5VACrVManPuydEuK3MmQ3MkQ64oyOhlheBJ9l3cJPNHsKiJLEcXdnnbVoD2g6H/Co/8Jj5G7DawOWG1waiFuJcJqKqymIj0fYjdCcsfAz0nkb1UsrIpFa0gRJmRhaWwNCFKK0JbDrho0R6E94LB6UUjqlDT8qAi6eTnHz4hjbI0+bVHIK4Kkoro9dt79SpSXTEVzyCBIa4m6Y756t+zjln2qWx28vFzbaiq6fYr6pEl90sStahKViMSywvSkcaq2LaK2LaI9HGG1ENx9zIe/Nk4VCYWCnesS5QNWLot6Y8mekGg9itNB7ZLC74vkSGs6QwFOxWDxcgO7pbH2ZLH2ZHEamuKTITPzRQYejdDpgOwVS2SvWMLoGOwYXaB/2woLM0XqWyJSX8+Q+noGlfeobjNYbGTYcvvPcexlH4V5F+ZdRsdW0VubBCmN1Va4tRA/I4tce+gHv/vcsA37Tu3ZcvBnRc5dRZDfWia6XXLEYVLw6iqA2s6A7FMWtYm4aemfCtSu9UktRTSHDfof19Sm4tRLQWF2pHGpOxBQeNJi6ULpJm1uCskdNmgNaYyuHad+5Pr1CRMvb2JfXMa9vY/qNnrYcoCReyqcvrZA5qg0BnVlmGgFqUVx6KlZi9q2iPzhNeEJKaQaAeSPB0SWYuF5spYm43OyxwVLnjklzVAgzU92DbrFiPSsQWRDZZvgK80udIpgtqXTMzsb9cTBq1tk8fJy4Gcj7MZ6KgRkZ9PtM8ge19Q3C48MSMOY6Wmyd6YJUpIGqk3FJ5VEuMPsyu7HrWgSQ035PKWx7skLVr+tWL4IgkFhZwxSrigolR3KOwxUU/XYHaPzQg49uJnI1px3yTSnPjNF5cI4leWZ9O8NmNuVwLA02z71Lp76CRHdvuQPb8RNy0KjDWgNmCSW42Ly8oZz37AfLns22CTPish9wzZswzZsw86sfVvnrpT6qFJqUSm172mv/alS6qBSaq9S6p+VUoWnvfdbSqmnlFKHlFIv/04GoS1oPVbs5ceJIHsyZOihJvknLbw4VZJY1ei3LGNULdpFgTJWthtYTRGyyJwKGdndYfPfHSU9baEVDO9uMby7RfExA23B0EMR6YWQ0r6uqGIqafjZ8jfTBA/3YXY0qTmFP+DjD/hM3lph5pUFRu+q0JwMcWqaqY+eYOqjJzACaL2gidWKyB2PyJw0esRhVgsKRzwSK5rKVovKOSZDD0UMPRQxdtuqEF+dCrHrApeMLOlcdWqa0fvahJkILwfjt5Up/c39lP7mfqpb6e0GQNJLQ3t8hvb4TN5Wx6lohh4KmPpXD7smvDaGL4Xm/HGfgUeadIqK/sc0dgPsBjhlaPcb3Po7H6RxWZvqNsXUzUfk+MwSgw+1GL6vSv6oR2vQwL0rh3tXjk2/48VEb1JL2PqZGrm9Lrm9Ln2HIs65pYp2ItyyCG0UXn+KwutP4S6Z7Hj+CXTJ49hXpvCz0qDU/4CJapvMX2kS+gZhx2TovEUu+cMbueQPb+TR376JTa88Tm5aUkrJlUhmrwGN8WcmkP1szO0N+8Hb2UTm9XR7+ejF39cUzXcSuX8MUZ95ut0OnK+1vhA4DPwWgFLqXOAtwHnxOTcppcxvewUtOPA1ZyQ6pgpr33HMrv7/2jv3GLnq645/zr133rM7+/Y+vTbYxHKh4VUgoUqcBCNAUQlSBEGoocqDJm3TpH+QuEVtqSq1/atVK6WoqK0CpQLUKKEWSkqoQ+SIGmKbEGPXrG2Msb1P73OeOzN37q9/nLtri3ghXXZ31uPfRxrNzJ3ZmfObe/bce3+/c76HSkug/UMDQ74Uw0RVIbL51PxiCmKkaChnHJz5GiQTiAlL5EdmiY7M4id14dIrBuT7XCLZMvFJQ3xS2/hN7NxIaVMF40KhzyCRAIkEBFFXm19XfKQmJCdrEIvqDbhtyxCVZlcXaOOQPDFF8sQUs7/uEx/NU+zRvqaVFhUyEwNBOkpi2scr1fCTQnTWUE1p79TcRsEp15Ckj3FBimXclgxuS4b4lJAf0DTH9EiA4xuSh0dIHh7BT0dUdCshlNsixKcMflJvXh4QcPNlYjO6CJserZEerdE0XMMrGD5//F5iRxPU4gYyacikCdJxom9P4JSqJIbGiWYNbklvtUyC6JxmAvkJQFQHp5LRbJv5njSpUx61mK5zTOZTTOZVstKTAC9Sw08YonPm/EK6F+CWBS/mEx2LcG42jZ9SbZxPH7uT56/6IcmxKo4P+R4Xr6jN1OPLn5ZZfd+21J31nl66WgF+WQ2y3/XaPcBnjTEPiMgfAxhj/jp87QXgUWPMvvf6/ETPgNl29x+Ru0KfR2dUKXFw5ylO/nSQjkMB2Y36f1RuM8SnhGoSSv0+iWEP82FNW2z5Xgr/gWmink/puQ0AFFRQkc6fB+R7XYofLZB5McnUxypEEpqC5/0iTS1uSJ1V1cgN+w1jt4R/t22S8vNdOHdOMvN2K/HeApmUVgDNvbyB/j0FJn4jRenWPF3PJBj9rE5mb3zCZfYPcsyMZIhNuGDOK1tWWwOMZ/i1bWc489xmajEt8QeopB3i949RfqqbWgy2feEor7ytk+Ct/x0nPeoTRISR+/V7/JIumzhZD3de8JsCaKrSfCC+2IWq2BuQuDJL4UwTXlcJxzHUTmqOeBA1dLyuuu1jt0DbYaH2menFfXNd1zAvn95MZSzJxhcCgj+c1N+1EmF2LkX8SILiRh8cQ/8LYb/T6Sqnb4/h+Jqpk726Quq4HgwdXw/g1ZQG/YV0TdBALaE7qkyxZjcBNL8TkByrsuepf2HL018hMySLhWbGgbe+uTzhsNX2bZsKaflVWU6a5GoXMX0BeDZ83Ae8csFrZ8Nt70ln+xxBVKiG0r1O2WPT8znGpgdx27Q5RXJcVzhLG4RCX0BsyqFrn8v0dkPkUBMA8Zkq2d3tlFwot+niauZ4qNLoaMVl7LUUbjmg+bUYhV4NOB2HfKJzVcY+kqBvr8/EdR44+n21/+hE4obqSx04vQH+sSY4rIHxmb/6W37/yNdJjQUU3klRixqaXlZZ32JXQOlgOwxUiE17KiAWVoW2vuQzvCPKifHNxIwuAE9/6HxhVO173SRKAdPXOJz4x21sGtZAPrVdqKYd/LjgvJUgPqVXIwBtR4oMfyJFYszFuKolv6hCWRPiuzOkKoZ7du3jqSd3kswtaN8Lbtkwu8UhNg2FPmj9N52JyPe5vBprJ2LA9cCp1pj8cQ+gi6ytBcM//8nf8fWvfQ0/4VDsCEXTnAgdhwzdv/cW03+zifygd74CN2vID6hsb8d+l9yVAZkhHft8mxYoOWWtNvZK59Mcs4MO+b7Y4iLrXTvvo9i7sLL9fh62bD6wb1ssvyorXej0gYK7iDwC+MC/L2y6yNsu+q8nIg8BDwHEnRQ91wZseVbPpEudwmPffYx7H31YuwS1uYvVn3179QBQ2KBn710HA8otGgEmro/gzkNyIsCI0P6/84zdrEUwlVaH2LQ2s57d6hDNQffPNIAP318l01zA+68Es1d6ROdUNhg0jbHrYJlyq0fXz6tUUx7T2/Vn++Y9X2T2Uy6VFq36nLvCoZoJDyYjDt2vVpmoRonkDE5NPwvg7Cej9Ly8UHwkJPdkKW3UA1R0psL4zSmGbzNses4nN+Cp2iOQGjVkB11ah3w63nDI9cvi/PvIx1NUmwzJUc30OXerT/dLGjTbj9bI/m6W2TfbeOl3biL/JZ90t1aTdqQLFJ7qpf2IT3ajR2qsxkx4oEmOGkClDNJn5yn0xhaDrVPRjKBd936Jibs0eC/ouc+3u/hpQ/MnZ3jnnwbZ9vezFDbrKXpuwCNzwuAcdZhvE656osDxB/Rg2feTgMT4PFPXpHDLmu5Y7FRbElMB+R6XzJBw1877+MGLzy6qSa5GcF8x3yb5S69biQHLUqxkgF92cBeRB4FPA58y5+d2zgIDF7ytHxi52N8bYx4HHgdIbhgw+V6HyINagp/wfO77s4eJ5QMmB2okJiKLei5zgxGCqC64Zje5JIaq1KJ6Bt7703lGfjOhZfNdcHZHlcReDYyRvOqgZzc7tBwPMA7MbgnPls/EKc8miAhkP+TTftClGhYrJSYMYx+JET9nMG6UYqdDbEqHO/7RDN37S0xvi+PHhbY3a4uf6ZYNYzerbkq5VT+r2KMHk47XtLHH5DUR/OtzDLbnOX1G56N7u3MEP07RftBl4stZ4j86P2/RdLqCW44QRITcgEOpUzssAbQeC5i4QZjZDh2vGyQaUOjReZnsJgfzajuZccOpbzlEh1wK4XxIqdpMolOYb/fwE7Dh1SLVlAbbUqdqzc9ucag0J/DjwnxXKK9sHDoOVZm6Jk3b0YCpq4XO13UnBTGHszs8Tv3pTXhThtGPty02sy636kFgoQr27YcdgpweoXL9Uc5dp1NkXl6YTp9Pdyy36hx7bhMUe9u54S++uqgmecOjX72Yiy2blfTtZmn7pQOADeyW92Kl0iSXlQopIncA3wJ+yxhzobDHbuBzIhITkc3AVuBny7bOYlljrG9bGoX3XVAVkaeBHUAHMA78OZpBEAOmwre9Yoz5Svj+R9C5Sh/4hjHmh+9nRLxvwFz7sW+osiKhVomvjRli0/q4mg71XK6q0nRUi5AA/vLLT7Lr6d8GoOlUWP0ZP18YtDCNEMlrM+1Sp9C3d57Tt8eIzehnLghtSU311J2qKieCnmVWWrSoqNZUg2hA276wMKpPaHpHtWbmtbc3Ub34ID1SY3QHJM+4xKdMWHyzcBaqHaYiUx7NJzUzaEHELJrTpt7RrGG+Q1SbPbxSyA0ImZMBflyotGiq44V/N7dV9Wb8FHpWvV3P3P2krjnEp6HYbUgN65w5hAJqRq8uvHmo3TZD5Ac65x5EhFJXWNU7q1NLXnGhUbkWhMVmhXKLof8nPn4ibKyREpJjVcZvii12h8p/WMVsMvvji5rwXf/jMnmtIX0mbLqRgMrVRVL7kqHsgqHlmNqZ79cCq9wmLZrSKSPl4KOP4XQfX9K/llpQXQvftguqlg/C+03RrPtmHTfeeKM5cOBAvc2wNChWOMzSqKx/4TCLxWKxrCg2uFssFksDYoO7xWKxNCA2uFssFksDYoO7xWKxNCA2uFssFksDYoO7xWKxNCDrIs9dRM4BBWCy3rasAR3Yca41g8aYznp8sYjkgKF6fPcas57292qznsa6pG+vi+AOICIH6lVospbYcV5eXC6/w+UyTrh0xmqnZSwWi6UBscHdYrFYGpD1FNwfr7cBa4Qd5+XF5fI7XC7jhEtkrOtmzt1isVgsK8d6OnO3WCwWywpR9+AuIneIyJCInBCRXfW2ZyURkVMi8oaIvC4iB8JtbSLyoogcD+9b623nchCRfxWRCRE5fMG2i45NlH8I9/EhEbm+fpavHda3Lz3fbiS/rmtwFxEX+DZwJ7AduF9EttfTplXgE8aYay9IndoF7DHGbAX2hM8vRb4D3PGubUuN7U60c9FWtLfoY2tkY92wvn3J+vZ3aBC/rveZ+03ACWPMSWNMBXgGuLvONq02dwNPhI+fAD5TR1uWjTFmLzD9rs1Lje1u4EmjvAK0iEjP2lhaN6xvX4K+3Uh+Xe/g3gecueD52XBbo2CAH4nIQRF5KNy2wRgzChDed9XNupVnqbE1+n6+GI0+5svJty9Jv/bq/P0Xaw/VSOk7txpjRkSkC3hRRN6st0F1otH388Vo9DFb317n+7jeZ+5ngYELnvcDI3WyZcUxxoyE9xPA99FL9fGFS7fwfqJ+Fq44S42toffzEjT0mC8z374k/brewX0/sFVENotIFPgcsLvONq0IIpISkaaFx8DtwGF0fA+Gb3sQ+M/6WLgqLDW23cDnw+yCW4C5hcvcBsb6duP49qXp18aYut6Au4BjwFvAI/W2ZwXHdQXwi/B2ZGFsQDu64n48vG+rt63LHN/TwChQRc9gvrjU2NDL12+H+/gN4MZ6279Gv5H17XVg7/9zbA3j17ZC1WKxWBqQek/LWCwWi2UVsMHdYrFYGhAb3C0Wi6UBscHdYrFYGhAb3C0Wi6UBscHdYrFYGhAb3C0Wi6UBscHdYrFYGpD/A14j6EujW1jmAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "##################################################################################################################\n",
    "#\n",
    "# YouDo:\n",
    "# Make a pair of plots as follows:\n",
    "#    1) Show the correlation matrix between columns of the data\n",
    "#        (hint 1: it should be 124x124, hint 2: check out plt.imshow() \n",
    "#         Hint 2: numpy has a function for generating the correlation matrix)\n",
    "#\n",
    "#    2) Make a similar plot showing correlation > 90%\n",
    "#\n",
    "#  Stretch goal:  using plt.subplot()  make a single figure with both plots side-by-side\n",
    "#\n",
    "#######################################  BEGIN STUDENT CODE  #####################################################\n",
    "\n",
    "xs = df[df.columns.drop('Label')]\n",
    "print(xs)\n",
    "\n",
    "plt.figure()\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(xs.corr())\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.imshow(np.abs(xs.corr())>.90)\n",
    "xs.corr()>.90\n",
    "\n",
    "\n",
    "\n",
    "#######################################   END STUDENT CODE   #####################################################\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Label', 'Pyruvate', 'MethylSuccinate', 'Phosphoglyceric Acid',\n",
       "       'Glucose 1-phosphate', 'Malonic Acid', 'Fumaric Acid',\n",
       "       'Alpha-Ketoglutaric Acid', 'Sarcosine', 'Cadaverine',\n",
       "       ...\n",
       "       'Histidine', 'Phenylalanine', 'Arginine', 'Tyrosine', 'Tryptophan',\n",
       "       'Cystine', 'lactate', '3-Hydroxybenzoic acid', 'Succinate', 'Glucose'],\n",
       "      dtype='object', length=125)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##################################################################################################################\n",
    "#\n",
    "# YouDo: (machine learning exercise - random forest decision tree)\n",
    "#  1) Remove any rows with na values\n",
    "# \n",
    "#  2) Split the data into \"test\" and \"train\" bits with ~30% in the test set.\n",
    "#       The \"Label\" column is the \"y\" data.  All other columns are the \"X\" data\n",
    "#       Give them these names:\n",
    "#       X_train, X_test, y_train, y_test\n",
    "\n",
    "#######################################  BEGIN STUDENT CODE  #####################################################\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 1)\n",
    "# drop rows with NA values (random forest decision tree), machine learning, wrangle their data\n",
    "\n",
    "dfml = df.dropna(axis=1, how='all')\n",
    "\n",
    " \n",
    "#2)\n",
    "y = dfml.Label\n",
    "X = dfml[dfml.columns.drop('Label')]\n",
    "x_train, x_test, y_train, y_test = train_test_split(X,y,test_size=0.3)\n",
    "#can also sampled row numbers in one group and not group on the other side \n",
    "\n",
    "#######################################   END STUDENT CODE   #####################################################\n",
    "dfml.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on package sklearn.ensemble in sklearn:\n",
      "\n",
      "NAME\n",
      "    sklearn.ensemble\n",
      "\n",
      "DESCRIPTION\n",
      "    The :mod:`sklearn.ensemble` module includes ensemble-based methods for\n",
      "    classification, regression and anomaly detection.\n",
      "\n",
      "PACKAGE CONTENTS\n",
      "    _gb_losses\n",
      "    _gradient_boosting\n",
      "    _hist_gradient_boosting (package)\n",
      "    bagging\n",
      "    base\n",
      "    forest\n",
      "    gradient_boosting\n",
      "    iforest\n",
      "    partial_dependence\n",
      "    setup\n",
      "    tests (package)\n",
      "    voting\n",
      "    weight_boosting\n",
      "\n",
      "CLASSES\n",
      "    sklearn.base.BaseEstimator(builtins.object)\n",
      "        sklearn.ensemble.base.BaseEnsemble(sklearn.base.BaseEstimator, sklearn.base.MetaEstimatorMixin)\n",
      "    sklearn.base.ClassifierMixin(builtins.object)\n",
      "        sklearn.ensemble.bagging.BaggingClassifier(sklearn.ensemble.bagging.BaseBagging, sklearn.base.ClassifierMixin)\n",
      "        sklearn.ensemble.gradient_boosting.GradientBoostingClassifier(sklearn.ensemble.gradient_boosting.BaseGradientBoosting, sklearn.base.ClassifierMixin)\n",
      "        sklearn.ensemble.voting.VotingClassifier(sklearn.ensemble.voting._BaseVoting, sklearn.base.ClassifierMixin)\n",
      "        sklearn.ensemble.weight_boosting.AdaBoostClassifier(sklearn.ensemble.weight_boosting.BaseWeightBoosting, sklearn.base.ClassifierMixin)\n",
      "    sklearn.base.MetaEstimatorMixin(builtins.object)\n",
      "        sklearn.ensemble.base.BaseEnsemble(sklearn.base.BaseEstimator, sklearn.base.MetaEstimatorMixin)\n",
      "    sklearn.base.OutlierMixin(builtins.object)\n",
      "        sklearn.ensemble.iforest.IsolationForest(sklearn.ensemble.bagging.BaseBagging, sklearn.base.OutlierMixin)\n",
      "    sklearn.base.RegressorMixin(builtins.object)\n",
      "        sklearn.ensemble.bagging.BaggingRegressor(sklearn.ensemble.bagging.BaseBagging, sklearn.base.RegressorMixin)\n",
      "        sklearn.ensemble.gradient_boosting.GradientBoostingRegressor(sklearn.ensemble.gradient_boosting.BaseGradientBoosting, sklearn.base.RegressorMixin)\n",
      "        sklearn.ensemble.voting.VotingRegressor(sklearn.ensemble.voting._BaseVoting, sklearn.base.RegressorMixin)\n",
      "        sklearn.ensemble.weight_boosting.AdaBoostRegressor(sklearn.ensemble.weight_boosting.BaseWeightBoosting, sklearn.base.RegressorMixin)\n",
      "    sklearn.ensemble.bagging.BaseBagging(sklearn.ensemble.base.BaseEnsemble)\n",
      "        sklearn.ensemble.bagging.BaggingClassifier(sklearn.ensemble.bagging.BaseBagging, sklearn.base.ClassifierMixin)\n",
      "        sklearn.ensemble.bagging.BaggingRegressor(sklearn.ensemble.bagging.BaseBagging, sklearn.base.RegressorMixin)\n",
      "        sklearn.ensemble.iforest.IsolationForest(sklearn.ensemble.bagging.BaseBagging, sklearn.base.OutlierMixin)\n",
      "    sklearn.ensemble.forest.BaseForest(sklearn.ensemble.base.BaseEnsemble, sklearn.base.MultiOutputMixin)\n",
      "        sklearn.ensemble.forest.RandomTreesEmbedding\n",
      "    sklearn.ensemble.forest.ForestClassifier(sklearn.ensemble.forest.BaseForest, sklearn.base.ClassifierMixin)\n",
      "        sklearn.ensemble.forest.ExtraTreesClassifier\n",
      "        sklearn.ensemble.forest.RandomForestClassifier\n",
      "    sklearn.ensemble.forest.ForestRegressor(sklearn.ensemble.forest.BaseForest, sklearn.base.RegressorMixin)\n",
      "        sklearn.ensemble.forest.ExtraTreesRegressor\n",
      "        sklearn.ensemble.forest.RandomForestRegressor\n",
      "    sklearn.ensemble.gradient_boosting.BaseGradientBoosting(sklearn.ensemble.base.BaseEnsemble)\n",
      "        sklearn.ensemble.gradient_boosting.GradientBoostingClassifier(sklearn.ensemble.gradient_boosting.BaseGradientBoosting, sklearn.base.ClassifierMixin)\n",
      "        sklearn.ensemble.gradient_boosting.GradientBoostingRegressor(sklearn.ensemble.gradient_boosting.BaseGradientBoosting, sklearn.base.RegressorMixin)\n",
      "    sklearn.ensemble.voting._BaseVoting(sklearn.utils.metaestimators._BaseComposition, sklearn.base.TransformerMixin)\n",
      "        sklearn.ensemble.voting.VotingClassifier(sklearn.ensemble.voting._BaseVoting, sklearn.base.ClassifierMixin)\n",
      "        sklearn.ensemble.voting.VotingRegressor(sklearn.ensemble.voting._BaseVoting, sklearn.base.RegressorMixin)\n",
      "    sklearn.ensemble.weight_boosting.BaseWeightBoosting(sklearn.ensemble.base.BaseEnsemble)\n",
      "        sklearn.ensemble.weight_boosting.AdaBoostClassifier(sklearn.ensemble.weight_boosting.BaseWeightBoosting, sklearn.base.ClassifierMixin)\n",
      "        sklearn.ensemble.weight_boosting.AdaBoostRegressor(sklearn.ensemble.weight_boosting.BaseWeightBoosting, sklearn.base.RegressorMixin)\n",
      "    \n",
      "    class AdaBoostClassifier(BaseWeightBoosting, sklearn.base.ClassifierMixin)\n",
      "     |  AdaBoostClassifier(base_estimator=None, n_estimators=50, learning_rate=1.0, algorithm='SAMME.R', random_state=None)\n",
      "     |  \n",
      "     |  An AdaBoost classifier.\n",
      "     |  \n",
      "     |  An AdaBoost [1] classifier is a meta-estimator that begins by fitting a\n",
      "     |  classifier on the original dataset and then fits additional copies of the\n",
      "     |  classifier on the same dataset but where the weights of incorrectly\n",
      "     |  classified instances are adjusted such that subsequent classifiers focus\n",
      "     |  more on difficult cases.\n",
      "     |  \n",
      "     |  This class implements the algorithm known as AdaBoost-SAMME [2].\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <adaboost>`.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  base_estimator : object, optional (default=None)\n",
      "     |      The base estimator from which the boosted ensemble is built.\n",
      "     |      Support for sample weighting is required, as well as proper\n",
      "     |      ``classes_`` and ``n_classes_`` attributes. If ``None``, then\n",
      "     |      the base estimator is ``DecisionTreeClassifier(max_depth=1)``\n",
      "     |  \n",
      "     |  n_estimators : integer, optional (default=50)\n",
      "     |      The maximum number of estimators at which boosting is terminated.\n",
      "     |      In case of perfect fit, the learning procedure is stopped early.\n",
      "     |  \n",
      "     |  learning_rate : float, optional (default=1.)\n",
      "     |      Learning rate shrinks the contribution of each classifier by\n",
      "     |      ``learning_rate``. There is a trade-off between ``learning_rate`` and\n",
      "     |      ``n_estimators``.\n",
      "     |  \n",
      "     |  algorithm : {'SAMME', 'SAMME.R'}, optional (default='SAMME.R')\n",
      "     |      If 'SAMME.R' then use the SAMME.R real boosting algorithm.\n",
      "     |      ``base_estimator`` must support calculation of class probabilities.\n",
      "     |      If 'SAMME' then use the SAMME discrete boosting algorithm.\n",
      "     |      The SAMME.R algorithm typically converges faster than SAMME,\n",
      "     |      achieving a lower test error with fewer boosting iterations.\n",
      "     |  \n",
      "     |  random_state : int, RandomState instance or None, optional (default=None)\n",
      "     |      If int, random_state is the seed used by the random number generator;\n",
      "     |      If RandomState instance, random_state is the random number generator;\n",
      "     |      If None, the random number generator is the RandomState instance used\n",
      "     |      by `np.random`.\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  estimators_ : list of classifiers\n",
      "     |      The collection of fitted sub-estimators.\n",
      "     |  \n",
      "     |  classes_ : array of shape = [n_classes]\n",
      "     |      The classes labels.\n",
      "     |  \n",
      "     |  n_classes_ : int\n",
      "     |      The number of classes.\n",
      "     |  \n",
      "     |  estimator_weights_ : array of floats\n",
      "     |      Weights for each estimator in the boosted ensemble.\n",
      "     |  \n",
      "     |  estimator_errors_ : array of floats\n",
      "     |      Classification error for each estimator in the boosted\n",
      "     |      ensemble.\n",
      "     |  \n",
      "     |  feature_importances_ : array of shape = [n_features]\n",
      "     |      The feature importances if supported by the ``base_estimator``.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> from sklearn.ensemble import AdaBoostClassifier\n",
      "     |  >>> from sklearn.datasets import make_classification\n",
      "     |  >>> X, y = make_classification(n_samples=1000, n_features=4,\n",
      "     |  ...                            n_informative=2, n_redundant=0,\n",
      "     |  ...                            random_state=0, shuffle=False)\n",
      "     |  >>> clf = AdaBoostClassifier(n_estimators=100, random_state=0)\n",
      "     |  >>> clf.fit(X, y)  # doctest: +NORMALIZE_WHITESPACE\n",
      "     |  AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None,\n",
      "     |          learning_rate=1.0, n_estimators=100, random_state=0)\n",
      "     |  >>> clf.feature_importances_  # doctest: +ELLIPSIS\n",
      "     |  array([0.28..., 0.42..., 0.14..., 0.16...])\n",
      "     |  >>> clf.predict([[0, 0, 0, 0]])\n",
      "     |  array([1])\n",
      "     |  >>> clf.score(X, y)  # doctest: +ELLIPSIS\n",
      "     |  0.983...\n",
      "     |  \n",
      "     |  See also\n",
      "     |  --------\n",
      "     |  AdaBoostRegressor, GradientBoostingClassifier,\n",
      "     |  sklearn.tree.DecisionTreeClassifier\n",
      "     |  \n",
      "     |  References\n",
      "     |  ----------\n",
      "     |  .. [1] Y. Freund, R. Schapire, \"A Decision-Theoretic Generalization of\n",
      "     |         on-Line Learning and an Application to Boosting\", 1995.\n",
      "     |  \n",
      "     |  .. [2] J. Zhu, H. Zou, S. Rosset, T. Hastie, \"Multi-class AdaBoost\", 2009.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      AdaBoostClassifier\n",
      "     |      BaseWeightBoosting\n",
      "     |      sklearn.ensemble.base.BaseEnsemble\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      sklearn.base.MetaEstimatorMixin\n",
      "     |      sklearn.base.ClassifierMixin\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, base_estimator=None, n_estimators=50, learning_rate=1.0, algorithm='SAMME.R', random_state=None)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  decision_function(self, X)\n",
      "     |      Compute the decision function of ``X``.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape = [n_samples, n_features]\n",
      "     |          The training input samples. Sparse matrix can be CSC, CSR, COO,\n",
      "     |          DOK, or LIL. COO, DOK, and LIL are converted to CSR.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      score : array, shape = [n_samples, k]\n",
      "     |          The decision function of the input samples. The order of\n",
      "     |          outputs is the same of that of the `classes_` attribute.\n",
      "     |          Binary classification is a special cases with ``k == 1``,\n",
      "     |          otherwise ``k==n_classes``. For binary classification,\n",
      "     |          values closer to -1 or 1 mean more like the first or second\n",
      "     |          class in ``classes_``, respectively.\n",
      "     |  \n",
      "     |  fit(self, X, y, sample_weight=None)\n",
      "     |      Build a boosted classifier from the training set (X, y).\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape = [n_samples, n_features]\n",
      "     |          The training input samples. Sparse matrix can be CSC, CSR, COO,\n",
      "     |          DOK, or LIL. COO, DOK, and LIL are converted to CSR.\n",
      "     |      \n",
      "     |      y : array-like of shape = [n_samples]\n",
      "     |          The target values (class labels).\n",
      "     |      \n",
      "     |      sample_weight : array-like of shape = [n_samples], optional\n",
      "     |          Sample weights. If None, the sample weights are initialized to\n",
      "     |          ``1 / n_samples``.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |  \n",
      "     |  predict(self, X)\n",
      "     |      Predict classes for X.\n",
      "     |      \n",
      "     |      The predicted class of an input sample is computed as the weighted mean\n",
      "     |      prediction of the classifiers in the ensemble.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape = [n_samples, n_features]\n",
      "     |          The training input samples. Sparse matrix can be CSC, CSR, COO,\n",
      "     |          DOK, or LIL. COO, DOK, and LIL are converted to CSR.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      y : array of shape = [n_samples]\n",
      "     |          The predicted classes.\n",
      "     |  \n",
      "     |  predict_log_proba(self, X)\n",
      "     |      Predict class log-probabilities for X.\n",
      "     |      \n",
      "     |      The predicted class log-probabilities of an input sample is computed as\n",
      "     |      the weighted mean predicted class log-probabilities of the classifiers\n",
      "     |      in the ensemble.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape = [n_samples, n_features]\n",
      "     |          The training input samples. Sparse matrix can be CSC, CSR, COO,\n",
      "     |          DOK, or LIL. COO, DOK, and LIL are converted to CSR.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      p : array of shape = [n_samples, n_classes]\n",
      "     |          The class probabilities of the input samples. The order of\n",
      "     |          outputs is the same of that of the `classes_` attribute.\n",
      "     |  \n",
      "     |  predict_proba(self, X)\n",
      "     |      Predict class probabilities for X.\n",
      "     |      \n",
      "     |      The predicted class probabilities of an input sample is computed as\n",
      "     |      the weighted mean predicted class probabilities of the classifiers\n",
      "     |      in the ensemble.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape = [n_samples, n_features]\n",
      "     |          The training input samples. Sparse matrix can be CSC, CSR, COO,\n",
      "     |          DOK, or LIL. COO, DOK, and LIL are converted to CSR.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      p : array of shape = [n_samples, n_classes]\n",
      "     |          The class probabilities of the input samples. The order of\n",
      "     |          outputs is the same of that of the `classes_` attribute.\n",
      "     |  \n",
      "     |  staged_decision_function(self, X)\n",
      "     |      Compute decision function of ``X`` for each boosting iteration.\n",
      "     |      \n",
      "     |      This method allows monitoring (i.e. determine error on testing set)\n",
      "     |      after each boosting iteration.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape = [n_samples, n_features]\n",
      "     |          The training input samples. Sparse matrix can be CSC, CSR, COO,\n",
      "     |          DOK, or LIL. COO, DOK, and LIL are converted to CSR.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      score : generator of array, shape = [n_samples, k]\n",
      "     |          The decision function of the input samples. The order of\n",
      "     |          outputs is the same of that of the `classes_` attribute.\n",
      "     |          Binary classification is a special cases with ``k == 1``,\n",
      "     |          otherwise ``k==n_classes``. For binary classification,\n",
      "     |          values closer to -1 or 1 mean more like the first or second\n",
      "     |          class in ``classes_``, respectively.\n",
      "     |  \n",
      "     |  staged_predict(self, X)\n",
      "     |      Return staged predictions for X.\n",
      "     |      \n",
      "     |      The predicted class of an input sample is computed as the weighted mean\n",
      "     |      prediction of the classifiers in the ensemble.\n",
      "     |      \n",
      "     |      This generator method yields the ensemble prediction after each\n",
      "     |      iteration of boosting and therefore allows monitoring, such as to\n",
      "     |      determine the prediction on a test set after each boost.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape = [n_samples, n_features]\n",
      "     |          The input samples. Sparse matrix can be CSC, CSR, COO,\n",
      "     |          DOK, or LIL. COO, DOK, and LIL are converted to CSR.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      y : generator of array, shape = [n_samples]\n",
      "     |          The predicted classes.\n",
      "     |  \n",
      "     |  staged_predict_proba(self, X)\n",
      "     |      Predict class probabilities for X.\n",
      "     |      \n",
      "     |      The predicted class probabilities of an input sample is computed as\n",
      "     |      the weighted mean predicted class probabilities of the classifiers\n",
      "     |      in the ensemble.\n",
      "     |      \n",
      "     |      This generator method yields the ensemble predicted class probabilities\n",
      "     |      after each iteration of boosting and therefore allows monitoring, such\n",
      "     |      as to determine the predicted class probabilities on a test set after\n",
      "     |      each boost.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape = [n_samples, n_features]\n",
      "     |          The training input samples. Sparse matrix can be CSC, CSR, COO,\n",
      "     |          DOK, or LIL. COO, DOK, and LIL are converted to CSR.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      p : generator of array, shape = [n_samples]\n",
      "     |          The class probabilities of the input samples. The order of\n",
      "     |          outputs is the same of that of the `classes_` attribute.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from BaseWeightBoosting:\n",
      "     |  \n",
      "     |  staged_score(self, X, y, sample_weight=None)\n",
      "     |      Return staged scores for X, y.\n",
      "     |      \n",
      "     |      This generator method yields the ensemble score after each iteration of\n",
      "     |      boosting and therefore allows monitoring, such as to determine the\n",
      "     |      score on a test set after each boost.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape = [n_samples, n_features]\n",
      "     |          The training input samples. Sparse matrix can be CSC, CSR, COO,\n",
      "     |          DOK, or LIL. COO, DOK, and LIL are converted to CSR.\n",
      "     |      \n",
      "     |      y : array-like, shape = [n_samples]\n",
      "     |          Labels for X.\n",
      "     |      \n",
      "     |      sample_weight : array-like, shape = [n_samples], optional\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      z : float\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from BaseWeightBoosting:\n",
      "     |  \n",
      "     |  feature_importances_\n",
      "     |      Return the feature importances (the higher, the more important the\n",
      "     |         feature).\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      feature_importances_ : array, shape = [n_features]\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.ensemble.base.BaseEnsemble:\n",
      "     |  \n",
      "     |  __getitem__(self, index)\n",
      "     |      Returns the index'th estimator in the ensemble.\n",
      "     |  \n",
      "     |  __iter__(self)\n",
      "     |      Returns iterator over estimators in the ensemble.\n",
      "     |  \n",
      "     |  __len__(self)\n",
      "     |      Returns the number of estimators in the ensemble.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __repr__(self, N_CHAR_MAX=700)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : boolean, optional\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : mapping of string to any\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as pipelines). The latter have parameters of the form\n",
      "     |      ``<component>__<parameter>`` so that it's possible to update each\n",
      "     |      component of a nested object.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.ClassifierMixin:\n",
      "     |  \n",
      "     |  score(self, X, y, sample_weight=None)\n",
      "     |      Returns the mean accuracy on the given test data and labels.\n",
      "     |      \n",
      "     |      In multi-label classification, this is the subset accuracy\n",
      "     |      which is a harsh metric since you require for each sample that\n",
      "     |      each label set be correctly predicted.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like, shape = (n_samples, n_features)\n",
      "     |          Test samples.\n",
      "     |      \n",
      "     |      y : array-like, shape = (n_samples) or (n_samples, n_outputs)\n",
      "     |          True labels for X.\n",
      "     |      \n",
      "     |      sample_weight : array-like, shape = [n_samples], optional\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      score : float\n",
      "     |          Mean accuracy of self.predict(X) wrt. y.\n",
      "    \n",
      "    class AdaBoostRegressor(BaseWeightBoosting, sklearn.base.RegressorMixin)\n",
      "     |  AdaBoostRegressor(base_estimator=None, n_estimators=50, learning_rate=1.0, loss='linear', random_state=None)\n",
      "     |  \n",
      "     |  An AdaBoost regressor.\n",
      "     |  \n",
      "     |  An AdaBoost [1] regressor is a meta-estimator that begins by fitting a\n",
      "     |  regressor on the original dataset and then fits additional copies of the\n",
      "     |  regressor on the same dataset but where the weights of instances are\n",
      "     |  adjusted according to the error of the current prediction. As such,\n",
      "     |  subsequent regressors focus more on difficult cases.\n",
      "     |  \n",
      "     |  This class implements the algorithm known as AdaBoost.R2 [2].\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <adaboost>`.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  base_estimator : object, optional (default=None)\n",
      "     |      The base estimator from which the boosted ensemble is built.\n",
      "     |      Support for sample weighting is required. If ``None``, then\n",
      "     |      the base estimator is ``DecisionTreeRegressor(max_depth=3)``\n",
      "     |  \n",
      "     |  n_estimators : integer, optional (default=50)\n",
      "     |      The maximum number of estimators at which boosting is terminated.\n",
      "     |      In case of perfect fit, the learning procedure is stopped early.\n",
      "     |  \n",
      "     |  learning_rate : float, optional (default=1.)\n",
      "     |      Learning rate shrinks the contribution of each regressor by\n",
      "     |      ``learning_rate``. There is a trade-off between ``learning_rate`` and\n",
      "     |      ``n_estimators``.\n",
      "     |  \n",
      "     |  loss : {'linear', 'square', 'exponential'}, optional (default='linear')\n",
      "     |      The loss function to use when updating the weights after each\n",
      "     |      boosting iteration.\n",
      "     |  \n",
      "     |  random_state : int, RandomState instance or None, optional (default=None)\n",
      "     |      If int, random_state is the seed used by the random number generator;\n",
      "     |      If RandomState instance, random_state is the random number generator;\n",
      "     |      If None, the random number generator is the RandomState instance used\n",
      "     |      by `np.random`.\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  estimators_ : list of classifiers\n",
      "     |      The collection of fitted sub-estimators.\n",
      "     |  \n",
      "     |  estimator_weights_ : array of floats\n",
      "     |      Weights for each estimator in the boosted ensemble.\n",
      "     |  \n",
      "     |  estimator_errors_ : array of floats\n",
      "     |      Regression error for each estimator in the boosted ensemble.\n",
      "     |  \n",
      "     |  feature_importances_ : array of shape = [n_features]\n",
      "     |      The feature importances if supported by the ``base_estimator``.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> from sklearn.ensemble import AdaBoostRegressor\n",
      "     |  >>> from sklearn.datasets import make_regression\n",
      "     |  >>> X, y = make_regression(n_features=4, n_informative=2,\n",
      "     |  ...                        random_state=0, shuffle=False)\n",
      "     |  >>> regr = AdaBoostRegressor(random_state=0, n_estimators=100)\n",
      "     |  >>> regr.fit(X, y)  # doctest: +NORMALIZE_WHITESPACE\n",
      "     |  AdaBoostRegressor(base_estimator=None, learning_rate=1.0, loss='linear',\n",
      "     |          n_estimators=100, random_state=0)\n",
      "     |  >>> regr.feature_importances_  # doctest: +ELLIPSIS\n",
      "     |  array([0.2788..., 0.7109..., 0.0065..., 0.0036...])\n",
      "     |  >>> regr.predict([[0, 0, 0, 0]])  # doctest: +ELLIPSIS\n",
      "     |  array([4.7972...])\n",
      "     |  >>> regr.score(X, y)  # doctest: +ELLIPSIS\n",
      "     |  0.9771...\n",
      "     |  \n",
      "     |  See also\n",
      "     |  --------\n",
      "     |  AdaBoostClassifier, GradientBoostingRegressor,\n",
      "     |  sklearn.tree.DecisionTreeRegressor\n",
      "     |  \n",
      "     |  References\n",
      "     |  ----------\n",
      "     |  .. [1] Y. Freund, R. Schapire, \"A Decision-Theoretic Generalization of\n",
      "     |         on-Line Learning and an Application to Boosting\", 1995.\n",
      "     |  \n",
      "     |  .. [2] H. Drucker, \"Improving Regressors using Boosting Techniques\", 1997.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      AdaBoostRegressor\n",
      "     |      BaseWeightBoosting\n",
      "     |      sklearn.ensemble.base.BaseEnsemble\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      sklearn.base.MetaEstimatorMixin\n",
      "     |      sklearn.base.RegressorMixin\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, base_estimator=None, n_estimators=50, learning_rate=1.0, loss='linear', random_state=None)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  fit(self, X, y, sample_weight=None)\n",
      "     |      Build a boosted regressor from the training set (X, y).\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape = [n_samples, n_features]\n",
      "     |          The training input samples. Sparse matrix can be CSC, CSR, COO,\n",
      "     |          DOK, or LIL. COO, DOK, and LIL are converted to CSR.\n",
      "     |      \n",
      "     |      y : array-like of shape = [n_samples]\n",
      "     |          The target values (real numbers).\n",
      "     |      \n",
      "     |      sample_weight : array-like of shape = [n_samples], optional\n",
      "     |          Sample weights. If None, the sample weights are initialized to\n",
      "     |          1 / n_samples.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |  \n",
      "     |  predict(self, X)\n",
      "     |      Predict regression value for X.\n",
      "     |      \n",
      "     |      The predicted regression value of an input sample is computed\n",
      "     |      as the weighted median prediction of the classifiers in the ensemble.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape = [n_samples, n_features]\n",
      "     |          The training input samples. Sparse matrix can be CSC, CSR, COO,\n",
      "     |          DOK, or LIL. COO, DOK, and LIL are converted to CSR.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      y : array of shape = [n_samples]\n",
      "     |          The predicted regression values.\n",
      "     |  \n",
      "     |  staged_predict(self, X)\n",
      "     |      Return staged predictions for X.\n",
      "     |      \n",
      "     |      The predicted regression value of an input sample is computed\n",
      "     |      as the weighted median prediction of the classifiers in the ensemble.\n",
      "     |      \n",
      "     |      This generator method yields the ensemble prediction after each\n",
      "     |      iteration of boosting and therefore allows monitoring, such as to\n",
      "     |      determine the prediction on a test set after each boost.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape = [n_samples, n_features]\n",
      "     |          The training input samples.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      y : generator of array, shape = [n_samples]\n",
      "     |          The predicted regression values.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from BaseWeightBoosting:\n",
      "     |  \n",
      "     |  staged_score(self, X, y, sample_weight=None)\n",
      "     |      Return staged scores for X, y.\n",
      "     |      \n",
      "     |      This generator method yields the ensemble score after each iteration of\n",
      "     |      boosting and therefore allows monitoring, such as to determine the\n",
      "     |      score on a test set after each boost.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape = [n_samples, n_features]\n",
      "     |          The training input samples. Sparse matrix can be CSC, CSR, COO,\n",
      "     |          DOK, or LIL. COO, DOK, and LIL are converted to CSR.\n",
      "     |      \n",
      "     |      y : array-like, shape = [n_samples]\n",
      "     |          Labels for X.\n",
      "     |      \n",
      "     |      sample_weight : array-like, shape = [n_samples], optional\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      z : float\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from BaseWeightBoosting:\n",
      "     |  \n",
      "     |  feature_importances_\n",
      "     |      Return the feature importances (the higher, the more important the\n",
      "     |         feature).\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      feature_importances_ : array, shape = [n_features]\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.ensemble.base.BaseEnsemble:\n",
      "     |  \n",
      "     |  __getitem__(self, index)\n",
      "     |      Returns the index'th estimator in the ensemble.\n",
      "     |  \n",
      "     |  __iter__(self)\n",
      "     |      Returns iterator over estimators in the ensemble.\n",
      "     |  \n",
      "     |  __len__(self)\n",
      "     |      Returns the number of estimators in the ensemble.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __repr__(self, N_CHAR_MAX=700)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : boolean, optional\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : mapping of string to any\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as pipelines). The latter have parameters of the form\n",
      "     |      ``<component>__<parameter>`` so that it's possible to update each\n",
      "     |      component of a nested object.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.RegressorMixin:\n",
      "     |  \n",
      "     |  score(self, X, y, sample_weight=None)\n",
      "     |      Returns the coefficient of determination R^2 of the prediction.\n",
      "     |      \n",
      "     |      The coefficient R^2 is defined as (1 - u/v), where u is the residual\n",
      "     |      sum of squares ((y_true - y_pred) ** 2).sum() and v is the total\n",
      "     |      sum of squares ((y_true - y_true.mean()) ** 2).sum().\n",
      "     |      The best possible score is 1.0 and it can be negative (because the\n",
      "     |      model can be arbitrarily worse). A constant model that always\n",
      "     |      predicts the expected value of y, disregarding the input features,\n",
      "     |      would get a R^2 score of 0.0.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like, shape = (n_samples, n_features)\n",
      "     |          Test samples. For some estimators this may be a\n",
      "     |          precomputed kernel matrix instead, shape = (n_samples,\n",
      "     |          n_samples_fitted], where n_samples_fitted is the number of\n",
      "     |          samples used in the fitting for the estimator.\n",
      "     |      \n",
      "     |      y : array-like, shape = (n_samples) or (n_samples, n_outputs)\n",
      "     |          True values for X.\n",
      "     |      \n",
      "     |      sample_weight : array-like, shape = [n_samples], optional\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      score : float\n",
      "     |          R^2 of self.predict(X) wrt. y.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      The R2 score used when calling ``score`` on a regressor will use\n",
      "     |      ``multioutput='uniform_average'`` from version 0.23 to keep consistent\n",
      "     |      with `metrics.r2_score`. This will influence the ``score`` method of\n",
      "     |      all the multioutput regressors (except for\n",
      "     |      `multioutput.MultiOutputRegressor`). To specify the default value\n",
      "     |      manually and avoid the warning, please either call `metrics.r2_score`\n",
      "     |      directly or make a custom scorer with `metrics.make_scorer` (the\n",
      "     |      built-in scorer ``'r2'`` uses ``multioutput='uniform_average'``).\n",
      "    \n",
      "    class BaggingClassifier(BaseBagging, sklearn.base.ClassifierMixin)\n",
      "     |  BaggingClassifier(base_estimator=None, n_estimators=10, max_samples=1.0, max_features=1.0, bootstrap=True, bootstrap_features=False, oob_score=False, warm_start=False, n_jobs=None, random_state=None, verbose=0)\n",
      "     |  \n",
      "     |  A Bagging classifier.\n",
      "     |  \n",
      "     |  A Bagging classifier is an ensemble meta-estimator that fits base\n",
      "     |  classifiers each on random subsets of the original dataset and then\n",
      "     |  aggregate their individual predictions (either by voting or by averaging)\n",
      "     |  to form a final prediction. Such a meta-estimator can typically be used as\n",
      "     |  a way to reduce the variance of a black-box estimator (e.g., a decision\n",
      "     |  tree), by introducing randomization into its construction procedure and\n",
      "     |  then making an ensemble out of it.\n",
      "     |  \n",
      "     |  This algorithm encompasses several works from the literature. When random\n",
      "     |  subsets of the dataset are drawn as random subsets of the samples, then\n",
      "     |  this algorithm is known as Pasting [1]_. If samples are drawn with\n",
      "     |  replacement, then the method is known as Bagging [2]_. When random subsets\n",
      "     |  of the dataset are drawn as random subsets of the features, then the method\n",
      "     |  is known as Random Subspaces [3]_. Finally, when base estimators are built\n",
      "     |  on subsets of both samples and features, then the method is known as\n",
      "     |  Random Patches [4]_.\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <bagging>`.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  base_estimator : object or None, optional (default=None)\n",
      "     |      The base estimator to fit on random subsets of the dataset.\n",
      "     |      If None, then the base estimator is a decision tree.\n",
      "     |  \n",
      "     |  n_estimators : int, optional (default=10)\n",
      "     |      The number of base estimators in the ensemble.\n",
      "     |  \n",
      "     |  max_samples : int or float, optional (default=1.0)\n",
      "     |      The number of samples to draw from X to train each base estimator.\n",
      "     |  \n",
      "     |      - If int, then draw `max_samples` samples.\n",
      "     |      - If float, then draw `max_samples * X.shape[0]` samples.\n",
      "     |  \n",
      "     |  max_features : int or float, optional (default=1.0)\n",
      "     |      The number of features to draw from X to train each base estimator.\n",
      "     |  \n",
      "     |      - If int, then draw `max_features` features.\n",
      "     |      - If float, then draw `max_features * X.shape[1]` features.\n",
      "     |  \n",
      "     |  bootstrap : boolean, optional (default=True)\n",
      "     |      Whether samples are drawn with replacement. If False, sampling\n",
      "     |      without replacement is performed.\n",
      "     |  \n",
      "     |  bootstrap_features : boolean, optional (default=False)\n",
      "     |      Whether features are drawn with replacement.\n",
      "     |  \n",
      "     |  oob_score : bool, optional (default=False)\n",
      "     |      Whether to use out-of-bag samples to estimate\n",
      "     |      the generalization error.\n",
      "     |  \n",
      "     |  warm_start : bool, optional (default=False)\n",
      "     |      When set to True, reuse the solution of the previous call to fit\n",
      "     |      and add more estimators to the ensemble, otherwise, just fit\n",
      "     |      a whole new ensemble. See :term:`the Glossary <warm_start>`.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.17\n",
      "     |         *warm_start* constructor parameter.\n",
      "     |  \n",
      "     |  n_jobs : int or None, optional (default=None)\n",
      "     |      The number of jobs to run in parallel for both `fit` and `predict`.\n",
      "     |      ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n",
      "     |      ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n",
      "     |      for more details.\n",
      "     |  \n",
      "     |  random_state : int, RandomState instance or None, optional (default=None)\n",
      "     |      If int, random_state is the seed used by the random number generator;\n",
      "     |      If RandomState instance, random_state is the random number generator;\n",
      "     |      If None, the random number generator is the RandomState instance used\n",
      "     |      by `np.random`.\n",
      "     |  \n",
      "     |  verbose : int, optional (default=0)\n",
      "     |      Controls the verbosity when fitting and predicting.\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  base_estimator_ : estimator\n",
      "     |      The base estimator from which the ensemble is grown.\n",
      "     |  \n",
      "     |  estimators_ : list of estimators\n",
      "     |      The collection of fitted base estimators.\n",
      "     |  \n",
      "     |  estimators_samples_ : list of arrays\n",
      "     |      The subset of drawn samples (i.e., the in-bag samples) for each base\n",
      "     |      estimator. Each subset is defined by an array of the indices selected.\n",
      "     |  \n",
      "     |  estimators_features_ : list of arrays\n",
      "     |      The subset of drawn features for each base estimator.\n",
      "     |  \n",
      "     |  classes_ : array of shape = [n_classes]\n",
      "     |      The classes labels.\n",
      "     |  \n",
      "     |  n_classes_ : int or list\n",
      "     |      The number of classes.\n",
      "     |  \n",
      "     |  oob_score_ : float\n",
      "     |      Score of the training dataset obtained using an out-of-bag estimate.\n",
      "     |  \n",
      "     |  oob_decision_function_ : array of shape = [n_samples, n_classes]\n",
      "     |      Decision function computed with out-of-bag estimate on the training\n",
      "     |      set. If n_estimators is small it might be possible that a data point\n",
      "     |      was never left out during the bootstrap. In this case,\n",
      "     |      `oob_decision_function_` might contain NaN.\n",
      "     |  \n",
      "     |  References\n",
      "     |  ----------\n",
      "     |  \n",
      "     |  .. [1] L. Breiman, \"Pasting small votes for classification in large\n",
      "     |         databases and on-line\", Machine Learning, 36(1), 85-103, 1999.\n",
      "     |  \n",
      "     |  .. [2] L. Breiman, \"Bagging predictors\", Machine Learning, 24(2), 123-140,\n",
      "     |         1996.\n",
      "     |  \n",
      "     |  .. [3] T. Ho, \"The random subspace method for constructing decision\n",
      "     |         forests\", Pattern Analysis and Machine Intelligence, 20(8), 832-844,\n",
      "     |         1998.\n",
      "     |  \n",
      "     |  .. [4] G. Louppe and P. Geurts, \"Ensembles on Random Patches\", Machine\n",
      "     |         Learning and Knowledge Discovery in Databases, 346-361, 2012.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      BaggingClassifier\n",
      "     |      BaseBagging\n",
      "     |      sklearn.ensemble.base.BaseEnsemble\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      sklearn.base.MetaEstimatorMixin\n",
      "     |      sklearn.base.ClassifierMixin\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, base_estimator=None, n_estimators=10, max_samples=1.0, max_features=1.0, bootstrap=True, bootstrap_features=False, oob_score=False, warm_start=False, n_jobs=None, random_state=None, verbose=0)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  decision_function(self, X)\n",
      "     |      Average of the decision functions of the base classifiers.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape = [n_samples, n_features]\n",
      "     |          The training input samples. Sparse matrices are accepted only if\n",
      "     |          they are supported by the base estimator.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      score : array, shape = [n_samples, k]\n",
      "     |          The decision function of the input samples. The columns correspond\n",
      "     |          to the classes in sorted order, as they appear in the attribute\n",
      "     |          ``classes_``. Regression and binary classification are special\n",
      "     |          cases with ``k == 1``, otherwise ``k==n_classes``.\n",
      "     |  \n",
      "     |  predict(self, X)\n",
      "     |      Predict class for X.\n",
      "     |      \n",
      "     |      The predicted class of an input sample is computed as the class with\n",
      "     |      the highest mean predicted probability. If base estimators do not\n",
      "     |      implement a ``predict_proba`` method, then it resorts to voting.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape = [n_samples, n_features]\n",
      "     |          The training input samples. Sparse matrices are accepted only if\n",
      "     |          they are supported by the base estimator.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      y : array of shape = [n_samples]\n",
      "     |          The predicted classes.\n",
      "     |  \n",
      "     |  predict_log_proba(self, X)\n",
      "     |      Predict class log-probabilities for X.\n",
      "     |      \n",
      "     |      The predicted class log-probabilities of an input sample is computed as\n",
      "     |      the log of the mean predicted class probabilities of the base\n",
      "     |      estimators in the ensemble.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape = [n_samples, n_features]\n",
      "     |          The training input samples. Sparse matrices are accepted only if\n",
      "     |          they are supported by the base estimator.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      p : array of shape = [n_samples, n_classes]\n",
      "     |          The class log-probabilities of the input samples. The order of the\n",
      "     |          classes corresponds to that in the attribute `classes_`.\n",
      "     |  \n",
      "     |  predict_proba(self, X)\n",
      "     |      Predict class probabilities for X.\n",
      "     |      \n",
      "     |      The predicted class probabilities of an input sample is computed as\n",
      "     |      the mean predicted class probabilities of the base estimators in the\n",
      "     |      ensemble. If base estimators do not implement a ``predict_proba``\n",
      "     |      method, then it resorts to voting and the predicted class probabilities\n",
      "     |      of an input sample represents the proportion of estimators predicting\n",
      "     |      each class.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape = [n_samples, n_features]\n",
      "     |          The training input samples. Sparse matrices are accepted only if\n",
      "     |          they are supported by the base estimator.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      p : array of shape = [n_samples, n_classes]\n",
      "     |          The class probabilities of the input samples. The order of the\n",
      "     |          classes corresponds to that in the attribute `classes_`.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from BaseBagging:\n",
      "     |  \n",
      "     |  fit(self, X, y, sample_weight=None)\n",
      "     |      Build a Bagging ensemble of estimators from the training\n",
      "     |         set (X, y).\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape = [n_samples, n_features]\n",
      "     |          The training input samples. Sparse matrices are accepted only if\n",
      "     |          they are supported by the base estimator.\n",
      "     |      \n",
      "     |      y : array-like, shape = [n_samples]\n",
      "     |          The target values (class labels in classification, real numbers in\n",
      "     |          regression).\n",
      "     |      \n",
      "     |      sample_weight : array-like, shape = [n_samples] or None\n",
      "     |          Sample weights. If None, then samples are equally weighted.\n",
      "     |          Note that this is supported only if the base estimator supports\n",
      "     |          sample weighting.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from BaseBagging:\n",
      "     |  \n",
      "     |  estimators_samples_\n",
      "     |      The subset of drawn samples for each base estimator.\n",
      "     |      \n",
      "     |      Returns a dynamically generated list of indices identifying\n",
      "     |      the samples used for fitting each member of the ensemble, i.e.,\n",
      "     |      the in-bag samples.\n",
      "     |      \n",
      "     |      Note: the list is re-created at each call to the property in order\n",
      "     |      to reduce the object memory footprint by not storing the sampling\n",
      "     |      data. Thus fetching the property may be slower than expected.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.ensemble.base.BaseEnsemble:\n",
      "     |  \n",
      "     |  __getitem__(self, index)\n",
      "     |      Returns the index'th estimator in the ensemble.\n",
      "     |  \n",
      "     |  __iter__(self)\n",
      "     |      Returns iterator over estimators in the ensemble.\n",
      "     |  \n",
      "     |  __len__(self)\n",
      "     |      Returns the number of estimators in the ensemble.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __repr__(self, N_CHAR_MAX=700)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : boolean, optional\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : mapping of string to any\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as pipelines). The latter have parameters of the form\n",
      "     |      ``<component>__<parameter>`` so that it's possible to update each\n",
      "     |      component of a nested object.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.ClassifierMixin:\n",
      "     |  \n",
      "     |  score(self, X, y, sample_weight=None)\n",
      "     |      Returns the mean accuracy on the given test data and labels.\n",
      "     |      \n",
      "     |      In multi-label classification, this is the subset accuracy\n",
      "     |      which is a harsh metric since you require for each sample that\n",
      "     |      each label set be correctly predicted.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like, shape = (n_samples, n_features)\n",
      "     |          Test samples.\n",
      "     |      \n",
      "     |      y : array-like, shape = (n_samples) or (n_samples, n_outputs)\n",
      "     |          True labels for X.\n",
      "     |      \n",
      "     |      sample_weight : array-like, shape = [n_samples], optional\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      score : float\n",
      "     |          Mean accuracy of self.predict(X) wrt. y.\n",
      "    \n",
      "    class BaggingRegressor(BaseBagging, sklearn.base.RegressorMixin)\n",
      "     |  BaggingRegressor(base_estimator=None, n_estimators=10, max_samples=1.0, max_features=1.0, bootstrap=True, bootstrap_features=False, oob_score=False, warm_start=False, n_jobs=None, random_state=None, verbose=0)\n",
      "     |  \n",
      "     |  A Bagging regressor.\n",
      "     |  \n",
      "     |  A Bagging regressor is an ensemble meta-estimator that fits base\n",
      "     |  regressors each on random subsets of the original dataset and then\n",
      "     |  aggregate their individual predictions (either by voting or by averaging)\n",
      "     |  to form a final prediction. Such a meta-estimator can typically be used as\n",
      "     |  a way to reduce the variance of a black-box estimator (e.g., a decision\n",
      "     |  tree), by introducing randomization into its construction procedure and\n",
      "     |  then making an ensemble out of it.\n",
      "     |  \n",
      "     |  This algorithm encompasses several works from the literature. When random\n",
      "     |  subsets of the dataset are drawn as random subsets of the samples, then\n",
      "     |  this algorithm is known as Pasting [1]_. If samples are drawn with\n",
      "     |  replacement, then the method is known as Bagging [2]_. When random subsets\n",
      "     |  of the dataset are drawn as random subsets of the features, then the method\n",
      "     |  is known as Random Subspaces [3]_. Finally, when base estimators are built\n",
      "     |  on subsets of both samples and features, then the method is known as\n",
      "     |  Random Patches [4]_.\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <bagging>`.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  base_estimator : object or None, optional (default=None)\n",
      "     |      The base estimator to fit on random subsets of the dataset.\n",
      "     |      If None, then the base estimator is a decision tree.\n",
      "     |  \n",
      "     |  n_estimators : int, optional (default=10)\n",
      "     |      The number of base estimators in the ensemble.\n",
      "     |  \n",
      "     |  max_samples : int or float, optional (default=1.0)\n",
      "     |      The number of samples to draw from X to train each base estimator.\n",
      "     |  \n",
      "     |      - If int, then draw `max_samples` samples.\n",
      "     |      - If float, then draw `max_samples * X.shape[0]` samples.\n",
      "     |  \n",
      "     |  max_features : int or float, optional (default=1.0)\n",
      "     |      The number of features to draw from X to train each base estimator.\n",
      "     |  \n",
      "     |      - If int, then draw `max_features` features.\n",
      "     |      - If float, then draw `max_features * X.shape[1]` features.\n",
      "     |  \n",
      "     |  bootstrap : boolean, optional (default=True)\n",
      "     |      Whether samples are drawn with replacement. If False, sampling\n",
      "     |      without replacement is performed.\n",
      "     |  \n",
      "     |  bootstrap_features : boolean, optional (default=False)\n",
      "     |      Whether features are drawn with replacement.\n",
      "     |  \n",
      "     |  oob_score : bool\n",
      "     |      Whether to use out-of-bag samples to estimate\n",
      "     |      the generalization error.\n",
      "     |  \n",
      "     |  warm_start : bool, optional (default=False)\n",
      "     |      When set to True, reuse the solution of the previous call to fit\n",
      "     |      and add more estimators to the ensemble, otherwise, just fit\n",
      "     |      a whole new ensemble. See :term:`the Glossary <warm_start>`.\n",
      "     |  \n",
      "     |  n_jobs : int or None, optional (default=None)\n",
      "     |      The number of jobs to run in parallel for both `fit` and `predict`.\n",
      "     |      ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n",
      "     |      ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n",
      "     |      for more details.\n",
      "     |  \n",
      "     |  random_state : int, RandomState instance or None, optional (default=None)\n",
      "     |      If int, random_state is the seed used by the random number generator;\n",
      "     |      If RandomState instance, random_state is the random number generator;\n",
      "     |      If None, the random number generator is the RandomState instance used\n",
      "     |      by `np.random`.\n",
      "     |  \n",
      "     |  verbose : int, optional (default=0)\n",
      "     |      Controls the verbosity when fitting and predicting.\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  estimators_ : list of estimators\n",
      "     |      The collection of fitted sub-estimators.\n",
      "     |  \n",
      "     |  estimators_samples_ : list of arrays\n",
      "     |      The subset of drawn samples (i.e., the in-bag samples) for each base\n",
      "     |      estimator. Each subset is defined by an array of the indices selected.\n",
      "     |  \n",
      "     |  estimators_features_ : list of arrays\n",
      "     |      The subset of drawn features for each base estimator.\n",
      "     |  \n",
      "     |  oob_score_ : float\n",
      "     |      Score of the training dataset obtained using an out-of-bag estimate.\n",
      "     |  \n",
      "     |  oob_prediction_ : array of shape = [n_samples]\n",
      "     |      Prediction computed with out-of-bag estimate on the training\n",
      "     |      set. If n_estimators is small it might be possible that a data point\n",
      "     |      was never left out during the bootstrap. In this case,\n",
      "     |      `oob_prediction_` might contain NaN.\n",
      "     |  \n",
      "     |  References\n",
      "     |  ----------\n",
      "     |  \n",
      "     |  .. [1] L. Breiman, \"Pasting small votes for classification in large\n",
      "     |         databases and on-line\", Machine Learning, 36(1), 85-103, 1999.\n",
      "     |  \n",
      "     |  .. [2] L. Breiman, \"Bagging predictors\", Machine Learning, 24(2), 123-140,\n",
      "     |         1996.\n",
      "     |  \n",
      "     |  .. [3] T. Ho, \"The random subspace method for constructing decision\n",
      "     |         forests\", Pattern Analysis and Machine Intelligence, 20(8), 832-844,\n",
      "     |         1998.\n",
      "     |  \n",
      "     |  .. [4] G. Louppe and P. Geurts, \"Ensembles on Random Patches\", Machine\n",
      "     |         Learning and Knowledge Discovery in Databases, 346-361, 2012.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      BaggingRegressor\n",
      "     |      BaseBagging\n",
      "     |      sklearn.ensemble.base.BaseEnsemble\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      sklearn.base.MetaEstimatorMixin\n",
      "     |      sklearn.base.RegressorMixin\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, base_estimator=None, n_estimators=10, max_samples=1.0, max_features=1.0, bootstrap=True, bootstrap_features=False, oob_score=False, warm_start=False, n_jobs=None, random_state=None, verbose=0)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  predict(self, X)\n",
      "     |      Predict regression target for X.\n",
      "     |      \n",
      "     |      The predicted regression target of an input sample is computed as the\n",
      "     |      mean predicted regression targets of the estimators in the ensemble.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape = [n_samples, n_features]\n",
      "     |          The training input samples. Sparse matrices are accepted only if\n",
      "     |          they are supported by the base estimator.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      y : array of shape = [n_samples]\n",
      "     |          The predicted values.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from BaseBagging:\n",
      "     |  \n",
      "     |  fit(self, X, y, sample_weight=None)\n",
      "     |      Build a Bagging ensemble of estimators from the training\n",
      "     |         set (X, y).\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape = [n_samples, n_features]\n",
      "     |          The training input samples. Sparse matrices are accepted only if\n",
      "     |          they are supported by the base estimator.\n",
      "     |      \n",
      "     |      y : array-like, shape = [n_samples]\n",
      "     |          The target values (class labels in classification, real numbers in\n",
      "     |          regression).\n",
      "     |      \n",
      "     |      sample_weight : array-like, shape = [n_samples] or None\n",
      "     |          Sample weights. If None, then samples are equally weighted.\n",
      "     |          Note that this is supported only if the base estimator supports\n",
      "     |          sample weighting.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from BaseBagging:\n",
      "     |  \n",
      "     |  estimators_samples_\n",
      "     |      The subset of drawn samples for each base estimator.\n",
      "     |      \n",
      "     |      Returns a dynamically generated list of indices identifying\n",
      "     |      the samples used for fitting each member of the ensemble, i.e.,\n",
      "     |      the in-bag samples.\n",
      "     |      \n",
      "     |      Note: the list is re-created at each call to the property in order\n",
      "     |      to reduce the object memory footprint by not storing the sampling\n",
      "     |      data. Thus fetching the property may be slower than expected.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.ensemble.base.BaseEnsemble:\n",
      "     |  \n",
      "     |  __getitem__(self, index)\n",
      "     |      Returns the index'th estimator in the ensemble.\n",
      "     |  \n",
      "     |  __iter__(self)\n",
      "     |      Returns iterator over estimators in the ensemble.\n",
      "     |  \n",
      "     |  __len__(self)\n",
      "     |      Returns the number of estimators in the ensemble.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __repr__(self, N_CHAR_MAX=700)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : boolean, optional\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : mapping of string to any\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as pipelines). The latter have parameters of the form\n",
      "     |      ``<component>__<parameter>`` so that it's possible to update each\n",
      "     |      component of a nested object.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.RegressorMixin:\n",
      "     |  \n",
      "     |  score(self, X, y, sample_weight=None)\n",
      "     |      Returns the coefficient of determination R^2 of the prediction.\n",
      "     |      \n",
      "     |      The coefficient R^2 is defined as (1 - u/v), where u is the residual\n",
      "     |      sum of squares ((y_true - y_pred) ** 2).sum() and v is the total\n",
      "     |      sum of squares ((y_true - y_true.mean()) ** 2).sum().\n",
      "     |      The best possible score is 1.0 and it can be negative (because the\n",
      "     |      model can be arbitrarily worse). A constant model that always\n",
      "     |      predicts the expected value of y, disregarding the input features,\n",
      "     |      would get a R^2 score of 0.0.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like, shape = (n_samples, n_features)\n",
      "     |          Test samples. For some estimators this may be a\n",
      "     |          precomputed kernel matrix instead, shape = (n_samples,\n",
      "     |          n_samples_fitted], where n_samples_fitted is the number of\n",
      "     |          samples used in the fitting for the estimator.\n",
      "     |      \n",
      "     |      y : array-like, shape = (n_samples) or (n_samples, n_outputs)\n",
      "     |          True values for X.\n",
      "     |      \n",
      "     |      sample_weight : array-like, shape = [n_samples], optional\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      score : float\n",
      "     |          R^2 of self.predict(X) wrt. y.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      The R2 score used when calling ``score`` on a regressor will use\n",
      "     |      ``multioutput='uniform_average'`` from version 0.23 to keep consistent\n",
      "     |      with `metrics.r2_score`. This will influence the ``score`` method of\n",
      "     |      all the multioutput regressors (except for\n",
      "     |      `multioutput.MultiOutputRegressor`). To specify the default value\n",
      "     |      manually and avoid the warning, please either call `metrics.r2_score`\n",
      "     |      directly or make a custom scorer with `metrics.make_scorer` (the\n",
      "     |      built-in scorer ``'r2'`` uses ``multioutput='uniform_average'``).\n",
      "    \n",
      "    class BaseEnsemble(sklearn.base.BaseEstimator, sklearn.base.MetaEstimatorMixin)\n",
      "     |  BaseEnsemble(base_estimator, n_estimators=10, estimator_params=())\n",
      "     |  \n",
      "     |  Base class for all ensemble classes.\n",
      "     |  \n",
      "     |  Warning: This class should not be used directly. Use derived classes\n",
      "     |  instead.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  base_estimator : object, optional (default=None)\n",
      "     |      The base estimator from which the ensemble is built.\n",
      "     |  \n",
      "     |  n_estimators : integer\n",
      "     |      The number of estimators in the ensemble.\n",
      "     |  \n",
      "     |  estimator_params : list of strings\n",
      "     |      The list of attributes to use as parameters when instantiating a\n",
      "     |      new base estimator. If none are given, default parameters are used.\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  base_estimator_ : estimator\n",
      "     |      The base estimator from which the ensemble is grown.\n",
      "     |  \n",
      "     |  estimators_ : list of estimators\n",
      "     |      The collection of fitted base estimators.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      BaseEnsemble\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      sklearn.base.MetaEstimatorMixin\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __getitem__(self, index)\n",
      "     |      Returns the index'th estimator in the ensemble.\n",
      "     |  \n",
      "     |  __init__(self, base_estimator, n_estimators=10, estimator_params=())\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  __iter__(self)\n",
      "     |      Returns iterator over estimators in the ensemble.\n",
      "     |  \n",
      "     |  __len__(self)\n",
      "     |      Returns the number of estimators in the ensemble.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset({'__init__'})\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __repr__(self, N_CHAR_MAX=700)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : boolean, optional\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : mapping of string to any\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as pipelines). The latter have parameters of the form\n",
      "     |      ``<component>__<parameter>`` so that it's possible to update each\n",
      "     |      component of a nested object.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class ExtraTreesClassifier(ForestClassifier)\n",
      "     |  ExtraTreesClassifier(n_estimators='warn', criterion='gini', max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features='auto', max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, bootstrap=False, oob_score=False, n_jobs=None, random_state=None, verbose=0, warm_start=False, class_weight=None)\n",
      "     |  \n",
      "     |  An extra-trees classifier.\n",
      "     |  \n",
      "     |  This class implements a meta estimator that fits a number of\n",
      "     |  randomized decision trees (a.k.a. extra-trees) on various sub-samples\n",
      "     |  of the dataset and uses averaging to improve the predictive accuracy\n",
      "     |  and control over-fitting.\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <forest>`.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  n_estimators : integer, optional (default=10)\n",
      "     |      The number of trees in the forest.\n",
      "     |  \n",
      "     |      .. versionchanged:: 0.20\n",
      "     |         The default value of ``n_estimators`` will change from 10 in\n",
      "     |         version 0.20 to 100 in version 0.22.\n",
      "     |  \n",
      "     |  criterion : string, optional (default=\"gini\")\n",
      "     |      The function to measure the quality of a split. Supported criteria are\n",
      "     |      \"gini\" for the Gini impurity and \"entropy\" for the information gain.\n",
      "     |  \n",
      "     |  max_depth : integer or None, optional (default=None)\n",
      "     |      The maximum depth of the tree. If None, then nodes are expanded until\n",
      "     |      all leaves are pure or until all leaves contain less than\n",
      "     |      min_samples_split samples.\n",
      "     |  \n",
      "     |  min_samples_split : int, float, optional (default=2)\n",
      "     |      The minimum number of samples required to split an internal node:\n",
      "     |  \n",
      "     |      - If int, then consider `min_samples_split` as the minimum number.\n",
      "     |      - If float, then `min_samples_split` is a fraction and\n",
      "     |        `ceil(min_samples_split * n_samples)` are the minimum\n",
      "     |        number of samples for each split.\n",
      "     |  \n",
      "     |      .. versionchanged:: 0.18\n",
      "     |         Added float values for fractions.\n",
      "     |  \n",
      "     |  min_samples_leaf : int, float, optional (default=1)\n",
      "     |      The minimum number of samples required to be at a leaf node.\n",
      "     |      A split point at any depth will only be considered if it leaves at\n",
      "     |      least ``min_samples_leaf`` training samples in each of the left and\n",
      "     |      right branches.  This may have the effect of smoothing the model,\n",
      "     |      especially in regression.\n",
      "     |  \n",
      "     |      - If int, then consider `min_samples_leaf` as the minimum number.\n",
      "     |      - If float, then `min_samples_leaf` is a fraction and\n",
      "     |        `ceil(min_samples_leaf * n_samples)` are the minimum\n",
      "     |        number of samples for each node.\n",
      "     |  \n",
      "     |      .. versionchanged:: 0.18\n",
      "     |         Added float values for fractions.\n",
      "     |  \n",
      "     |  min_weight_fraction_leaf : float, optional (default=0.)\n",
      "     |      The minimum weighted fraction of the sum total of weights (of all\n",
      "     |      the input samples) required to be at a leaf node. Samples have\n",
      "     |      equal weight when sample_weight is not provided.\n",
      "     |  \n",
      "     |  max_features : int, float, string or None, optional (default=\"auto\")\n",
      "     |      The number of features to consider when looking for the best split:\n",
      "     |  \n",
      "     |      - If int, then consider `max_features` features at each split.\n",
      "     |      - If float, then `max_features` is a fraction and\n",
      "     |        `int(max_features * n_features)` features are considered at each\n",
      "     |        split.\n",
      "     |      - If \"auto\", then `max_features=sqrt(n_features)`.\n",
      "     |      - If \"sqrt\", then `max_features=sqrt(n_features)`.\n",
      "     |      - If \"log2\", then `max_features=log2(n_features)`.\n",
      "     |      - If None, then `max_features=n_features`.\n",
      "     |  \n",
      "     |      Note: the search for a split does not stop until at least one\n",
      "     |      valid partition of the node samples is found, even if it requires to\n",
      "     |      effectively inspect more than ``max_features`` features.\n",
      "     |  \n",
      "     |  max_leaf_nodes : int or None, optional (default=None)\n",
      "     |      Grow trees with ``max_leaf_nodes`` in best-first fashion.\n",
      "     |      Best nodes are defined as relative reduction in impurity.\n",
      "     |      If None then unlimited number of leaf nodes.\n",
      "     |  \n",
      "     |  min_impurity_decrease : float, optional (default=0.)\n",
      "     |      A node will be split if this split induces a decrease of the impurity\n",
      "     |      greater than or equal to this value.\n",
      "     |  \n",
      "     |      The weighted impurity decrease equation is the following::\n",
      "     |  \n",
      "     |          N_t / N * (impurity - N_t_R / N_t * right_impurity\n",
      "     |                              - N_t_L / N_t * left_impurity)\n",
      "     |  \n",
      "     |      where ``N`` is the total number of samples, ``N_t`` is the number of\n",
      "     |      samples at the current node, ``N_t_L`` is the number of samples in the\n",
      "     |      left child, and ``N_t_R`` is the number of samples in the right child.\n",
      "     |  \n",
      "     |      ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,\n",
      "     |      if ``sample_weight`` is passed.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.19\n",
      "     |  \n",
      "     |  min_impurity_split : float, (default=1e-7)\n",
      "     |      Threshold for early stopping in tree growth. A node will split\n",
      "     |      if its impurity is above the threshold, otherwise it is a leaf.\n",
      "     |  \n",
      "     |      .. deprecated:: 0.19\n",
      "     |         ``min_impurity_split`` has been deprecated in favor of\n",
      "     |         ``min_impurity_decrease`` in 0.19. The default value of\n",
      "     |         ``min_impurity_split`` will change from 1e-7 to 0 in 0.23 and it\n",
      "     |         will be removed in 0.25. Use ``min_impurity_decrease`` instead.\n",
      "     |  \n",
      "     |  bootstrap : boolean, optional (default=False)\n",
      "     |      Whether bootstrap samples are used when building trees. If False, the\n",
      "     |      whole datset is used to build each tree.\n",
      "     |  \n",
      "     |  oob_score : bool, optional (default=False)\n",
      "     |      Whether to use out-of-bag samples to estimate\n",
      "     |      the generalization accuracy.\n",
      "     |  \n",
      "     |  n_jobs : int or None, optional (default=None)\n",
      "     |      The number of jobs to run in parallel for both `fit` and `predict`.\n",
      "     |      ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n",
      "     |      ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n",
      "     |      for more details.\n",
      "     |  \n",
      "     |  random_state : int, RandomState instance or None, optional (default=None)\n",
      "     |      If int, random_state is the seed used by the random number generator;\n",
      "     |      If RandomState instance, random_state is the random number generator;\n",
      "     |      If None, the random number generator is the RandomState instance used\n",
      "     |      by `np.random`.\n",
      "     |  \n",
      "     |  verbose : int, optional (default=0)\n",
      "     |      Controls the verbosity when fitting and predicting.\n",
      "     |  \n",
      "     |  warm_start : bool, optional (default=False)\n",
      "     |      When set to ``True``, reuse the solution of the previous call to fit\n",
      "     |      and add more estimators to the ensemble, otherwise, just fit a whole\n",
      "     |      new forest. See :term:`the Glossary <warm_start>`.\n",
      "     |  \n",
      "     |  class_weight : dict, list of dicts, \"balanced\", \"balanced_subsample\" or     None, optional (default=None)\n",
      "     |      Weights associated with classes in the form ``{class_label: weight}``.\n",
      "     |      If not given, all classes are supposed to have weight one. For\n",
      "     |      multi-output problems, a list of dicts can be provided in the same\n",
      "     |      order as the columns of y.\n",
      "     |  \n",
      "     |      Note that for multioutput (including multilabel) weights should be\n",
      "     |      defined for each class of every column in its own dict. For example,\n",
      "     |      for four-class multilabel classification weights should be\n",
      "     |      [{0: 1, 1: 1}, {0: 1, 1: 5}, {0: 1, 1: 1}, {0: 1, 1: 1}] instead of\n",
      "     |      [{1:1}, {2:5}, {3:1}, {4:1}].\n",
      "     |  \n",
      "     |      The \"balanced\" mode uses the values of y to automatically adjust\n",
      "     |      weights inversely proportional to class frequencies in the input data\n",
      "     |      as ``n_samples / (n_classes * np.bincount(y))``\n",
      "     |  \n",
      "     |      The \"balanced_subsample\" mode is the same as \"balanced\" except that weights are\n",
      "     |      computed based on the bootstrap sample for every tree grown.\n",
      "     |  \n",
      "     |      For multi-output, the weights of each column of y will be multiplied.\n",
      "     |  \n",
      "     |      Note that these weights will be multiplied with sample_weight (passed\n",
      "     |      through the fit method) if sample_weight is specified.\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  estimators_ : list of DecisionTreeClassifier\n",
      "     |      The collection of fitted sub-estimators.\n",
      "     |  \n",
      "     |  classes_ : array of shape = [n_classes] or a list of such arrays\n",
      "     |      The classes labels (single output problem), or a list of arrays of\n",
      "     |      class labels (multi-output problem).\n",
      "     |  \n",
      "     |  n_classes_ : int or list\n",
      "     |      The number of classes (single output problem), or a list containing the\n",
      "     |      number of classes for each output (multi-output problem).\n",
      "     |  \n",
      "     |  feature_importances_ : array of shape = [n_features]\n",
      "     |      The feature importances (the higher, the more important the feature).\n",
      "     |  \n",
      "     |  n_features_ : int\n",
      "     |      The number of features when ``fit`` is performed.\n",
      "     |  \n",
      "     |  n_outputs_ : int\n",
      "     |      The number of outputs when ``fit`` is performed.\n",
      "     |  \n",
      "     |  oob_score_ : float\n",
      "     |      Score of the training dataset obtained using an out-of-bag estimate.\n",
      "     |  \n",
      "     |  oob_decision_function_ : array of shape = [n_samples, n_classes]\n",
      "     |      Decision function computed with out-of-bag estimate on the training\n",
      "     |      set. If n_estimators is small it might be possible that a data point\n",
      "     |      was never left out during the bootstrap. In this case,\n",
      "     |      `oob_decision_function_` might contain NaN.\n",
      "     |  \n",
      "     |  Notes\n",
      "     |  -----\n",
      "     |  The default values for the parameters controlling the size of the trees\n",
      "     |  (e.g. ``max_depth``, ``min_samples_leaf``, etc.) lead to fully grown and\n",
      "     |  unpruned trees which can potentially be very large on some data sets. To\n",
      "     |  reduce memory consumption, the complexity and size of the trees should be\n",
      "     |  controlled by setting those parameter values.\n",
      "     |  \n",
      "     |  References\n",
      "     |  ----------\n",
      "     |  \n",
      "     |  .. [1] P. Geurts, D. Ernst., and L. Wehenkel, \"Extremely randomized\n",
      "     |         trees\", Machine Learning, 63(1), 3-42, 2006.\n",
      "     |  \n",
      "     |  See also\n",
      "     |  --------\n",
      "     |  sklearn.tree.ExtraTreeClassifier : Base classifier for this ensemble.\n",
      "     |  RandomForestClassifier : Ensemble Classifier based on trees with optimal\n",
      "     |      splits.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      ExtraTreesClassifier\n",
      "     |      ForestClassifier\n",
      "     |      BaseForest\n",
      "     |      sklearn.ensemble.base.BaseEnsemble\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      sklearn.base.MetaEstimatorMixin\n",
      "     |      sklearn.base.MultiOutputMixin\n",
      "     |      sklearn.base.ClassifierMixin\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, n_estimators='warn', criterion='gini', max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features='auto', max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, bootstrap=False, oob_score=False, n_jobs=None, random_state=None, verbose=0, warm_start=False, class_weight=None)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from ForestClassifier:\n",
      "     |  \n",
      "     |  predict(self, X)\n",
      "     |      Predict class for X.\n",
      "     |      \n",
      "     |      The predicted class of an input sample is a vote by the trees in\n",
      "     |      the forest, weighted by their probability estimates. That is,\n",
      "     |      the predicted class is the one with highest mean probability\n",
      "     |      estimate across the trees.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like or sparse matrix of shape = [n_samples, n_features]\n",
      "     |          The input samples. Internally, its dtype will be converted to\n",
      "     |          ``dtype=np.float32``. If a sparse matrix is provided, it will be\n",
      "     |          converted into a sparse ``csr_matrix``.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      y : array of shape = [n_samples] or [n_samples, n_outputs]\n",
      "     |          The predicted classes.\n",
      "     |  \n",
      "     |  predict_log_proba(self, X)\n",
      "     |      Predict class log-probabilities for X.\n",
      "     |      \n",
      "     |      The predicted class log-probabilities of an input sample is computed as\n",
      "     |      the log of the mean predicted class probabilities of the trees in the\n",
      "     |      forest.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like or sparse matrix of shape = [n_samples, n_features]\n",
      "     |          The input samples. Internally, its dtype will be converted to\n",
      "     |          ``dtype=np.float32``. If a sparse matrix is provided, it will be\n",
      "     |          converted into a sparse ``csr_matrix``.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      p : array of shape = [n_samples, n_classes], or a list of n_outputs\n",
      "     |          such arrays if n_outputs > 1.\n",
      "     |          The class probabilities of the input samples. The order of the\n",
      "     |          classes corresponds to that in the attribute `classes_`.\n",
      "     |  \n",
      "     |  predict_proba(self, X)\n",
      "     |      Predict class probabilities for X.\n",
      "     |      \n",
      "     |      The predicted class probabilities of an input sample are computed as\n",
      "     |      the mean predicted class probabilities of the trees in the forest. The\n",
      "     |      class probability of a single tree is the fraction of samples of the same\n",
      "     |      class in a leaf.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like or sparse matrix of shape = [n_samples, n_features]\n",
      "     |          The input samples. Internally, its dtype will be converted to\n",
      "     |          ``dtype=np.float32``. If a sparse matrix is provided, it will be\n",
      "     |          converted into a sparse ``csr_matrix``.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      p : array of shape = [n_samples, n_classes], or a list of n_outputs\n",
      "     |          such arrays if n_outputs > 1.\n",
      "     |          The class probabilities of the input samples. The order of the\n",
      "     |          classes corresponds to that in the attribute `classes_`.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from BaseForest:\n",
      "     |  \n",
      "     |  apply(self, X)\n",
      "     |      Apply trees in the forest to X, return leaf indices.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like or sparse matrix, shape = [n_samples, n_features]\n",
      "     |          The input samples. Internally, its dtype will be converted to\n",
      "     |          ``dtype=np.float32``. If a sparse matrix is provided, it will be\n",
      "     |          converted into a sparse ``csr_matrix``.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      X_leaves : array_like, shape = [n_samples, n_estimators]\n",
      "     |          For each datapoint x in X and for each tree in the forest,\n",
      "     |          return the index of the leaf x ends up in.\n",
      "     |  \n",
      "     |  decision_path(self, X)\n",
      "     |      Return the decision path in the forest\n",
      "     |      \n",
      "     |      .. versionadded:: 0.18\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like or sparse matrix, shape = [n_samples, n_features]\n",
      "     |          The input samples. Internally, its dtype will be converted to\n",
      "     |          ``dtype=np.float32``. If a sparse matrix is provided, it will be\n",
      "     |          converted into a sparse ``csr_matrix``.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      indicator : sparse csr array, shape = [n_samples, n_nodes]\n",
      "     |          Return a node indicator matrix where non zero elements\n",
      "     |          indicates that the samples goes through the nodes.\n",
      "     |      \n",
      "     |      n_nodes_ptr : array of size (n_estimators + 1, )\n",
      "     |          The columns from indicator[n_nodes_ptr[i]:n_nodes_ptr[i+1]]\n",
      "     |          gives the indicator value for the i-th estimator.\n",
      "     |  \n",
      "     |  fit(self, X, y, sample_weight=None)\n",
      "     |      Build a forest of trees from the training set (X, y).\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like or sparse matrix of shape = [n_samples, n_features]\n",
      "     |          The training input samples. Internally, its dtype will be converted\n",
      "     |          to ``dtype=np.float32``. If a sparse matrix is provided, it will be\n",
      "     |          converted into a sparse ``csc_matrix``.\n",
      "     |      \n",
      "     |      y : array-like, shape = [n_samples] or [n_samples, n_outputs]\n",
      "     |          The target values (class labels in classification, real numbers in\n",
      "     |          regression).\n",
      "     |      \n",
      "     |      sample_weight : array-like, shape = [n_samples] or None\n",
      "     |          Sample weights. If None, then samples are equally weighted. Splits\n",
      "     |          that would create child nodes with net zero or negative weight are\n",
      "     |          ignored while searching for a split in each node. In the case of\n",
      "     |          classification, splits are also ignored if they would result in any\n",
      "     |          single class carrying a negative weight in either child node.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from BaseForest:\n",
      "     |  \n",
      "     |  feature_importances_\n",
      "     |      Return the feature importances (the higher, the more important the\n",
      "     |         feature).\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      feature_importances_ : array, shape = [n_features]\n",
      "     |          The values of this array sum to 1, unless all trees are single node\n",
      "     |          trees consisting of only the root node, in which case it will be an\n",
      "     |          array of zeros.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.ensemble.base.BaseEnsemble:\n",
      "     |  \n",
      "     |  __getitem__(self, index)\n",
      "     |      Returns the index'th estimator in the ensemble.\n",
      "     |  \n",
      "     |  __iter__(self)\n",
      "     |      Returns iterator over estimators in the ensemble.\n",
      "     |  \n",
      "     |  __len__(self)\n",
      "     |      Returns the number of estimators in the ensemble.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __repr__(self, N_CHAR_MAX=700)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : boolean, optional\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : mapping of string to any\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as pipelines). The latter have parameters of the form\n",
      "     |      ``<component>__<parameter>`` so that it's possible to update each\n",
      "     |      component of a nested object.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.ClassifierMixin:\n",
      "     |  \n",
      "     |  score(self, X, y, sample_weight=None)\n",
      "     |      Returns the mean accuracy on the given test data and labels.\n",
      "     |      \n",
      "     |      In multi-label classification, this is the subset accuracy\n",
      "     |      which is a harsh metric since you require for each sample that\n",
      "     |      each label set be correctly predicted.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like, shape = (n_samples, n_features)\n",
      "     |          Test samples.\n",
      "     |      \n",
      "     |      y : array-like, shape = (n_samples) or (n_samples, n_outputs)\n",
      "     |          True labels for X.\n",
      "     |      \n",
      "     |      sample_weight : array-like, shape = [n_samples], optional\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      score : float\n",
      "     |          Mean accuracy of self.predict(X) wrt. y.\n",
      "    \n",
      "    class ExtraTreesRegressor(ForestRegressor)\n",
      "     |  ExtraTreesRegressor(n_estimators='warn', criterion='mse', max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features='auto', max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, bootstrap=False, oob_score=False, n_jobs=None, random_state=None, verbose=0, warm_start=False)\n",
      "     |  \n",
      "     |  An extra-trees regressor.\n",
      "     |  \n",
      "     |  This class implements a meta estimator that fits a number of\n",
      "     |  randomized decision trees (a.k.a. extra-trees) on various sub-samples\n",
      "     |  of the dataset and uses averaging to improve the predictive accuracy\n",
      "     |  and control over-fitting.\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <forest>`.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  n_estimators : integer, optional (default=10)\n",
      "     |      The number of trees in the forest.\n",
      "     |  \n",
      "     |      .. versionchanged:: 0.20\n",
      "     |         The default value of ``n_estimators`` will change from 10 in\n",
      "     |         version 0.20 to 100 in version 0.22.\n",
      "     |  \n",
      "     |  criterion : string, optional (default=\"mse\")\n",
      "     |      The function to measure the quality of a split. Supported criteria\n",
      "     |      are \"mse\" for the mean squared error, which is equal to variance\n",
      "     |      reduction as feature selection criterion, and \"mae\" for the mean\n",
      "     |      absolute error.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.18\n",
      "     |         Mean Absolute Error (MAE) criterion.\n",
      "     |  \n",
      "     |  max_depth : integer or None, optional (default=None)\n",
      "     |      The maximum depth of the tree. If None, then nodes are expanded until\n",
      "     |      all leaves are pure or until all leaves contain less than\n",
      "     |      min_samples_split samples.\n",
      "     |  \n",
      "     |  min_samples_split : int, float, optional (default=2)\n",
      "     |      The minimum number of samples required to split an internal node:\n",
      "     |  \n",
      "     |      - If int, then consider `min_samples_split` as the minimum number.\n",
      "     |      - If float, then `min_samples_split` is a fraction and\n",
      "     |        `ceil(min_samples_split * n_samples)` are the minimum\n",
      "     |        number of samples for each split.\n",
      "     |  \n",
      "     |      .. versionchanged:: 0.18\n",
      "     |         Added float values for fractions.\n",
      "     |  \n",
      "     |  min_samples_leaf : int, float, optional (default=1)\n",
      "     |      The minimum number of samples required to be at a leaf node.\n",
      "     |      A split point at any depth will only be considered if it leaves at\n",
      "     |      least ``min_samples_leaf`` training samples in each of the left and\n",
      "     |      right branches.  This may have the effect of smoothing the model,\n",
      "     |      especially in regression.\n",
      "     |  \n",
      "     |      - If int, then consider `min_samples_leaf` as the minimum number.\n",
      "     |      - If float, then `min_samples_leaf` is a fraction and\n",
      "     |        `ceil(min_samples_leaf * n_samples)` are the minimum\n",
      "     |        number of samples for each node.\n",
      "     |  \n",
      "     |      .. versionchanged:: 0.18\n",
      "     |         Added float values for fractions.\n",
      "     |  \n",
      "     |  min_weight_fraction_leaf : float, optional (default=0.)\n",
      "     |      The minimum weighted fraction of the sum total of weights (of all\n",
      "     |      the input samples) required to be at a leaf node. Samples have\n",
      "     |      equal weight when sample_weight is not provided.\n",
      "     |  \n",
      "     |  max_features : int, float, string or None, optional (default=\"auto\")\n",
      "     |      The number of features to consider when looking for the best split:\n",
      "     |  \n",
      "     |      - If int, then consider `max_features` features at each split.\n",
      "     |      - If float, then `max_features` is a fraction and\n",
      "     |        `int(max_features * n_features)` features are considered at each\n",
      "     |        split.\n",
      "     |      - If \"auto\", then `max_features=n_features`.\n",
      "     |      - If \"sqrt\", then `max_features=sqrt(n_features)`.\n",
      "     |      - If \"log2\", then `max_features=log2(n_features)`.\n",
      "     |      - If None, then `max_features=n_features`.\n",
      "     |  \n",
      "     |      Note: the search for a split does not stop until at least one\n",
      "     |      valid partition of the node samples is found, even if it requires to\n",
      "     |      effectively inspect more than ``max_features`` features.\n",
      "     |  \n",
      "     |  max_leaf_nodes : int or None, optional (default=None)\n",
      "     |      Grow trees with ``max_leaf_nodes`` in best-first fashion.\n",
      "     |      Best nodes are defined as relative reduction in impurity.\n",
      "     |      If None then unlimited number of leaf nodes.\n",
      "     |  \n",
      "     |  min_impurity_decrease : float, optional (default=0.)\n",
      "     |      A node will be split if this split induces a decrease of the impurity\n",
      "     |      greater than or equal to this value.\n",
      "     |  \n",
      "     |      The weighted impurity decrease equation is the following::\n",
      "     |  \n",
      "     |          N_t / N * (impurity - N_t_R / N_t * right_impurity\n",
      "     |                              - N_t_L / N_t * left_impurity)\n",
      "     |  \n",
      "     |      where ``N`` is the total number of samples, ``N_t`` is the number of\n",
      "     |      samples at the current node, ``N_t_L`` is the number of samples in the\n",
      "     |      left child, and ``N_t_R`` is the number of samples in the right child.\n",
      "     |  \n",
      "     |      ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,\n",
      "     |      if ``sample_weight`` is passed.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.19\n",
      "     |  \n",
      "     |  min_impurity_split : float, (default=1e-7)\n",
      "     |      Threshold for early stopping in tree growth. A node will split\n",
      "     |      if its impurity is above the threshold, otherwise it is a leaf.\n",
      "     |  \n",
      "     |      .. deprecated:: 0.19\n",
      "     |         ``min_impurity_split`` has been deprecated in favor of\n",
      "     |         ``min_impurity_decrease`` in 0.19. The default value of\n",
      "     |         ``min_impurity_split`` will change from 1e-7 to 0 in 0.23 and it\n",
      "     |         will be removed in 0.25. Use ``min_impurity_decrease`` instead.\n",
      "     |  \n",
      "     |  bootstrap : boolean, optional (default=False)\n",
      "     |      Whether bootstrap samples are used when building trees. If False, the\n",
      "     |      whole datset is used to build each tree.\n",
      "     |  \n",
      "     |  oob_score : bool, optional (default=False)\n",
      "     |      Whether to use out-of-bag samples to estimate the R^2 on unseen data.\n",
      "     |  \n",
      "     |  n_jobs : int or None, optional (default=None)\n",
      "     |      The number of jobs to run in parallel for both `fit` and `predict`.\n",
      "     |      ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n",
      "     |      ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n",
      "     |      for more details.\n",
      "     |  \n",
      "     |  random_state : int, RandomState instance or None, optional (default=None)\n",
      "     |      If int, random_state is the seed used by the random number generator;\n",
      "     |      If RandomState instance, random_state is the random number generator;\n",
      "     |      If None, the random number generator is the RandomState instance used\n",
      "     |      by `np.random`.\n",
      "     |  \n",
      "     |  verbose : int, optional (default=0)\n",
      "     |      Controls the verbosity when fitting and predicting.\n",
      "     |  \n",
      "     |  warm_start : bool, optional (default=False)\n",
      "     |      When set to ``True``, reuse the solution of the previous call to fit\n",
      "     |      and add more estimators to the ensemble, otherwise, just fit a whole\n",
      "     |      new forest. See :term:`the Glossary <warm_start>`.\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  estimators_ : list of DecisionTreeRegressor\n",
      "     |      The collection of fitted sub-estimators.\n",
      "     |  \n",
      "     |  feature_importances_ : array of shape = [n_features]\n",
      "     |      The feature importances (the higher, the more important the feature).\n",
      "     |  \n",
      "     |  n_features_ : int\n",
      "     |      The number of features.\n",
      "     |  \n",
      "     |  n_outputs_ : int\n",
      "     |      The number of outputs.\n",
      "     |  \n",
      "     |  oob_score_ : float\n",
      "     |      Score of the training dataset obtained using an out-of-bag estimate.\n",
      "     |  \n",
      "     |  oob_prediction_ : array of shape = [n_samples]\n",
      "     |      Prediction computed with out-of-bag estimate on the training set.\n",
      "     |  \n",
      "     |  Notes\n",
      "     |  -----\n",
      "     |  The default values for the parameters controlling the size of the trees\n",
      "     |  (e.g. ``max_depth``, ``min_samples_leaf``, etc.) lead to fully grown and\n",
      "     |  unpruned trees which can potentially be very large on some data sets. To\n",
      "     |  reduce memory consumption, the complexity and size of the trees should be\n",
      "     |  controlled by setting those parameter values.\n",
      "     |  \n",
      "     |  References\n",
      "     |  ----------\n",
      "     |  \n",
      "     |  .. [1] P. Geurts, D. Ernst., and L. Wehenkel, \"Extremely randomized trees\",\n",
      "     |         Machine Learning, 63(1), 3-42, 2006.\n",
      "     |  \n",
      "     |  See also\n",
      "     |  --------\n",
      "     |  sklearn.tree.ExtraTreeRegressor: Base estimator for this ensemble.\n",
      "     |  RandomForestRegressor: Ensemble regressor using trees with optimal splits.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      ExtraTreesRegressor\n",
      "     |      ForestRegressor\n",
      "     |      BaseForest\n",
      "     |      sklearn.ensemble.base.BaseEnsemble\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      sklearn.base.MetaEstimatorMixin\n",
      "     |      sklearn.base.MultiOutputMixin\n",
      "     |      sklearn.base.RegressorMixin\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, n_estimators='warn', criterion='mse', max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features='auto', max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, bootstrap=False, oob_score=False, n_jobs=None, random_state=None, verbose=0, warm_start=False)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from ForestRegressor:\n",
      "     |  \n",
      "     |  predict(self, X)\n",
      "     |      Predict regression target for X.\n",
      "     |      \n",
      "     |      The predicted regression target of an input sample is computed as the\n",
      "     |      mean predicted regression targets of the trees in the forest.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like or sparse matrix of shape = [n_samples, n_features]\n",
      "     |          The input samples. Internally, its dtype will be converted to\n",
      "     |          ``dtype=np.float32``. If a sparse matrix is provided, it will be\n",
      "     |          converted into a sparse ``csr_matrix``.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      y : array of shape = [n_samples] or [n_samples, n_outputs]\n",
      "     |          The predicted values.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from BaseForest:\n",
      "     |  \n",
      "     |  apply(self, X)\n",
      "     |      Apply trees in the forest to X, return leaf indices.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like or sparse matrix, shape = [n_samples, n_features]\n",
      "     |          The input samples. Internally, its dtype will be converted to\n",
      "     |          ``dtype=np.float32``. If a sparse matrix is provided, it will be\n",
      "     |          converted into a sparse ``csr_matrix``.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      X_leaves : array_like, shape = [n_samples, n_estimators]\n",
      "     |          For each datapoint x in X and for each tree in the forest,\n",
      "     |          return the index of the leaf x ends up in.\n",
      "     |  \n",
      "     |  decision_path(self, X)\n",
      "     |      Return the decision path in the forest\n",
      "     |      \n",
      "     |      .. versionadded:: 0.18\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like or sparse matrix, shape = [n_samples, n_features]\n",
      "     |          The input samples. Internally, its dtype will be converted to\n",
      "     |          ``dtype=np.float32``. If a sparse matrix is provided, it will be\n",
      "     |          converted into a sparse ``csr_matrix``.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      indicator : sparse csr array, shape = [n_samples, n_nodes]\n",
      "     |          Return a node indicator matrix where non zero elements\n",
      "     |          indicates that the samples goes through the nodes.\n",
      "     |      \n",
      "     |      n_nodes_ptr : array of size (n_estimators + 1, )\n",
      "     |          The columns from indicator[n_nodes_ptr[i]:n_nodes_ptr[i+1]]\n",
      "     |          gives the indicator value for the i-th estimator.\n",
      "     |  \n",
      "     |  fit(self, X, y, sample_weight=None)\n",
      "     |      Build a forest of trees from the training set (X, y).\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like or sparse matrix of shape = [n_samples, n_features]\n",
      "     |          The training input samples. Internally, its dtype will be converted\n",
      "     |          to ``dtype=np.float32``. If a sparse matrix is provided, it will be\n",
      "     |          converted into a sparse ``csc_matrix``.\n",
      "     |      \n",
      "     |      y : array-like, shape = [n_samples] or [n_samples, n_outputs]\n",
      "     |          The target values (class labels in classification, real numbers in\n",
      "     |          regression).\n",
      "     |      \n",
      "     |      sample_weight : array-like, shape = [n_samples] or None\n",
      "     |          Sample weights. If None, then samples are equally weighted. Splits\n",
      "     |          that would create child nodes with net zero or negative weight are\n",
      "     |          ignored while searching for a split in each node. In the case of\n",
      "     |          classification, splits are also ignored if they would result in any\n",
      "     |          single class carrying a negative weight in either child node.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from BaseForest:\n",
      "     |  \n",
      "     |  feature_importances_\n",
      "     |      Return the feature importances (the higher, the more important the\n",
      "     |         feature).\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      feature_importances_ : array, shape = [n_features]\n",
      "     |          The values of this array sum to 1, unless all trees are single node\n",
      "     |          trees consisting of only the root node, in which case it will be an\n",
      "     |          array of zeros.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.ensemble.base.BaseEnsemble:\n",
      "     |  \n",
      "     |  __getitem__(self, index)\n",
      "     |      Returns the index'th estimator in the ensemble.\n",
      "     |  \n",
      "     |  __iter__(self)\n",
      "     |      Returns iterator over estimators in the ensemble.\n",
      "     |  \n",
      "     |  __len__(self)\n",
      "     |      Returns the number of estimators in the ensemble.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __repr__(self, N_CHAR_MAX=700)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : boolean, optional\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : mapping of string to any\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as pipelines). The latter have parameters of the form\n",
      "     |      ``<component>__<parameter>`` so that it's possible to update each\n",
      "     |      component of a nested object.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.RegressorMixin:\n",
      "     |  \n",
      "     |  score(self, X, y, sample_weight=None)\n",
      "     |      Returns the coefficient of determination R^2 of the prediction.\n",
      "     |      \n",
      "     |      The coefficient R^2 is defined as (1 - u/v), where u is the residual\n",
      "     |      sum of squares ((y_true - y_pred) ** 2).sum() and v is the total\n",
      "     |      sum of squares ((y_true - y_true.mean()) ** 2).sum().\n",
      "     |      The best possible score is 1.0 and it can be negative (because the\n",
      "     |      model can be arbitrarily worse). A constant model that always\n",
      "     |      predicts the expected value of y, disregarding the input features,\n",
      "     |      would get a R^2 score of 0.0.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like, shape = (n_samples, n_features)\n",
      "     |          Test samples. For some estimators this may be a\n",
      "     |          precomputed kernel matrix instead, shape = (n_samples,\n",
      "     |          n_samples_fitted], where n_samples_fitted is the number of\n",
      "     |          samples used in the fitting for the estimator.\n",
      "     |      \n",
      "     |      y : array-like, shape = (n_samples) or (n_samples, n_outputs)\n",
      "     |          True values for X.\n",
      "     |      \n",
      "     |      sample_weight : array-like, shape = [n_samples], optional\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      score : float\n",
      "     |          R^2 of self.predict(X) wrt. y.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      The R2 score used when calling ``score`` on a regressor will use\n",
      "     |      ``multioutput='uniform_average'`` from version 0.23 to keep consistent\n",
      "     |      with `metrics.r2_score`. This will influence the ``score`` method of\n",
      "     |      all the multioutput regressors (except for\n",
      "     |      `multioutput.MultiOutputRegressor`). To specify the default value\n",
      "     |      manually and avoid the warning, please either call `metrics.r2_score`\n",
      "     |      directly or make a custom scorer with `metrics.make_scorer` (the\n",
      "     |      built-in scorer ``'r2'`` uses ``multioutput='uniform_average'``).\n",
      "    \n",
      "    class GradientBoostingClassifier(BaseGradientBoosting, sklearn.base.ClassifierMixin)\n",
      "     |  GradientBoostingClassifier(loss='deviance', learning_rate=0.1, n_estimators=100, subsample=1.0, criterion='friedman_mse', min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_depth=3, min_impurity_decrease=0.0, min_impurity_split=None, init=None, random_state=None, max_features=None, verbose=0, max_leaf_nodes=None, warm_start=False, presort='auto', validation_fraction=0.1, n_iter_no_change=None, tol=0.0001)\n",
      "     |  \n",
      "     |  Gradient Boosting for classification.\n",
      "     |  \n",
      "     |  GB builds an additive model in a\n",
      "     |  forward stage-wise fashion; it allows for the optimization of\n",
      "     |  arbitrary differentiable loss functions. In each stage ``n_classes_``\n",
      "     |  regression trees are fit on the negative gradient of the\n",
      "     |  binomial or multinomial deviance loss function. Binary classification\n",
      "     |  is a special case where only a single regression tree is induced.\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <gradient_boosting>`.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  loss : {'deviance', 'exponential'}, optional (default='deviance')\n",
      "     |      loss function to be optimized. 'deviance' refers to\n",
      "     |      deviance (= logistic regression) for classification\n",
      "     |      with probabilistic outputs. For loss 'exponential' gradient\n",
      "     |      boosting recovers the AdaBoost algorithm.\n",
      "     |  \n",
      "     |  learning_rate : float, optional (default=0.1)\n",
      "     |      learning rate shrinks the contribution of each tree by `learning_rate`.\n",
      "     |      There is a trade-off between learning_rate and n_estimators.\n",
      "     |  \n",
      "     |  n_estimators : int (default=100)\n",
      "     |      The number of boosting stages to perform. Gradient boosting\n",
      "     |      is fairly robust to over-fitting so a large number usually\n",
      "     |      results in better performance.\n",
      "     |  \n",
      "     |  subsample : float, optional (default=1.0)\n",
      "     |      The fraction of samples to be used for fitting the individual base\n",
      "     |      learners. If smaller than 1.0 this results in Stochastic Gradient\n",
      "     |      Boosting. `subsample` interacts with the parameter `n_estimators`.\n",
      "     |      Choosing `subsample < 1.0` leads to a reduction of variance\n",
      "     |      and an increase in bias.\n",
      "     |  \n",
      "     |  criterion : string, optional (default=\"friedman_mse\")\n",
      "     |      The function to measure the quality of a split. Supported criteria\n",
      "     |      are \"friedman_mse\" for the mean squared error with improvement\n",
      "     |      score by Friedman, \"mse\" for mean squared error, and \"mae\" for\n",
      "     |      the mean absolute error. The default value of \"friedman_mse\" is\n",
      "     |      generally the best as it can provide a better approximation in\n",
      "     |      some cases.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.18\n",
      "     |  \n",
      "     |  min_samples_split : int, float, optional (default=2)\n",
      "     |      The minimum number of samples required to split an internal node:\n",
      "     |  \n",
      "     |      - If int, then consider `min_samples_split` as the minimum number.\n",
      "     |      - If float, then `min_samples_split` is a fraction and\n",
      "     |        `ceil(min_samples_split * n_samples)` are the minimum\n",
      "     |        number of samples for each split.\n",
      "     |  \n",
      "     |      .. versionchanged:: 0.18\n",
      "     |         Added float values for fractions.\n",
      "     |  \n",
      "     |  min_samples_leaf : int, float, optional (default=1)\n",
      "     |      The minimum number of samples required to be at a leaf node.\n",
      "     |      A split point at any depth will only be considered if it leaves at\n",
      "     |      least ``min_samples_leaf`` training samples in each of the left and\n",
      "     |      right branches.  This may have the effect of smoothing the model,\n",
      "     |      especially in regression.\n",
      "     |  \n",
      "     |      - If int, then consider `min_samples_leaf` as the minimum number.\n",
      "     |      - If float, then `min_samples_leaf` is a fraction and\n",
      "     |        `ceil(min_samples_leaf * n_samples)` are the minimum\n",
      "     |        number of samples for each node.\n",
      "     |  \n",
      "     |      .. versionchanged:: 0.18\n",
      "     |         Added float values for fractions.\n",
      "     |  \n",
      "     |  min_weight_fraction_leaf : float, optional (default=0.)\n",
      "     |      The minimum weighted fraction of the sum total of weights (of all\n",
      "     |      the input samples) required to be at a leaf node. Samples have\n",
      "     |      equal weight when sample_weight is not provided.\n",
      "     |  \n",
      "     |  max_depth : integer, optional (default=3)\n",
      "     |      maximum depth of the individual regression estimators. The maximum\n",
      "     |      depth limits the number of nodes in the tree. Tune this parameter\n",
      "     |      for best performance; the best value depends on the interaction\n",
      "     |      of the input variables.\n",
      "     |  \n",
      "     |  min_impurity_decrease : float, optional (default=0.)\n",
      "     |      A node will be split if this split induces a decrease of the impurity\n",
      "     |      greater than or equal to this value.\n",
      "     |  \n",
      "     |      The weighted impurity decrease equation is the following::\n",
      "     |  \n",
      "     |          N_t / N * (impurity - N_t_R / N_t * right_impurity\n",
      "     |                              - N_t_L / N_t * left_impurity)\n",
      "     |  \n",
      "     |      where ``N`` is the total number of samples, ``N_t`` is the number of\n",
      "     |      samples at the current node, ``N_t_L`` is the number of samples in the\n",
      "     |      left child, and ``N_t_R`` is the number of samples in the right child.\n",
      "     |  \n",
      "     |      ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,\n",
      "     |      if ``sample_weight`` is passed.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.19\n",
      "     |  \n",
      "     |  min_impurity_split : float, (default=1e-7)\n",
      "     |      Threshold for early stopping in tree growth. A node will split\n",
      "     |      if its impurity is above the threshold, otherwise it is a leaf.\n",
      "     |  \n",
      "     |      .. deprecated:: 0.19\n",
      "     |         ``min_impurity_split`` has been deprecated in favor of\n",
      "     |         ``min_impurity_decrease`` in 0.19. The default value of\n",
      "     |         ``min_impurity_split`` will change from 1e-7 to 0 in 0.23 and it\n",
      "     |         will be removed in 0.25. Use ``min_impurity_decrease`` instead.\n",
      "     |  \n",
      "     |  init : estimator or 'zero', optional (default=None)\n",
      "     |      An estimator object that is used to compute the initial predictions.\n",
      "     |      ``init`` has to provide `fit` and `predict_proba`. If 'zero', the\n",
      "     |      initial raw predictions are set to zero. By default, a\n",
      "     |      ``DummyEstimator`` predicting the classes priors is used.\n",
      "     |  \n",
      "     |  random_state : int, RandomState instance or None, optional (default=None)\n",
      "     |      If int, random_state is the seed used by the random number generator;\n",
      "     |      If RandomState instance, random_state is the random number generator;\n",
      "     |      If None, the random number generator is the RandomState instance used\n",
      "     |      by `np.random`.\n",
      "     |  \n",
      "     |  max_features : int, float, string or None, optional (default=None)\n",
      "     |      The number of features to consider when looking for the best split:\n",
      "     |  \n",
      "     |      - If int, then consider `max_features` features at each split.\n",
      "     |      - If float, then `max_features` is a fraction and\n",
      "     |        `int(max_features * n_features)` features are considered at each\n",
      "     |        split.\n",
      "     |      - If \"auto\", then `max_features=sqrt(n_features)`.\n",
      "     |      - If \"sqrt\", then `max_features=sqrt(n_features)`.\n",
      "     |      - If \"log2\", then `max_features=log2(n_features)`.\n",
      "     |      - If None, then `max_features=n_features`.\n",
      "     |  \n",
      "     |      Choosing `max_features < n_features` leads to a reduction of variance\n",
      "     |      and an increase in bias.\n",
      "     |  \n",
      "     |      Note: the search for a split does not stop until at least one\n",
      "     |      valid partition of the node samples is found, even if it requires to\n",
      "     |      effectively inspect more than ``max_features`` features.\n",
      "     |  \n",
      "     |  verbose : int, default: 0\n",
      "     |      Enable verbose output. If 1 then it prints progress and performance\n",
      "     |      once in a while (the more trees the lower the frequency). If greater\n",
      "     |      than 1 then it prints progress and performance for every tree.\n",
      "     |  \n",
      "     |  max_leaf_nodes : int or None, optional (default=None)\n",
      "     |      Grow trees with ``max_leaf_nodes`` in best-first fashion.\n",
      "     |      Best nodes are defined as relative reduction in impurity.\n",
      "     |      If None then unlimited number of leaf nodes.\n",
      "     |  \n",
      "     |  warm_start : bool, default: False\n",
      "     |      When set to ``True``, reuse the solution of the previous call to fit\n",
      "     |      and add more estimators to the ensemble, otherwise, just erase the\n",
      "     |      previous solution. See :term:`the Glossary <warm_start>`.\n",
      "     |  \n",
      "     |  presort : bool or 'auto', optional (default='auto')\n",
      "     |      Whether to presort the data to speed up the finding of best splits in\n",
      "     |      fitting. Auto mode by default will use presorting on dense data and\n",
      "     |      default to normal sorting on sparse data. Setting presort to true on\n",
      "     |      sparse data will raise an error.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.17\n",
      "     |         *presort* parameter.\n",
      "     |  \n",
      "     |  validation_fraction : float, optional, default 0.1\n",
      "     |      The proportion of training data to set aside as validation set for\n",
      "     |      early stopping. Must be between 0 and 1.\n",
      "     |      Only used if ``n_iter_no_change`` is set to an integer.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.20\n",
      "     |  \n",
      "     |  n_iter_no_change : int, default None\n",
      "     |      ``n_iter_no_change`` is used to decide if early stopping will be used\n",
      "     |      to terminate training when validation score is not improving. By\n",
      "     |      default it is set to None to disable early stopping. If set to a\n",
      "     |      number, it will set aside ``validation_fraction`` size of the training\n",
      "     |      data as validation and terminate training when validation score is not\n",
      "     |      improving in all of the previous ``n_iter_no_change`` numbers of\n",
      "     |      iterations. The split is stratified.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.20\n",
      "     |  \n",
      "     |  tol : float, optional, default 1e-4\n",
      "     |      Tolerance for the early stopping. When the loss is not improving\n",
      "     |      by at least tol for ``n_iter_no_change`` iterations (if set to a\n",
      "     |      number), the training stops.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.20\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  n_estimators_ : int\n",
      "     |      The number of estimators as selected by early stopping (if\n",
      "     |      ``n_iter_no_change`` is specified). Otherwise it is set to\n",
      "     |      ``n_estimators``.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.20\n",
      "     |  \n",
      "     |  feature_importances_ : array, shape (n_features,)\n",
      "     |      The feature importances (the higher, the more important the feature).\n",
      "     |  \n",
      "     |  oob_improvement_ : array, shape (n_estimators,)\n",
      "     |      The improvement in loss (= deviance) on the out-of-bag samples\n",
      "     |      relative to the previous iteration.\n",
      "     |      ``oob_improvement_[0]`` is the improvement in\n",
      "     |      loss of the first stage over the ``init`` estimator.\n",
      "     |  \n",
      "     |  train_score_ : array, shape (n_estimators,)\n",
      "     |      The i-th score ``train_score_[i]`` is the deviance (= loss) of the\n",
      "     |      model at iteration ``i`` on the in-bag sample.\n",
      "     |      If ``subsample == 1`` this is the deviance on the training data.\n",
      "     |  \n",
      "     |  loss_ : LossFunction\n",
      "     |      The concrete ``LossFunction`` object.\n",
      "     |  \n",
      "     |  init_ : estimator\n",
      "     |      The estimator that provides the initial predictions.\n",
      "     |      Set via the ``init`` argument or ``loss.init_estimator``.\n",
      "     |  \n",
      "     |  estimators_ : ndarray of DecisionTreeRegressor,shape (n_estimators, ``loss_.K``)\n",
      "     |      The collection of fitted sub-estimators. ``loss_.K`` is 1 for binary\n",
      "     |      classification, otherwise n_classes.\n",
      "     |  \n",
      "     |  Notes\n",
      "     |  -----\n",
      "     |  The features are always randomly permuted at each split. Therefore,\n",
      "     |  the best found split may vary, even with the same training data and\n",
      "     |  ``max_features=n_features``, if the improvement of the criterion is\n",
      "     |  identical for several splits enumerated during the search of the best\n",
      "     |  split. To obtain a deterministic behaviour during fitting,\n",
      "     |  ``random_state`` has to be fixed.\n",
      "     |  \n",
      "     |  See also\n",
      "     |  --------\n",
      "     |  sklearn.ensemble.HistGradientBoostingClassifier,\n",
      "     |  sklearn.tree.DecisionTreeClassifier, RandomForestClassifier\n",
      "     |  AdaBoostClassifier\n",
      "     |  \n",
      "     |  References\n",
      "     |  ----------\n",
      "     |  J. Friedman, Greedy Function Approximation: A Gradient Boosting\n",
      "     |  Machine, The Annals of Statistics, Vol. 29, No. 5, 2001.\n",
      "     |  \n",
      "     |  J. Friedman, Stochastic Gradient Boosting, 1999\n",
      "     |  \n",
      "     |  T. Hastie, R. Tibshirani and J. Friedman.\n",
      "     |  Elements of Statistical Learning Ed. 2, Springer, 2009.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      GradientBoostingClassifier\n",
      "     |      BaseGradientBoosting\n",
      "     |      sklearn.ensemble.base.BaseEnsemble\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      sklearn.base.MetaEstimatorMixin\n",
      "     |      sklearn.base.ClassifierMixin\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, loss='deviance', learning_rate=0.1, n_estimators=100, subsample=1.0, criterion='friedman_mse', min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_depth=3, min_impurity_decrease=0.0, min_impurity_split=None, init=None, random_state=None, max_features=None, verbose=0, max_leaf_nodes=None, warm_start=False, presort='auto', validation_fraction=0.1, n_iter_no_change=None, tol=0.0001)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  decision_function(self, X)\n",
      "     |      Compute the decision function of ``X``.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix}, shape (n_samples, n_features)\n",
      "     |          The input samples. Internally, it will be converted to\n",
      "     |          ``dtype=np.float32`` and if a sparse matrix is provided\n",
      "     |          to a sparse ``csr_matrix``.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      score : array, shape (n_samples, n_classes) or (n_samples,)\n",
      "     |          The decision function of the input samples, which corresponds to\n",
      "     |          the raw values predicted from the trees of the ensemble . The\n",
      "     |          order of the classes corresponds to that in the attribute\n",
      "     |          `classes_`. Regression and binary classification produce an\n",
      "     |          array of shape [n_samples].\n",
      "     |  \n",
      "     |  predict(self, X)\n",
      "     |      Predict class for X.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix}, shape (n_samples, n_features)\n",
      "     |          The input samples. Internally, it will be converted to\n",
      "     |          ``dtype=np.float32`` and if a sparse matrix is provided\n",
      "     |          to a sparse ``csr_matrix``.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      y : array, shape (n_samples,)\n",
      "     |          The predicted values.\n",
      "     |  \n",
      "     |  predict_log_proba(self, X)\n",
      "     |      Predict class log-probabilities for X.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix}, shape (n_samples, n_features)\n",
      "     |          The input samples. Internally, it will be converted to\n",
      "     |          ``dtype=np.float32`` and if a sparse matrix is provided\n",
      "     |          to a sparse ``csr_matrix``.\n",
      "     |      \n",
      "     |      Raises\n",
      "     |      ------\n",
      "     |      AttributeError\n",
      "     |          If the ``loss`` does not support probabilities.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      p : array, shape (n_samples, n_classes)\n",
      "     |          The class log-probabilities of the input samples. The order of the\n",
      "     |          classes corresponds to that in the attribute `classes_`.\n",
      "     |  \n",
      "     |  predict_proba(self, X)\n",
      "     |      Predict class probabilities for X.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix}, shape (n_samples, n_features)\n",
      "     |          The input samples. Internally, it will be converted to\n",
      "     |          ``dtype=np.float32`` and if a sparse matrix is provided\n",
      "     |          to a sparse ``csr_matrix``.\n",
      "     |      \n",
      "     |      Raises\n",
      "     |      ------\n",
      "     |      AttributeError\n",
      "     |          If the ``loss`` does not support probabilities.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      p : array, shape (n_samples, n_classes)\n",
      "     |          The class probabilities of the input samples. The order of the\n",
      "     |          classes corresponds to that in the attribute `classes_`.\n",
      "     |  \n",
      "     |  staged_decision_function(self, X)\n",
      "     |      Compute decision function of ``X`` for each iteration.\n",
      "     |      \n",
      "     |      This method allows monitoring (i.e. determine error on testing set)\n",
      "     |      after each stage.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix}, shape (n_samples, n_features)\n",
      "     |          The input samples. Internally, it will be converted to\n",
      "     |          ``dtype=np.float32`` and if a sparse matrix is provided\n",
      "     |          to a sparse ``csr_matrix``.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      score : generator of array, shape (n_samples, k)\n",
      "     |          The decision function of the input samples, which corresponds to\n",
      "     |          the raw values predicted from the trees of the ensemble . The\n",
      "     |          classes corresponds to that in the attribute `classes_`.\n",
      "     |          Regression and binary classification are special cases with\n",
      "     |          ``k == 1``, otherwise ``k==n_classes``.\n",
      "     |  \n",
      "     |  staged_predict(self, X)\n",
      "     |      Predict class at each stage for X.\n",
      "     |      \n",
      "     |      This method allows monitoring (i.e. determine error on testing set)\n",
      "     |      after each stage.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix}, shape (n_samples, n_features)\n",
      "     |          The input samples. Internally, it will be converted to\n",
      "     |          ``dtype=np.float32`` and if a sparse matrix is provided\n",
      "     |          to a sparse ``csr_matrix``.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      y : generator of array of shape (n_samples,)\n",
      "     |          The predicted value of the input samples.\n",
      "     |  \n",
      "     |  staged_predict_proba(self, X)\n",
      "     |      Predict class probabilities at each stage for X.\n",
      "     |      \n",
      "     |      This method allows monitoring (i.e. determine error on testing set)\n",
      "     |      after each stage.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix}, shape (n_samples, n_features)\n",
      "     |          The input samples. Internally, it will be converted to\n",
      "     |          ``dtype=np.float32`` and if a sparse matrix is provided\n",
      "     |          to a sparse ``csr_matrix``.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      y : generator of array of shape (n_samples,)\n",
      "     |          The predicted value of the input samples.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from BaseGradientBoosting:\n",
      "     |  \n",
      "     |  apply(self, X)\n",
      "     |      Apply trees in the ensemble to X, return leaf indices.\n",
      "     |      \n",
      "     |      .. versionadded:: 0.17\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix}, shape (n_samples, n_features)\n",
      "     |          The input samples. Internally, its dtype will be converted to\n",
      "     |          ``dtype=np.float32``. If a sparse matrix is provided, it will\n",
      "     |          be converted to a sparse ``csr_matrix``.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      X_leaves : array-like, shape (n_samples, n_estimators, n_classes)\n",
      "     |          For each datapoint x in X and for each tree in the ensemble,\n",
      "     |          return the index of the leaf x ends up in each estimator.\n",
      "     |          In the case of binary classification n_classes is 1.\n",
      "     |  \n",
      "     |  fit(self, X, y, sample_weight=None, monitor=None)\n",
      "     |      Fit the gradient boosting model.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix}, shape (n_samples, n_features)\n",
      "     |          The input samples. Internally, it will be converted to\n",
      "     |          ``dtype=np.float32`` and if a sparse matrix is provided\n",
      "     |          to a sparse ``csr_matrix``.\n",
      "     |      \n",
      "     |      y : array-like, shape (n_samples,)\n",
      "     |          Target values (strings or integers in classification, real numbers\n",
      "     |          in regression)\n",
      "     |          For classification, labels must correspond to classes.\n",
      "     |      \n",
      "     |      sample_weight : array-like, shape (n_samples,) or None\n",
      "     |          Sample weights. If None, then samples are equally weighted. Splits\n",
      "     |          that would create child nodes with net zero or negative weight are\n",
      "     |          ignored while searching for a split in each node. In the case of\n",
      "     |          classification, splits are also ignored if they would result in any\n",
      "     |          single class carrying a negative weight in either child node.\n",
      "     |      \n",
      "     |      monitor : callable, optional\n",
      "     |          The monitor is called after each iteration with the current\n",
      "     |          iteration, a reference to the estimator and the local variables of\n",
      "     |          ``_fit_stages`` as keyword arguments ``callable(i, self,\n",
      "     |          locals())``. If the callable returns ``True`` the fitting procedure\n",
      "     |          is stopped. The monitor can be used for various things such as\n",
      "     |          computing held-out estimates, early stopping, model introspect, and\n",
      "     |          snapshoting.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from BaseGradientBoosting:\n",
      "     |  \n",
      "     |  feature_importances_\n",
      "     |      Return the feature importances (the higher, the more important the\n",
      "     |         feature).\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      feature_importances_ : array, shape (n_features,)\n",
      "     |          The values of this array sum to 1, unless all trees are single node\n",
      "     |          trees consisting of only the root node, in which case it will be an\n",
      "     |          array of zeros.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.ensemble.base.BaseEnsemble:\n",
      "     |  \n",
      "     |  __getitem__(self, index)\n",
      "     |      Returns the index'th estimator in the ensemble.\n",
      "     |  \n",
      "     |  __iter__(self)\n",
      "     |      Returns iterator over estimators in the ensemble.\n",
      "     |  \n",
      "     |  __len__(self)\n",
      "     |      Returns the number of estimators in the ensemble.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __repr__(self, N_CHAR_MAX=700)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : boolean, optional\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : mapping of string to any\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as pipelines). The latter have parameters of the form\n",
      "     |      ``<component>__<parameter>`` so that it's possible to update each\n",
      "     |      component of a nested object.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.ClassifierMixin:\n",
      "     |  \n",
      "     |  score(self, X, y, sample_weight=None)\n",
      "     |      Returns the mean accuracy on the given test data and labels.\n",
      "     |      \n",
      "     |      In multi-label classification, this is the subset accuracy\n",
      "     |      which is a harsh metric since you require for each sample that\n",
      "     |      each label set be correctly predicted.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like, shape = (n_samples, n_features)\n",
      "     |          Test samples.\n",
      "     |      \n",
      "     |      y : array-like, shape = (n_samples) or (n_samples, n_outputs)\n",
      "     |          True labels for X.\n",
      "     |      \n",
      "     |      sample_weight : array-like, shape = [n_samples], optional\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      score : float\n",
      "     |          Mean accuracy of self.predict(X) wrt. y.\n",
      "    \n",
      "    class GradientBoostingRegressor(BaseGradientBoosting, sklearn.base.RegressorMixin)\n",
      "     |  GradientBoostingRegressor(loss='ls', learning_rate=0.1, n_estimators=100, subsample=1.0, criterion='friedman_mse', min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_depth=3, min_impurity_decrease=0.0, min_impurity_split=None, init=None, random_state=None, max_features=None, alpha=0.9, verbose=0, max_leaf_nodes=None, warm_start=False, presort='auto', validation_fraction=0.1, n_iter_no_change=None, tol=0.0001)\n",
      "     |  \n",
      "     |  Gradient Boosting for regression.\n",
      "     |  \n",
      "     |  GB builds an additive model in a forward stage-wise fashion;\n",
      "     |  it allows for the optimization of arbitrary differentiable loss functions.\n",
      "     |  In each stage a regression tree is fit on the negative gradient of the\n",
      "     |  given loss function.\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <gradient_boosting>`.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  loss : {'ls', 'lad', 'huber', 'quantile'}, optional (default='ls')\n",
      "     |      loss function to be optimized. 'ls' refers to least squares\n",
      "     |      regression. 'lad' (least absolute deviation) is a highly robust\n",
      "     |      loss function solely based on order information of the input\n",
      "     |      variables. 'huber' is a combination of the two. 'quantile'\n",
      "     |      allows quantile regression (use `alpha` to specify the quantile).\n",
      "     |  \n",
      "     |  learning_rate : float, optional (default=0.1)\n",
      "     |      learning rate shrinks the contribution of each tree by `learning_rate`.\n",
      "     |      There is a trade-off between learning_rate and n_estimators.\n",
      "     |  \n",
      "     |  n_estimators : int (default=100)\n",
      "     |      The number of boosting stages to perform. Gradient boosting\n",
      "     |      is fairly robust to over-fitting so a large number usually\n",
      "     |      results in better performance.\n",
      "     |  \n",
      "     |  subsample : float, optional (default=1.0)\n",
      "     |      The fraction of samples to be used for fitting the individual base\n",
      "     |      learners. If smaller than 1.0 this results in Stochastic Gradient\n",
      "     |      Boosting. `subsample` interacts with the parameter `n_estimators`.\n",
      "     |      Choosing `subsample < 1.0` leads to a reduction of variance\n",
      "     |      and an increase in bias.\n",
      "     |  \n",
      "     |  criterion : string, optional (default=\"friedman_mse\")\n",
      "     |      The function to measure the quality of a split. Supported criteria\n",
      "     |      are \"friedman_mse\" for the mean squared error with improvement\n",
      "     |      score by Friedman, \"mse\" for mean squared error, and \"mae\" for\n",
      "     |      the mean absolute error. The default value of \"friedman_mse\" is\n",
      "     |      generally the best as it can provide a better approximation in\n",
      "     |      some cases.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.18\n",
      "     |  \n",
      "     |  min_samples_split : int, float, optional (default=2)\n",
      "     |      The minimum number of samples required to split an internal node:\n",
      "     |  \n",
      "     |      - If int, then consider `min_samples_split` as the minimum number.\n",
      "     |      - If float, then `min_samples_split` is a fraction and\n",
      "     |        `ceil(min_samples_split * n_samples)` are the minimum\n",
      "     |        number of samples for each split.\n",
      "     |  \n",
      "     |      .. versionchanged:: 0.18\n",
      "     |         Added float values for fractions.\n",
      "     |  \n",
      "     |  min_samples_leaf : int, float, optional (default=1)\n",
      "     |      The minimum number of samples required to be at a leaf node.\n",
      "     |      A split point at any depth will only be considered if it leaves at\n",
      "     |      least ``min_samples_leaf`` training samples in each of the left and\n",
      "     |      right branches.  This may have the effect of smoothing the model,\n",
      "     |      especially in regression.\n",
      "     |  \n",
      "     |      - If int, then consider `min_samples_leaf` as the minimum number.\n",
      "     |      - If float, then `min_samples_leaf` is a fraction and\n",
      "     |        `ceil(min_samples_leaf * n_samples)` are the minimum\n",
      "     |        number of samples for each node.\n",
      "     |  \n",
      "     |      .. versionchanged:: 0.18\n",
      "     |         Added float values for fractions.\n",
      "     |  \n",
      "     |  min_weight_fraction_leaf : float, optional (default=0.)\n",
      "     |      The minimum weighted fraction of the sum total of weights (of all\n",
      "     |      the input samples) required to be at a leaf node. Samples have\n",
      "     |      equal weight when sample_weight is not provided.\n",
      "     |  \n",
      "     |  max_depth : integer, optional (default=3)\n",
      "     |      maximum depth of the individual regression estimators. The maximum\n",
      "     |      depth limits the number of nodes in the tree. Tune this parameter\n",
      "     |      for best performance; the best value depends on the interaction\n",
      "     |      of the input variables.\n",
      "     |  \n",
      "     |  min_impurity_decrease : float, optional (default=0.)\n",
      "     |      A node will be split if this split induces a decrease of the impurity\n",
      "     |      greater than or equal to this value.\n",
      "     |  \n",
      "     |      The weighted impurity decrease equation is the following::\n",
      "     |  \n",
      "     |          N_t / N * (impurity - N_t_R / N_t * right_impurity\n",
      "     |                              - N_t_L / N_t * left_impurity)\n",
      "     |  \n",
      "     |      where ``N`` is the total number of samples, ``N_t`` is the number of\n",
      "     |      samples at the current node, ``N_t_L`` is the number of samples in the\n",
      "     |      left child, and ``N_t_R`` is the number of samples in the right child.\n",
      "     |  \n",
      "     |      ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,\n",
      "     |      if ``sample_weight`` is passed.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.19\n",
      "     |  \n",
      "     |  min_impurity_split : float, (default=1e-7)\n",
      "     |      Threshold for early stopping in tree growth. A node will split\n",
      "     |      if its impurity is above the threshold, otherwise it is a leaf.\n",
      "     |  \n",
      "     |      .. deprecated:: 0.19\n",
      "     |         ``min_impurity_split`` has been deprecated in favor of\n",
      "     |         ``min_impurity_decrease`` in 0.19. The default value of\n",
      "     |         ``min_impurity_split`` will change from 1e-7 to 0 in 0.23 and it\n",
      "     |         will be removed in 0.25. Use ``min_impurity_decrease`` instead.\n",
      "     |  \n",
      "     |  init : estimator or 'zero', optional (default=None)\n",
      "     |      An estimator object that is used to compute the initial predictions.\n",
      "     |      ``init`` has to provide `fit` and `predict`. If 'zero', the initial\n",
      "     |      raw predictions are set to zero. By default a ``DummyEstimator`` is\n",
      "     |      used, predicting either the average target value (for loss='ls'), or\n",
      "     |      a quantile for the other losses.\n",
      "     |  \n",
      "     |  random_state : int, RandomState instance or None, optional (default=None)\n",
      "     |      If int, random_state is the seed used by the random number generator;\n",
      "     |      If RandomState instance, random_state is the random number generator;\n",
      "     |      If None, the random number generator is the RandomState instance used\n",
      "     |      by `np.random`.\n",
      "     |  \n",
      "     |  max_features : int, float, string or None, optional (default=None)\n",
      "     |      The number of features to consider when looking for the best split:\n",
      "     |  \n",
      "     |      - If int, then consider `max_features` features at each split.\n",
      "     |      - If float, then `max_features` is a fraction and\n",
      "     |        `int(max_features * n_features)` features are considered at each\n",
      "     |        split.\n",
      "     |      - If \"auto\", then `max_features=n_features`.\n",
      "     |      - If \"sqrt\", then `max_features=sqrt(n_features)`.\n",
      "     |      - If \"log2\", then `max_features=log2(n_features)`.\n",
      "     |      - If None, then `max_features=n_features`.\n",
      "     |  \n",
      "     |      Choosing `max_features < n_features` leads to a reduction of variance\n",
      "     |      and an increase in bias.\n",
      "     |  \n",
      "     |      Note: the search for a split does not stop until at least one\n",
      "     |      valid partition of the node samples is found, even if it requires to\n",
      "     |      effectively inspect more than ``max_features`` features.\n",
      "     |  \n",
      "     |  alpha : float (default=0.9)\n",
      "     |      The alpha-quantile of the huber loss function and the quantile\n",
      "     |      loss function. Only if ``loss='huber'`` or ``loss='quantile'``.\n",
      "     |  \n",
      "     |  verbose : int, default: 0\n",
      "     |      Enable verbose output. If 1 then it prints progress and performance\n",
      "     |      once in a while (the more trees the lower the frequency). If greater\n",
      "     |      than 1 then it prints progress and performance for every tree.\n",
      "     |  \n",
      "     |  max_leaf_nodes : int or None, optional (default=None)\n",
      "     |      Grow trees with ``max_leaf_nodes`` in best-first fashion.\n",
      "     |      Best nodes are defined as relative reduction in impurity.\n",
      "     |      If None then unlimited number of leaf nodes.\n",
      "     |  \n",
      "     |  warm_start : bool, default: False\n",
      "     |      When set to ``True``, reuse the solution of the previous call to fit\n",
      "     |      and add more estimators to the ensemble, otherwise, just erase the\n",
      "     |      previous solution. See :term:`the Glossary <warm_start>`.\n",
      "     |  \n",
      "     |  presort : bool or 'auto', optional (default='auto')\n",
      "     |      Whether to presort the data to speed up the finding of best splits in\n",
      "     |      fitting. Auto mode by default will use presorting on dense data and\n",
      "     |      default to normal sorting on sparse data. Setting presort to true on\n",
      "     |      sparse data will raise an error.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.17\n",
      "     |         optional parameter *presort*.\n",
      "     |  \n",
      "     |  validation_fraction : float, optional, default 0.1\n",
      "     |      The proportion of training data to set aside as validation set for\n",
      "     |      early stopping. Must be between 0 and 1.\n",
      "     |      Only used if ``n_iter_no_change`` is set to an integer.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.20\n",
      "     |  \n",
      "     |  n_iter_no_change : int, default None\n",
      "     |      ``n_iter_no_change`` is used to decide if early stopping will be used\n",
      "     |      to terminate training when validation score is not improving. By\n",
      "     |      default it is set to None to disable early stopping. If set to a\n",
      "     |      number, it will set aside ``validation_fraction`` size of the training\n",
      "     |      data as validation and terminate training when validation score is not\n",
      "     |      improving in all of the previous ``n_iter_no_change`` numbers of\n",
      "     |      iterations.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.20\n",
      "     |  \n",
      "     |  tol : float, optional, default 1e-4\n",
      "     |      Tolerance for the early stopping. When the loss is not improving\n",
      "     |      by at least tol for ``n_iter_no_change`` iterations (if set to a\n",
      "     |      number), the training stops.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.20\n",
      "     |  \n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  feature_importances_ : array, shape (n_features,)\n",
      "     |      The feature importances (the higher, the more important the feature).\n",
      "     |  \n",
      "     |  oob_improvement_ : array, shape (n_estimators,)\n",
      "     |      The improvement in loss (= deviance) on the out-of-bag samples\n",
      "     |      relative to the previous iteration.\n",
      "     |      ``oob_improvement_[0]`` is the improvement in\n",
      "     |      loss of the first stage over the ``init`` estimator.\n",
      "     |  \n",
      "     |  train_score_ : array, shape (n_estimators,)\n",
      "     |      The i-th score ``train_score_[i]`` is the deviance (= loss) of the\n",
      "     |      model at iteration ``i`` on the in-bag sample.\n",
      "     |      If ``subsample == 1`` this is the deviance on the training data.\n",
      "     |  \n",
      "     |  loss_ : LossFunction\n",
      "     |      The concrete ``LossFunction`` object.\n",
      "     |  \n",
      "     |  init_ : estimator\n",
      "     |      The estimator that provides the initial predictions.\n",
      "     |      Set via the ``init`` argument or ``loss.init_estimator``.\n",
      "     |  \n",
      "     |  estimators_ : array of DecisionTreeRegressor, shape (n_estimators, 1)\n",
      "     |      The collection of fitted sub-estimators.\n",
      "     |  \n",
      "     |  Notes\n",
      "     |  -----\n",
      "     |  The features are always randomly permuted at each split. Therefore,\n",
      "     |  the best found split may vary, even with the same training data and\n",
      "     |  ``max_features=n_features``, if the improvement of the criterion is\n",
      "     |  identical for several splits enumerated during the search of the best\n",
      "     |  split. To obtain a deterministic behaviour during fitting,\n",
      "     |  ``random_state`` has to be fixed.\n",
      "     |  \n",
      "     |  See also\n",
      "     |  --------\n",
      "     |  sklearn.ensemble.HistGradientBoostingRegressor,\n",
      "     |  sklearn.tree.DecisionTreeRegressor, RandomForestRegressor\n",
      "     |  \n",
      "     |  References\n",
      "     |  ----------\n",
      "     |  J. Friedman, Greedy Function Approximation: A Gradient Boosting\n",
      "     |  Machine, The Annals of Statistics, Vol. 29, No. 5, 2001.\n",
      "     |  \n",
      "     |  J. Friedman, Stochastic Gradient Boosting, 1999\n",
      "     |  \n",
      "     |  T. Hastie, R. Tibshirani and J. Friedman.\n",
      "     |  Elements of Statistical Learning Ed. 2, Springer, 2009.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      GradientBoostingRegressor\n",
      "     |      BaseGradientBoosting\n",
      "     |      sklearn.ensemble.base.BaseEnsemble\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      sklearn.base.MetaEstimatorMixin\n",
      "     |      sklearn.base.RegressorMixin\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, loss='ls', learning_rate=0.1, n_estimators=100, subsample=1.0, criterion='friedman_mse', min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_depth=3, min_impurity_decrease=0.0, min_impurity_split=None, init=None, random_state=None, max_features=None, alpha=0.9, verbose=0, max_leaf_nodes=None, warm_start=False, presort='auto', validation_fraction=0.1, n_iter_no_change=None, tol=0.0001)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  apply(self, X)\n",
      "     |      Apply trees in the ensemble to X, return leaf indices.\n",
      "     |      \n",
      "     |      .. versionadded:: 0.17\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix}, shape (n_samples, n_features)\n",
      "     |          The input samples. Internally, its dtype will be converted to\n",
      "     |          ``dtype=np.float32``. If a sparse matrix is provided, it will\n",
      "     |          be converted to a sparse ``csr_matrix``.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      X_leaves : array-like, shape (n_samples, n_estimators)\n",
      "     |          For each datapoint x in X and for each tree in the ensemble,\n",
      "     |          return the index of the leaf x ends up in each estimator.\n",
      "     |  \n",
      "     |  predict(self, X)\n",
      "     |      Predict regression target for X.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix}, shape (n_samples, n_features)\n",
      "     |          The input samples. Internally, it will be converted to\n",
      "     |          ``dtype=np.float32`` and if a sparse matrix is provided\n",
      "     |          to a sparse ``csr_matrix``.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      y : array, shape (n_samples,)\n",
      "     |          The predicted values.\n",
      "     |  \n",
      "     |  staged_predict(self, X)\n",
      "     |      Predict regression target at each stage for X.\n",
      "     |      \n",
      "     |      This method allows monitoring (i.e. determine error on testing set)\n",
      "     |      after each stage.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix}, shape (n_samples, n_features)\n",
      "     |          The input samples. Internally, it will be converted to\n",
      "     |          ``dtype=np.float32`` and if a sparse matrix is provided\n",
      "     |          to a sparse ``csr_matrix``.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      y : generator of array of shape (n_samples,)\n",
      "     |          The predicted value of the input samples.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from BaseGradientBoosting:\n",
      "     |  \n",
      "     |  fit(self, X, y, sample_weight=None, monitor=None)\n",
      "     |      Fit the gradient boosting model.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix}, shape (n_samples, n_features)\n",
      "     |          The input samples. Internally, it will be converted to\n",
      "     |          ``dtype=np.float32`` and if a sparse matrix is provided\n",
      "     |          to a sparse ``csr_matrix``.\n",
      "     |      \n",
      "     |      y : array-like, shape (n_samples,)\n",
      "     |          Target values (strings or integers in classification, real numbers\n",
      "     |          in regression)\n",
      "     |          For classification, labels must correspond to classes.\n",
      "     |      \n",
      "     |      sample_weight : array-like, shape (n_samples,) or None\n",
      "     |          Sample weights. If None, then samples are equally weighted. Splits\n",
      "     |          that would create child nodes with net zero or negative weight are\n",
      "     |          ignored while searching for a split in each node. In the case of\n",
      "     |          classification, splits are also ignored if they would result in any\n",
      "     |          single class carrying a negative weight in either child node.\n",
      "     |      \n",
      "     |      monitor : callable, optional\n",
      "     |          The monitor is called after each iteration with the current\n",
      "     |          iteration, a reference to the estimator and the local variables of\n",
      "     |          ``_fit_stages`` as keyword arguments ``callable(i, self,\n",
      "     |          locals())``. If the callable returns ``True`` the fitting procedure\n",
      "     |          is stopped. The monitor can be used for various things such as\n",
      "     |          computing held-out estimates, early stopping, model introspect, and\n",
      "     |          snapshoting.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from BaseGradientBoosting:\n",
      "     |  \n",
      "     |  feature_importances_\n",
      "     |      Return the feature importances (the higher, the more important the\n",
      "     |         feature).\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      feature_importances_ : array, shape (n_features,)\n",
      "     |          The values of this array sum to 1, unless all trees are single node\n",
      "     |          trees consisting of only the root node, in which case it will be an\n",
      "     |          array of zeros.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.ensemble.base.BaseEnsemble:\n",
      "     |  \n",
      "     |  __getitem__(self, index)\n",
      "     |      Returns the index'th estimator in the ensemble.\n",
      "     |  \n",
      "     |  __iter__(self)\n",
      "     |      Returns iterator over estimators in the ensemble.\n",
      "     |  \n",
      "     |  __len__(self)\n",
      "     |      Returns the number of estimators in the ensemble.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __repr__(self, N_CHAR_MAX=700)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : boolean, optional\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : mapping of string to any\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as pipelines). The latter have parameters of the form\n",
      "     |      ``<component>__<parameter>`` so that it's possible to update each\n",
      "     |      component of a nested object.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.RegressorMixin:\n",
      "     |  \n",
      "     |  score(self, X, y, sample_weight=None)\n",
      "     |      Returns the coefficient of determination R^2 of the prediction.\n",
      "     |      \n",
      "     |      The coefficient R^2 is defined as (1 - u/v), where u is the residual\n",
      "     |      sum of squares ((y_true - y_pred) ** 2).sum() and v is the total\n",
      "     |      sum of squares ((y_true - y_true.mean()) ** 2).sum().\n",
      "     |      The best possible score is 1.0 and it can be negative (because the\n",
      "     |      model can be arbitrarily worse). A constant model that always\n",
      "     |      predicts the expected value of y, disregarding the input features,\n",
      "     |      would get a R^2 score of 0.0.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like, shape = (n_samples, n_features)\n",
      "     |          Test samples. For some estimators this may be a\n",
      "     |          precomputed kernel matrix instead, shape = (n_samples,\n",
      "     |          n_samples_fitted], where n_samples_fitted is the number of\n",
      "     |          samples used in the fitting for the estimator.\n",
      "     |      \n",
      "     |      y : array-like, shape = (n_samples) or (n_samples, n_outputs)\n",
      "     |          True values for X.\n",
      "     |      \n",
      "     |      sample_weight : array-like, shape = [n_samples], optional\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      score : float\n",
      "     |          R^2 of self.predict(X) wrt. y.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      The R2 score used when calling ``score`` on a regressor will use\n",
      "     |      ``multioutput='uniform_average'`` from version 0.23 to keep consistent\n",
      "     |      with `metrics.r2_score`. This will influence the ``score`` method of\n",
      "     |      all the multioutput regressors (except for\n",
      "     |      `multioutput.MultiOutputRegressor`). To specify the default value\n",
      "     |      manually and avoid the warning, please either call `metrics.r2_score`\n",
      "     |      directly or make a custom scorer with `metrics.make_scorer` (the\n",
      "     |      built-in scorer ``'r2'`` uses ``multioutput='uniform_average'``).\n",
      "    \n",
      "    class IsolationForest(sklearn.ensemble.bagging.BaseBagging, sklearn.base.OutlierMixin)\n",
      "     |  IsolationForest(n_estimators=100, max_samples='auto', contamination='legacy', max_features=1.0, bootstrap=False, n_jobs=None, behaviour='old', random_state=None, verbose=0, warm_start=False)\n",
      "     |  \n",
      "     |  Isolation Forest Algorithm\n",
      "     |  \n",
      "     |  Return the anomaly score of each sample using the IsolationForest algorithm\n",
      "     |  \n",
      "     |  The IsolationForest 'isolates' observations by randomly selecting a feature\n",
      "     |  and then randomly selecting a split value between the maximum and minimum\n",
      "     |  values of the selected feature.\n",
      "     |  \n",
      "     |  Since recursive partitioning can be represented by a tree structure, the\n",
      "     |  number of splittings required to isolate a sample is equivalent to the path\n",
      "     |  length from the root node to the terminating node.\n",
      "     |  \n",
      "     |  This path length, averaged over a forest of such random trees, is a\n",
      "     |  measure of normality and our decision function.\n",
      "     |  \n",
      "     |  Random partitioning produces noticeably shorter paths for anomalies.\n",
      "     |  Hence, when a forest of random trees collectively produce shorter path\n",
      "     |  lengths for particular samples, they are highly likely to be anomalies.\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <isolation_forest>`.\n",
      "     |  \n",
      "     |  .. versionadded:: 0.18\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  n_estimators : int, optional (default=100)\n",
      "     |      The number of base estimators in the ensemble.\n",
      "     |  \n",
      "     |  max_samples : int or float, optional (default=\"auto\")\n",
      "     |      The number of samples to draw from X to train each base estimator.\n",
      "     |          - If int, then draw `max_samples` samples.\n",
      "     |          - If float, then draw `max_samples * X.shape[0]` samples.\n",
      "     |          - If \"auto\", then `max_samples=min(256, n_samples)`.\n",
      "     |  \n",
      "     |      If max_samples is larger than the number of samples provided,\n",
      "     |      all samples will be used for all trees (no sampling).\n",
      "     |  \n",
      "     |  contamination : float in (0., 0.5), optional (default=0.1)\n",
      "     |      The amount of contamination of the data set, i.e. the proportion\n",
      "     |      of outliers in the data set. Used when fitting to define the threshold\n",
      "     |      on the decision function. If 'auto', the decision function threshold is\n",
      "     |      determined as in the original paper.\n",
      "     |  \n",
      "     |      .. versionchanged:: 0.20\n",
      "     |         The default value of ``contamination`` will change from 0.1 in 0.20\n",
      "     |         to ``'auto'`` in 0.22.\n",
      "     |  \n",
      "     |  max_features : int or float, optional (default=1.0)\n",
      "     |      The number of features to draw from X to train each base estimator.\n",
      "     |  \n",
      "     |          - If int, then draw `max_features` features.\n",
      "     |          - If float, then draw `max_features * X.shape[1]` features.\n",
      "     |  \n",
      "     |  bootstrap : boolean, optional (default=False)\n",
      "     |      If True, individual trees are fit on random subsets of the training\n",
      "     |      data sampled with replacement. If False, sampling without replacement\n",
      "     |      is performed.\n",
      "     |  \n",
      "     |  n_jobs : int or None, optional (default=None)\n",
      "     |      The number of jobs to run in parallel for both `fit` and `predict`.\n",
      "     |      ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n",
      "     |      ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n",
      "     |      for more details.\n",
      "     |  \n",
      "     |  behaviour : str, default='old'\n",
      "     |      Behaviour of the ``decision_function`` which can be either 'old' or\n",
      "     |      'new'. Passing ``behaviour='new'`` makes the ``decision_function``\n",
      "     |      change to match other anomaly detection algorithm API which will be\n",
      "     |      the default behaviour in the future. As explained in details in the\n",
      "     |      ``offset_`` attribute documentation, the ``decision_function`` becomes\n",
      "     |      dependent on the contamination parameter, in such a way that 0 becomes\n",
      "     |      its natural threshold to detect outliers.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.20\n",
      "     |         ``behaviour`` is added in 0.20 for back-compatibility purpose.\n",
      "     |  \n",
      "     |      .. deprecated:: 0.20\n",
      "     |         ``behaviour='old'`` is deprecated in 0.20 and will not be possible\n",
      "     |         in 0.22.\n",
      "     |  \n",
      "     |      .. deprecated:: 0.22\n",
      "     |         ``behaviour`` parameter will be deprecated in 0.22 and removed in\n",
      "     |         0.24.\n",
      "     |  \n",
      "     |  random_state : int, RandomState instance or None, optional (default=None)\n",
      "     |      If int, random_state is the seed used by the random number generator;\n",
      "     |      If RandomState instance, random_state is the random number generator;\n",
      "     |      If None, the random number generator is the RandomState instance used\n",
      "     |      by `np.random`.\n",
      "     |  \n",
      "     |  verbose : int, optional (default=0)\n",
      "     |      Controls the verbosity of the tree building process.\n",
      "     |  \n",
      "     |  warm_start : bool, optional (default=False)\n",
      "     |      When set to ``True``, reuse the solution of the previous call to fit\n",
      "     |      and add more estimators to the ensemble, otherwise, just fit a whole\n",
      "     |      new forest. See :term:`the Glossary <warm_start>`.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.21\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  estimators_ : list of DecisionTreeClassifier\n",
      "     |      The collection of fitted sub-estimators.\n",
      "     |  \n",
      "     |  estimators_samples_ : list of arrays\n",
      "     |      The subset of drawn samples (i.e., the in-bag samples) for each base\n",
      "     |      estimator.\n",
      "     |  \n",
      "     |  max_samples_ : integer\n",
      "     |      The actual number of samples\n",
      "     |  \n",
      "     |  offset_ : float\n",
      "     |      Offset used to define the decision function from the raw scores.\n",
      "     |      We have the relation: ``decision_function = score_samples - offset_``.\n",
      "     |      Assuming behaviour == 'new', ``offset_`` is defined as follows.\n",
      "     |      When the contamination parameter is set to \"auto\", the offset is equal\n",
      "     |      to -0.5 as the scores of inliers are close to 0 and the scores of\n",
      "     |      outliers are close to -1. When a contamination parameter different\n",
      "     |      than \"auto\" is provided, the offset is defined in such a way we obtain\n",
      "     |      the expected number of outliers (samples with decision function < 0)\n",
      "     |      in training.\n",
      "     |      Assuming the behaviour parameter is set to 'old', we always have\n",
      "     |      ``offset_ = -0.5``, making the decision function independent from the\n",
      "     |      contamination parameter.\n",
      "     |  \n",
      "     |  Notes\n",
      "     |  -----\n",
      "     |  The implementation is based on an ensemble of ExtraTreeRegressor. The\n",
      "     |  maximum depth of each tree is set to ``ceil(log_2(n))`` where\n",
      "     |  :math:`n` is the number of samples used to build the tree\n",
      "     |  (see (Liu et al., 2008) for more details).\n",
      "     |  \n",
      "     |  References\n",
      "     |  ----------\n",
      "     |  .. [1] Liu, Fei Tony, Ting, Kai Ming and Zhou, Zhi-Hua. \"Isolation forest.\"\n",
      "     |         Data Mining, 2008. ICDM'08. Eighth IEEE International Conference on.\n",
      "     |  .. [2] Liu, Fei Tony, Ting, Kai Ming and Zhou, Zhi-Hua. \"Isolation-based\n",
      "     |         anomaly detection.\" ACM Transactions on Knowledge Discovery from\n",
      "     |         Data (TKDD) 6.1 (2012): 3.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      IsolationForest\n",
      "     |      sklearn.ensemble.bagging.BaseBagging\n",
      "     |      sklearn.ensemble.base.BaseEnsemble\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      sklearn.base.MetaEstimatorMixin\n",
      "     |      sklearn.base.OutlierMixin\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, n_estimators=100, max_samples='auto', contamination='legacy', max_features=1.0, bootstrap=False, n_jobs=None, behaviour='old', random_state=None, verbose=0, warm_start=False)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  decision_function(self, X)\n",
      "     |      Average anomaly score of X of the base classifiers.\n",
      "     |      \n",
      "     |      The anomaly score of an input sample is computed as\n",
      "     |      the mean anomaly score of the trees in the forest.\n",
      "     |      \n",
      "     |      The measure of normality of an observation given a tree is the depth\n",
      "     |      of the leaf containing this observation, which is equivalent to\n",
      "     |      the number of splittings required to isolate this point. In case of\n",
      "     |      several observations n_left in the leaf, the average path length of\n",
      "     |      a n_left samples isolation tree is added.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like or sparse matrix, shape (n_samples, n_features)\n",
      "     |          The input samples. Internally, it will be converted to\n",
      "     |          ``dtype=np.float32`` and if a sparse matrix is provided\n",
      "     |          to a sparse ``csr_matrix``.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      scores : array, shape (n_samples,)\n",
      "     |          The anomaly score of the input samples.\n",
      "     |          The lower, the more abnormal. Negative scores represent outliers,\n",
      "     |          positive scores represent inliers.\n",
      "     |  \n",
      "     |  fit(self, X, y=None, sample_weight=None)\n",
      "     |      Fit estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like or sparse matrix, shape (n_samples, n_features)\n",
      "     |          The input samples. Use ``dtype=np.float32`` for maximum\n",
      "     |          efficiency. Sparse matrices are also supported, use sparse\n",
      "     |          ``csc_matrix`` for maximum efficiency.\n",
      "     |      \n",
      "     |      sample_weight : array-like, shape = [n_samples] or None\n",
      "     |          Sample weights. If None, then samples are equally weighted.\n",
      "     |      \n",
      "     |      y : Ignored\n",
      "     |          not used, present for API consistency by convention.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |  \n",
      "     |  predict(self, X)\n",
      "     |      Predict if a particular sample is an outlier or not.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like or sparse matrix, shape (n_samples, n_features)\n",
      "     |          The input samples. Internally, it will be converted to\n",
      "     |          ``dtype=np.float32`` and if a sparse matrix is provided\n",
      "     |          to a sparse ``csr_matrix``.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      is_inlier : array, shape (n_samples,)\n",
      "     |          For each observation, tells whether or not (+1 or -1) it should\n",
      "     |          be considered as an inlier according to the fitted model.\n",
      "     |  \n",
      "     |  score_samples(self, X)\n",
      "     |      Opposite of the anomaly score defined in the original paper.\n",
      "     |      \n",
      "     |      The anomaly score of an input sample is computed as\n",
      "     |      the mean anomaly score of the trees in the forest.\n",
      "     |      \n",
      "     |      The measure of normality of an observation given a tree is the depth\n",
      "     |      of the leaf containing this observation, which is equivalent to\n",
      "     |      the number of splittings required to isolate this point. In case of\n",
      "     |      several observations n_left in the leaf, the average path length of\n",
      "     |      a n_left samples isolation tree is added.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like or sparse matrix, shape (n_samples, n_features)\n",
      "     |          The input samples.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      scores : array, shape (n_samples,)\n",
      "     |          The anomaly score of the input samples.\n",
      "     |          The lower, the more abnormal.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  threshold_\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.ensemble.bagging.BaseBagging:\n",
      "     |  \n",
      "     |  estimators_samples_\n",
      "     |      The subset of drawn samples for each base estimator.\n",
      "     |      \n",
      "     |      Returns a dynamically generated list of indices identifying\n",
      "     |      the samples used for fitting each member of the ensemble, i.e.,\n",
      "     |      the in-bag samples.\n",
      "     |      \n",
      "     |      Note: the list is re-created at each call to the property in order\n",
      "     |      to reduce the object memory footprint by not storing the sampling\n",
      "     |      data. Thus fetching the property may be slower than expected.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.ensemble.base.BaseEnsemble:\n",
      "     |  \n",
      "     |  __getitem__(self, index)\n",
      "     |      Returns the index'th estimator in the ensemble.\n",
      "     |  \n",
      "     |  __iter__(self)\n",
      "     |      Returns iterator over estimators in the ensemble.\n",
      "     |  \n",
      "     |  __len__(self)\n",
      "     |      Returns the number of estimators in the ensemble.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __repr__(self, N_CHAR_MAX=700)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : boolean, optional\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : mapping of string to any\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as pipelines). The latter have parameters of the form\n",
      "     |      ``<component>__<parameter>`` so that it's possible to update each\n",
      "     |      component of a nested object.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.OutlierMixin:\n",
      "     |  \n",
      "     |  fit_predict(self, X, y=None)\n",
      "     |      Performs fit on X and returns labels for X.\n",
      "     |      \n",
      "     |      Returns -1 for outliers and 1 for inliers.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : ndarray, shape (n_samples, n_features)\n",
      "     |          Input data.\n",
      "     |      \n",
      "     |      y : Ignored\n",
      "     |          not used, present for API consistency by convention.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      y : ndarray, shape (n_samples,)\n",
      "     |          1 for inliers, -1 for outliers.\n",
      "    \n",
      "    class RandomForestClassifier(ForestClassifier)\n",
      "     |  RandomForestClassifier(n_estimators='warn', criterion='gini', max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features='auto', max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, bootstrap=True, oob_score=False, n_jobs=None, random_state=None, verbose=0, warm_start=False, class_weight=None)\n",
      "     |  \n",
      "     |  A random forest classifier.\n",
      "     |  \n",
      "     |  A random forest is a meta estimator that fits a number of decision tree\n",
      "     |  classifiers on various sub-samples of the dataset and uses averaging to\n",
      "     |  improve the predictive accuracy and control over-fitting.\n",
      "     |  The sub-sample size is always the same as the original\n",
      "     |  input sample size but the samples are drawn with replacement if\n",
      "     |  `bootstrap=True` (default).\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <forest>`.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  n_estimators : integer, optional (default=10)\n",
      "     |      The number of trees in the forest.\n",
      "     |  \n",
      "     |      .. versionchanged:: 0.20\n",
      "     |         The default value of ``n_estimators`` will change from 10 in\n",
      "     |         version 0.20 to 100 in version 0.22.\n",
      "     |  \n",
      "     |  criterion : string, optional (default=\"gini\")\n",
      "     |      The function to measure the quality of a split. Supported criteria are\n",
      "     |      \"gini\" for the Gini impurity and \"entropy\" for the information gain.\n",
      "     |      Note: this parameter is tree-specific.\n",
      "     |  \n",
      "     |  max_depth : integer or None, optional (default=None)\n",
      "     |      The maximum depth of the tree. If None, then nodes are expanded until\n",
      "     |      all leaves are pure or until all leaves contain less than\n",
      "     |      min_samples_split samples.\n",
      "     |  \n",
      "     |  min_samples_split : int, float, optional (default=2)\n",
      "     |      The minimum number of samples required to split an internal node:\n",
      "     |  \n",
      "     |      - If int, then consider `min_samples_split` as the minimum number.\n",
      "     |      - If float, then `min_samples_split` is a fraction and\n",
      "     |        `ceil(min_samples_split * n_samples)` are the minimum\n",
      "     |        number of samples for each split.\n",
      "     |  \n",
      "     |      .. versionchanged:: 0.18\n",
      "     |         Added float values for fractions.\n",
      "     |  \n",
      "     |  min_samples_leaf : int, float, optional (default=1)\n",
      "     |      The minimum number of samples required to be at a leaf node.\n",
      "     |      A split point at any depth will only be considered if it leaves at\n",
      "     |      least ``min_samples_leaf`` training samples in each of the left and\n",
      "     |      right branches.  This may have the effect of smoothing the model,\n",
      "     |      especially in regression.\n",
      "     |  \n",
      "     |      - If int, then consider `min_samples_leaf` as the minimum number.\n",
      "     |      - If float, then `min_samples_leaf` is a fraction and\n",
      "     |        `ceil(min_samples_leaf * n_samples)` are the minimum\n",
      "     |        number of samples for each node.\n",
      "     |  \n",
      "     |      .. versionchanged:: 0.18\n",
      "     |         Added float values for fractions.\n",
      "     |  \n",
      "     |  min_weight_fraction_leaf : float, optional (default=0.)\n",
      "     |      The minimum weighted fraction of the sum total of weights (of all\n",
      "     |      the input samples) required to be at a leaf node. Samples have\n",
      "     |      equal weight when sample_weight is not provided.\n",
      "     |  \n",
      "     |  max_features : int, float, string or None, optional (default=\"auto\")\n",
      "     |      The number of features to consider when looking for the best split:\n",
      "     |  \n",
      "     |      - If int, then consider `max_features` features at each split.\n",
      "     |      - If float, then `max_features` is a fraction and\n",
      "     |        `int(max_features * n_features)` features are considered at each\n",
      "     |        split.\n",
      "     |      - If \"auto\", then `max_features=sqrt(n_features)`.\n",
      "     |      - If \"sqrt\", then `max_features=sqrt(n_features)` (same as \"auto\").\n",
      "     |      - If \"log2\", then `max_features=log2(n_features)`.\n",
      "     |      - If None, then `max_features=n_features`.\n",
      "     |  \n",
      "     |      Note: the search for a split does not stop until at least one\n",
      "     |      valid partition of the node samples is found, even if it requires to\n",
      "     |      effectively inspect more than ``max_features`` features.\n",
      "     |  \n",
      "     |  max_leaf_nodes : int or None, optional (default=None)\n",
      "     |      Grow trees with ``max_leaf_nodes`` in best-first fashion.\n",
      "     |      Best nodes are defined as relative reduction in impurity.\n",
      "     |      If None then unlimited number of leaf nodes.\n",
      "     |  \n",
      "     |  min_impurity_decrease : float, optional (default=0.)\n",
      "     |      A node will be split if this split induces a decrease of the impurity\n",
      "     |      greater than or equal to this value.\n",
      "     |  \n",
      "     |      The weighted impurity decrease equation is the following::\n",
      "     |  \n",
      "     |          N_t / N * (impurity - N_t_R / N_t * right_impurity\n",
      "     |                              - N_t_L / N_t * left_impurity)\n",
      "     |  \n",
      "     |      where ``N`` is the total number of samples, ``N_t`` is the number of\n",
      "     |      samples at the current node, ``N_t_L`` is the number of samples in the\n",
      "     |      left child, and ``N_t_R`` is the number of samples in the right child.\n",
      "     |  \n",
      "     |      ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,\n",
      "     |      if ``sample_weight`` is passed.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.19\n",
      "     |  \n",
      "     |  min_impurity_split : float, (default=1e-7)\n",
      "     |      Threshold for early stopping in tree growth. A node will split\n",
      "     |      if its impurity is above the threshold, otherwise it is a leaf.\n",
      "     |  \n",
      "     |      .. deprecated:: 0.19\n",
      "     |         ``min_impurity_split`` has been deprecated in favor of\n",
      "     |         ``min_impurity_decrease`` in 0.19. The default value of\n",
      "     |         ``min_impurity_split`` will change from 1e-7 to 0 in 0.23 and it\n",
      "     |         will be removed in 0.25. Use ``min_impurity_decrease`` instead.\n",
      "     |  \n",
      "     |  \n",
      "     |  bootstrap : boolean, optional (default=True)\n",
      "     |      Whether bootstrap samples are used when building trees. If False, the\n",
      "     |      whole datset is used to build each tree.\n",
      "     |  \n",
      "     |  oob_score : bool (default=False)\n",
      "     |      Whether to use out-of-bag samples to estimate\n",
      "     |      the generalization accuracy.\n",
      "     |  \n",
      "     |  n_jobs : int or None, optional (default=None)\n",
      "     |      The number of jobs to run in parallel for both `fit` and `predict`.\n",
      "     |      ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n",
      "     |      ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n",
      "     |      for more details.\n",
      "     |  \n",
      "     |  random_state : int, RandomState instance or None, optional (default=None)\n",
      "     |      If int, random_state is the seed used by the random number generator;\n",
      "     |      If RandomState instance, random_state is the random number generator;\n",
      "     |      If None, the random number generator is the RandomState instance used\n",
      "     |      by `np.random`.\n",
      "     |  \n",
      "     |  verbose : int, optional (default=0)\n",
      "     |      Controls the verbosity when fitting and predicting.\n",
      "     |  \n",
      "     |  warm_start : bool, optional (default=False)\n",
      "     |      When set to ``True``, reuse the solution of the previous call to fit\n",
      "     |      and add more estimators to the ensemble, otherwise, just fit a whole\n",
      "     |      new forest. See :term:`the Glossary <warm_start>`.\n",
      "     |  \n",
      "     |  class_weight : dict, list of dicts, \"balanced\", \"balanced_subsample\" or     None, optional (default=None)\n",
      "     |      Weights associated with classes in the form ``{class_label: weight}``.\n",
      "     |      If not given, all classes are supposed to have weight one. For\n",
      "     |      multi-output problems, a list of dicts can be provided in the same\n",
      "     |      order as the columns of y.\n",
      "     |  \n",
      "     |      Note that for multioutput (including multilabel) weights should be\n",
      "     |      defined for each class of every column in its own dict. For example,\n",
      "     |      for four-class multilabel classification weights should be\n",
      "     |      [{0: 1, 1: 1}, {0: 1, 1: 5}, {0: 1, 1: 1}, {0: 1, 1: 1}] instead of\n",
      "     |      [{1:1}, {2:5}, {3:1}, {4:1}].\n",
      "     |  \n",
      "     |      The \"balanced\" mode uses the values of y to automatically adjust\n",
      "     |      weights inversely proportional to class frequencies in the input data\n",
      "     |      as ``n_samples / (n_classes * np.bincount(y))``\n",
      "     |  \n",
      "     |      The \"balanced_subsample\" mode is the same as \"balanced\" except that\n",
      "     |      weights are computed based on the bootstrap sample for every tree\n",
      "     |      grown.\n",
      "     |  \n",
      "     |      For multi-output, the weights of each column of y will be multiplied.\n",
      "     |  \n",
      "     |      Note that these weights will be multiplied with sample_weight (passed\n",
      "     |      through the fit method) if sample_weight is specified.\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  estimators_ : list of DecisionTreeClassifier\n",
      "     |      The collection of fitted sub-estimators.\n",
      "     |  \n",
      "     |  classes_ : array of shape = [n_classes] or a list of such arrays\n",
      "     |      The classes labels (single output problem), or a list of arrays of\n",
      "     |      class labels (multi-output problem).\n",
      "     |  \n",
      "     |  n_classes_ : int or list\n",
      "     |      The number of classes (single output problem), or a list containing the\n",
      "     |      number of classes for each output (multi-output problem).\n",
      "     |  \n",
      "     |  n_features_ : int\n",
      "     |      The number of features when ``fit`` is performed.\n",
      "     |  \n",
      "     |  n_outputs_ : int\n",
      "     |      The number of outputs when ``fit`` is performed.\n",
      "     |  \n",
      "     |  feature_importances_ : array of shape = [n_features]\n",
      "     |      The feature importances (the higher, the more important the feature).\n",
      "     |  \n",
      "     |  oob_score_ : float\n",
      "     |      Score of the training dataset obtained using an out-of-bag estimate.\n",
      "     |  \n",
      "     |  oob_decision_function_ : array of shape = [n_samples, n_classes]\n",
      "     |      Decision function computed with out-of-bag estimate on the training\n",
      "     |      set. If n_estimators is small it might be possible that a data point\n",
      "     |      was never left out during the bootstrap. In this case,\n",
      "     |      `oob_decision_function_` might contain NaN.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> from sklearn.ensemble import RandomForestClassifier\n",
      "     |  >>> from sklearn.datasets import make_classification\n",
      "     |  \n",
      "     |  >>> X, y = make_classification(n_samples=1000, n_features=4,\n",
      "     |  ...                            n_informative=2, n_redundant=0,\n",
      "     |  ...                            random_state=0, shuffle=False)\n",
      "     |  >>> clf = RandomForestClassifier(n_estimators=100, max_depth=2,\n",
      "     |  ...                              random_state=0)\n",
      "     |  >>> clf.fit(X, y)  # doctest: +NORMALIZE_WHITESPACE\n",
      "     |  RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "     |              max_depth=2, max_features='auto', max_leaf_nodes=None,\n",
      "     |              min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "     |              min_samples_leaf=1, min_samples_split=2,\n",
      "     |              min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=None,\n",
      "     |              oob_score=False, random_state=0, verbose=0, warm_start=False)\n",
      "     |  >>> print(clf.feature_importances_)\n",
      "     |  [0.14205973 0.76664038 0.0282433  0.06305659]\n",
      "     |  >>> print(clf.predict([[0, 0, 0, 0]]))\n",
      "     |  [1]\n",
      "     |  \n",
      "     |  Notes\n",
      "     |  -----\n",
      "     |  The default values for the parameters controlling the size of the trees\n",
      "     |  (e.g. ``max_depth``, ``min_samples_leaf``, etc.) lead to fully grown and\n",
      "     |  unpruned trees which can potentially be very large on some data sets. To\n",
      "     |  reduce memory consumption, the complexity and size of the trees should be\n",
      "     |  controlled by setting those parameter values.\n",
      "     |  \n",
      "     |  The features are always randomly permuted at each split. Therefore,\n",
      "     |  the best found split may vary, even with the same training data,\n",
      "     |  ``max_features=n_features`` and ``bootstrap=False``, if the improvement\n",
      "     |  of the criterion is identical for several splits enumerated during the\n",
      "     |  search of the best split. To obtain a deterministic behaviour during\n",
      "     |  fitting, ``random_state`` has to be fixed.\n",
      "     |  \n",
      "     |  References\n",
      "     |  ----------\n",
      "     |  \n",
      "     |  .. [1] L. Breiman, \"Random Forests\", Machine Learning, 45(1), 5-32, 2001.\n",
      "     |  \n",
      "     |  See also\n",
      "     |  --------\n",
      "     |  DecisionTreeClassifier, ExtraTreesClassifier\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      RandomForestClassifier\n",
      "     |      ForestClassifier\n",
      "     |      BaseForest\n",
      "     |      sklearn.ensemble.base.BaseEnsemble\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      sklearn.base.MetaEstimatorMixin\n",
      "     |      sklearn.base.MultiOutputMixin\n",
      "     |      sklearn.base.ClassifierMixin\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, n_estimators='warn', criterion='gini', max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features='auto', max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, bootstrap=True, oob_score=False, n_jobs=None, random_state=None, verbose=0, warm_start=False, class_weight=None)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from ForestClassifier:\n",
      "     |  \n",
      "     |  predict(self, X)\n",
      "     |      Predict class for X.\n",
      "     |      \n",
      "     |      The predicted class of an input sample is a vote by the trees in\n",
      "     |      the forest, weighted by their probability estimates. That is,\n",
      "     |      the predicted class is the one with highest mean probability\n",
      "     |      estimate across the trees.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like or sparse matrix of shape = [n_samples, n_features]\n",
      "     |          The input samples. Internally, its dtype will be converted to\n",
      "     |          ``dtype=np.float32``. If a sparse matrix is provided, it will be\n",
      "     |          converted into a sparse ``csr_matrix``.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      y : array of shape = [n_samples] or [n_samples, n_outputs]\n",
      "     |          The predicted classes.\n",
      "     |  \n",
      "     |  predict_log_proba(self, X)\n",
      "     |      Predict class log-probabilities for X.\n",
      "     |      \n",
      "     |      The predicted class log-probabilities of an input sample is computed as\n",
      "     |      the log of the mean predicted class probabilities of the trees in the\n",
      "     |      forest.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like or sparse matrix of shape = [n_samples, n_features]\n",
      "     |          The input samples. Internally, its dtype will be converted to\n",
      "     |          ``dtype=np.float32``. If a sparse matrix is provided, it will be\n",
      "     |          converted into a sparse ``csr_matrix``.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      p : array of shape = [n_samples, n_classes], or a list of n_outputs\n",
      "     |          such arrays if n_outputs > 1.\n",
      "     |          The class probabilities of the input samples. The order of the\n",
      "     |          classes corresponds to that in the attribute `classes_`.\n",
      "     |  \n",
      "     |  predict_proba(self, X)\n",
      "     |      Predict class probabilities for X.\n",
      "     |      \n",
      "     |      The predicted class probabilities of an input sample are computed as\n",
      "     |      the mean predicted class probabilities of the trees in the forest. The\n",
      "     |      class probability of a single tree is the fraction of samples of the same\n",
      "     |      class in a leaf.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like or sparse matrix of shape = [n_samples, n_features]\n",
      "     |          The input samples. Internally, its dtype will be converted to\n",
      "     |          ``dtype=np.float32``. If a sparse matrix is provided, it will be\n",
      "     |          converted into a sparse ``csr_matrix``.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      p : array of shape = [n_samples, n_classes], or a list of n_outputs\n",
      "     |          such arrays if n_outputs > 1.\n",
      "     |          The class probabilities of the input samples. The order of the\n",
      "     |          classes corresponds to that in the attribute `classes_`.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from BaseForest:\n",
      "     |  \n",
      "     |  apply(self, X)\n",
      "     |      Apply trees in the forest to X, return leaf indices.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like or sparse matrix, shape = [n_samples, n_features]\n",
      "     |          The input samples. Internally, its dtype will be converted to\n",
      "     |          ``dtype=np.float32``. If a sparse matrix is provided, it will be\n",
      "     |          converted into a sparse ``csr_matrix``.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      X_leaves : array_like, shape = [n_samples, n_estimators]\n",
      "     |          For each datapoint x in X and for each tree in the forest,\n",
      "     |          return the index of the leaf x ends up in.\n",
      "     |  \n",
      "     |  decision_path(self, X)\n",
      "     |      Return the decision path in the forest\n",
      "     |      \n",
      "     |      .. versionadded:: 0.18\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like or sparse matrix, shape = [n_samples, n_features]\n",
      "     |          The input samples. Internally, its dtype will be converted to\n",
      "     |          ``dtype=np.float32``. If a sparse matrix is provided, it will be\n",
      "     |          converted into a sparse ``csr_matrix``.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      indicator : sparse csr array, shape = [n_samples, n_nodes]\n",
      "     |          Return a node indicator matrix where non zero elements\n",
      "     |          indicates that the samples goes through the nodes.\n",
      "     |      \n",
      "     |      n_nodes_ptr : array of size (n_estimators + 1, )\n",
      "     |          The columns from indicator[n_nodes_ptr[i]:n_nodes_ptr[i+1]]\n",
      "     |          gives the indicator value for the i-th estimator.\n",
      "     |  \n",
      "     |  fit(self, X, y, sample_weight=None)\n",
      "     |      Build a forest of trees from the training set (X, y).\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like or sparse matrix of shape = [n_samples, n_features]\n",
      "     |          The training input samples. Internally, its dtype will be converted\n",
      "     |          to ``dtype=np.float32``. If a sparse matrix is provided, it will be\n",
      "     |          converted into a sparse ``csc_matrix``.\n",
      "     |      \n",
      "     |      y : array-like, shape = [n_samples] or [n_samples, n_outputs]\n",
      "     |          The target values (class labels in classification, real numbers in\n",
      "     |          regression).\n",
      "     |      \n",
      "     |      sample_weight : array-like, shape = [n_samples] or None\n",
      "     |          Sample weights. If None, then samples are equally weighted. Splits\n",
      "     |          that would create child nodes with net zero or negative weight are\n",
      "     |          ignored while searching for a split in each node. In the case of\n",
      "     |          classification, splits are also ignored if they would result in any\n",
      "     |          single class carrying a negative weight in either child node.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from BaseForest:\n",
      "     |  \n",
      "     |  feature_importances_\n",
      "     |      Return the feature importances (the higher, the more important the\n",
      "     |         feature).\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      feature_importances_ : array, shape = [n_features]\n",
      "     |          The values of this array sum to 1, unless all trees are single node\n",
      "     |          trees consisting of only the root node, in which case it will be an\n",
      "     |          array of zeros.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.ensemble.base.BaseEnsemble:\n",
      "     |  \n",
      "     |  __getitem__(self, index)\n",
      "     |      Returns the index'th estimator in the ensemble.\n",
      "     |  \n",
      "     |  __iter__(self)\n",
      "     |      Returns iterator over estimators in the ensemble.\n",
      "     |  \n",
      "     |  __len__(self)\n",
      "     |      Returns the number of estimators in the ensemble.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __repr__(self, N_CHAR_MAX=700)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : boolean, optional\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : mapping of string to any\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as pipelines). The latter have parameters of the form\n",
      "     |      ``<component>__<parameter>`` so that it's possible to update each\n",
      "     |      component of a nested object.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.ClassifierMixin:\n",
      "     |  \n",
      "     |  score(self, X, y, sample_weight=None)\n",
      "     |      Returns the mean accuracy on the given test data and labels.\n",
      "     |      \n",
      "     |      In multi-label classification, this is the subset accuracy\n",
      "     |      which is a harsh metric since you require for each sample that\n",
      "     |      each label set be correctly predicted.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like, shape = (n_samples, n_features)\n",
      "     |          Test samples.\n",
      "     |      \n",
      "     |      y : array-like, shape = (n_samples) or (n_samples, n_outputs)\n",
      "     |          True labels for X.\n",
      "     |      \n",
      "     |      sample_weight : array-like, shape = [n_samples], optional\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      score : float\n",
      "     |          Mean accuracy of self.predict(X) wrt. y.\n",
      "    \n",
      "    class RandomForestRegressor(ForestRegressor)\n",
      "     |  RandomForestRegressor(n_estimators='warn', criterion='mse', max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features='auto', max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, bootstrap=True, oob_score=False, n_jobs=None, random_state=None, verbose=0, warm_start=False)\n",
      "     |  \n",
      "     |  A random forest regressor.\n",
      "     |  \n",
      "     |  A random forest is a meta estimator that fits a number of classifying\n",
      "     |  decision trees on various sub-samples of the dataset and uses averaging\n",
      "     |  to improve the predictive accuracy and control over-fitting.\n",
      "     |  The sub-sample size is always the same as the original\n",
      "     |  input sample size but the samples are drawn with replacement if\n",
      "     |  `bootstrap=True` (default).\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <forest>`.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  n_estimators : integer, optional (default=10)\n",
      "     |      The number of trees in the forest.\n",
      "     |  \n",
      "     |      .. versionchanged:: 0.20\n",
      "     |         The default value of ``n_estimators`` will change from 10 in\n",
      "     |         version 0.20 to 100 in version 0.22.\n",
      "     |  \n",
      "     |  criterion : string, optional (default=\"mse\")\n",
      "     |      The function to measure the quality of a split. Supported criteria\n",
      "     |      are \"mse\" for the mean squared error, which is equal to variance\n",
      "     |      reduction as feature selection criterion, and \"mae\" for the mean\n",
      "     |      absolute error.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.18\n",
      "     |         Mean Absolute Error (MAE) criterion.\n",
      "     |  \n",
      "     |  max_depth : integer or None, optional (default=None)\n",
      "     |      The maximum depth of the tree. If None, then nodes are expanded until\n",
      "     |      all leaves are pure or until all leaves contain less than\n",
      "     |      min_samples_split samples.\n",
      "     |  \n",
      "     |  min_samples_split : int, float, optional (default=2)\n",
      "     |      The minimum number of samples required to split an internal node:\n",
      "     |  \n",
      "     |      - If int, then consider `min_samples_split` as the minimum number.\n",
      "     |      - If float, then `min_samples_split` is a fraction and\n",
      "     |        `ceil(min_samples_split * n_samples)` are the minimum\n",
      "     |        number of samples for each split.\n",
      "     |  \n",
      "     |      .. versionchanged:: 0.18\n",
      "     |         Added float values for fractions.\n",
      "     |  \n",
      "     |  min_samples_leaf : int, float, optional (default=1)\n",
      "     |      The minimum number of samples required to be at a leaf node.\n",
      "     |      A split point at any depth will only be considered if it leaves at\n",
      "     |      least ``min_samples_leaf`` training samples in each of the left and\n",
      "     |      right branches.  This may have the effect of smoothing the model,\n",
      "     |      especially in regression.\n",
      "     |  \n",
      "     |      - If int, then consider `min_samples_leaf` as the minimum number.\n",
      "     |      - If float, then `min_samples_leaf` is a fraction and\n",
      "     |        `ceil(min_samples_leaf * n_samples)` are the minimum\n",
      "     |        number of samples for each node.\n",
      "     |  \n",
      "     |      .. versionchanged:: 0.18\n",
      "     |         Added float values for fractions.\n",
      "     |  \n",
      "     |  min_weight_fraction_leaf : float, optional (default=0.)\n",
      "     |      The minimum weighted fraction of the sum total of weights (of all\n",
      "     |      the input samples) required to be at a leaf node. Samples have\n",
      "     |      equal weight when sample_weight is not provided.\n",
      "     |  \n",
      "     |  max_features : int, float, string or None, optional (default=\"auto\")\n",
      "     |      The number of features to consider when looking for the best split:\n",
      "     |  \n",
      "     |      - If int, then consider `max_features` features at each split.\n",
      "     |      - If float, then `max_features` is a fraction and\n",
      "     |        `int(max_features * n_features)` features are considered at each\n",
      "     |        split.\n",
      "     |      - If \"auto\", then `max_features=n_features`.\n",
      "     |      - If \"sqrt\", then `max_features=sqrt(n_features)`.\n",
      "     |      - If \"log2\", then `max_features=log2(n_features)`.\n",
      "     |      - If None, then `max_features=n_features`.\n",
      "     |  \n",
      "     |      Note: the search for a split does not stop until at least one\n",
      "     |      valid partition of the node samples is found, even if it requires to\n",
      "     |      effectively inspect more than ``max_features`` features.\n",
      "     |  \n",
      "     |  max_leaf_nodes : int or None, optional (default=None)\n",
      "     |      Grow trees with ``max_leaf_nodes`` in best-first fashion.\n",
      "     |      Best nodes are defined as relative reduction in impurity.\n",
      "     |      If None then unlimited number of leaf nodes.\n",
      "     |  \n",
      "     |  min_impurity_decrease : float, optional (default=0.)\n",
      "     |      A node will be split if this split induces a decrease of the impurity\n",
      "     |      greater than or equal to this value.\n",
      "     |  \n",
      "     |      The weighted impurity decrease equation is the following::\n",
      "     |  \n",
      "     |          N_t / N * (impurity - N_t_R / N_t * right_impurity\n",
      "     |                              - N_t_L / N_t * left_impurity)\n",
      "     |  \n",
      "     |      where ``N`` is the total number of samples, ``N_t`` is the number of\n",
      "     |      samples at the current node, ``N_t_L`` is the number of samples in the\n",
      "     |      left child, and ``N_t_R`` is the number of samples in the right child.\n",
      "     |  \n",
      "     |      ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,\n",
      "     |      if ``sample_weight`` is passed.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.19\n",
      "     |  \n",
      "     |  min_impurity_split : float, (default=1e-7)\n",
      "     |      Threshold for early stopping in tree growth. A node will split\n",
      "     |      if its impurity is above the threshold, otherwise it is a leaf.\n",
      "     |  \n",
      "     |      .. deprecated:: 0.19\n",
      "     |         ``min_impurity_split`` has been deprecated in favor of\n",
      "     |         ``min_impurity_decrease`` in 0.19. The default value of\n",
      "     |         ``min_impurity_split`` will change from 1e-7 to 0 in 0.23 and it\n",
      "     |         will be removed in 0.25. Use ``min_impurity_decrease`` instead.\n",
      "     |  \n",
      "     |  bootstrap : boolean, optional (default=True)\n",
      "     |      Whether bootstrap samples are used when building trees. If False, the\n",
      "     |      whole datset is used to build each tree.\n",
      "     |  \n",
      "     |  oob_score : bool, optional (default=False)\n",
      "     |      whether to use out-of-bag samples to estimate\n",
      "     |      the R^2 on unseen data.\n",
      "     |  \n",
      "     |  n_jobs : int or None, optional (default=None)\n",
      "     |      The number of jobs to run in parallel for both `fit` and `predict`.\n",
      "     |      `None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n",
      "     |      ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n",
      "     |      for more details.\n",
      "     |  \n",
      "     |  random_state : int, RandomState instance or None, optional (default=None)\n",
      "     |      If int, random_state is the seed used by the random number generator;\n",
      "     |      If RandomState instance, random_state is the random number generator;\n",
      "     |      If None, the random number generator is the RandomState instance used\n",
      "     |      by `np.random`.\n",
      "     |  \n",
      "     |  verbose : int, optional (default=0)\n",
      "     |      Controls the verbosity when fitting and predicting.\n",
      "     |  \n",
      "     |  warm_start : bool, optional (default=False)\n",
      "     |      When set to ``True``, reuse the solution of the previous call to fit\n",
      "     |      and add more estimators to the ensemble, otherwise, just fit a whole\n",
      "     |      new forest. See :term:`the Glossary <warm_start>`.\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  estimators_ : list of DecisionTreeRegressor\n",
      "     |      The collection of fitted sub-estimators.\n",
      "     |  \n",
      "     |  feature_importances_ : array of shape = [n_features]\n",
      "     |      The feature importances (the higher, the more important the feature).\n",
      "     |  \n",
      "     |  n_features_ : int\n",
      "     |      The number of features when ``fit`` is performed.\n",
      "     |  \n",
      "     |  n_outputs_ : int\n",
      "     |      The number of outputs when ``fit`` is performed.\n",
      "     |  \n",
      "     |  oob_score_ : float\n",
      "     |      Score of the training dataset obtained using an out-of-bag estimate.\n",
      "     |  \n",
      "     |  oob_prediction_ : array of shape = [n_samples]\n",
      "     |      Prediction computed with out-of-bag estimate on the training set.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> from sklearn.ensemble import RandomForestRegressor\n",
      "     |  >>> from sklearn.datasets import make_regression\n",
      "     |  \n",
      "     |  >>> X, y = make_regression(n_features=4, n_informative=2,\n",
      "     |  ...                        random_state=0, shuffle=False)\n",
      "     |  >>> regr = RandomForestRegressor(max_depth=2, random_state=0,\n",
      "     |  ...                              n_estimators=100)\n",
      "     |  >>> regr.fit(X, y)  # doctest: +NORMALIZE_WHITESPACE\n",
      "     |  RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=2,\n",
      "     |             max_features='auto', max_leaf_nodes=None,\n",
      "     |             min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "     |             min_samples_leaf=1, min_samples_split=2,\n",
      "     |             min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=None,\n",
      "     |             oob_score=False, random_state=0, verbose=0, warm_start=False)\n",
      "     |  >>> print(regr.feature_importances_)\n",
      "     |  [0.18146984 0.81473937 0.00145312 0.00233767]\n",
      "     |  >>> print(regr.predict([[0, 0, 0, 0]]))\n",
      "     |  [-8.32987858]\n",
      "     |  \n",
      "     |  Notes\n",
      "     |  -----\n",
      "     |  The default values for the parameters controlling the size of the trees\n",
      "     |  (e.g. ``max_depth``, ``min_samples_leaf``, etc.) lead to fully grown and\n",
      "     |  unpruned trees which can potentially be very large on some data sets. To\n",
      "     |  reduce memory consumption, the complexity and size of the trees should be\n",
      "     |  controlled by setting those parameter values.\n",
      "     |  \n",
      "     |  The features are always randomly permuted at each split. Therefore,\n",
      "     |  the best found split may vary, even with the same training data,\n",
      "     |  ``max_features=n_features`` and ``bootstrap=False``, if the improvement\n",
      "     |  of the criterion is identical for several splits enumerated during the\n",
      "     |  search of the best split. To obtain a deterministic behaviour during\n",
      "     |  fitting, ``random_state`` has to be fixed.\n",
      "     |  \n",
      "     |  The default value ``max_features=\"auto\"`` uses ``n_features``\n",
      "     |  rather than ``n_features / 3``. The latter was originally suggested in\n",
      "     |  [1], whereas the former was more recently justified empirically in [2].\n",
      "     |  \n",
      "     |  References\n",
      "     |  ----------\n",
      "     |  \n",
      "     |  .. [1] L. Breiman, \"Random Forests\", Machine Learning, 45(1), 5-32, 2001.\n",
      "     |  \n",
      "     |  .. [2] P. Geurts, D. Ernst., and L. Wehenkel, \"Extremely randomized\n",
      "     |         trees\", Machine Learning, 63(1), 3-42, 2006.\n",
      "     |  \n",
      "     |  See also\n",
      "     |  --------\n",
      "     |  DecisionTreeRegressor, ExtraTreesRegressor\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      RandomForestRegressor\n",
      "     |      ForestRegressor\n",
      "     |      BaseForest\n",
      "     |      sklearn.ensemble.base.BaseEnsemble\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      sklearn.base.MetaEstimatorMixin\n",
      "     |      sklearn.base.MultiOutputMixin\n",
      "     |      sklearn.base.RegressorMixin\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, n_estimators='warn', criterion='mse', max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features='auto', max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, bootstrap=True, oob_score=False, n_jobs=None, random_state=None, verbose=0, warm_start=False)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from ForestRegressor:\n",
      "     |  \n",
      "     |  predict(self, X)\n",
      "     |      Predict regression target for X.\n",
      "     |      \n",
      "     |      The predicted regression target of an input sample is computed as the\n",
      "     |      mean predicted regression targets of the trees in the forest.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like or sparse matrix of shape = [n_samples, n_features]\n",
      "     |          The input samples. Internally, its dtype will be converted to\n",
      "     |          ``dtype=np.float32``. If a sparse matrix is provided, it will be\n",
      "     |          converted into a sparse ``csr_matrix``.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      y : array of shape = [n_samples] or [n_samples, n_outputs]\n",
      "     |          The predicted values.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from BaseForest:\n",
      "     |  \n",
      "     |  apply(self, X)\n",
      "     |      Apply trees in the forest to X, return leaf indices.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like or sparse matrix, shape = [n_samples, n_features]\n",
      "     |          The input samples. Internally, its dtype will be converted to\n",
      "     |          ``dtype=np.float32``. If a sparse matrix is provided, it will be\n",
      "     |          converted into a sparse ``csr_matrix``.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      X_leaves : array_like, shape = [n_samples, n_estimators]\n",
      "     |          For each datapoint x in X and for each tree in the forest,\n",
      "     |          return the index of the leaf x ends up in.\n",
      "     |  \n",
      "     |  decision_path(self, X)\n",
      "     |      Return the decision path in the forest\n",
      "     |      \n",
      "     |      .. versionadded:: 0.18\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like or sparse matrix, shape = [n_samples, n_features]\n",
      "     |          The input samples. Internally, its dtype will be converted to\n",
      "     |          ``dtype=np.float32``. If a sparse matrix is provided, it will be\n",
      "     |          converted into a sparse ``csr_matrix``.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      indicator : sparse csr array, shape = [n_samples, n_nodes]\n",
      "     |          Return a node indicator matrix where non zero elements\n",
      "     |          indicates that the samples goes through the nodes.\n",
      "     |      \n",
      "     |      n_nodes_ptr : array of size (n_estimators + 1, )\n",
      "     |          The columns from indicator[n_nodes_ptr[i]:n_nodes_ptr[i+1]]\n",
      "     |          gives the indicator value for the i-th estimator.\n",
      "     |  \n",
      "     |  fit(self, X, y, sample_weight=None)\n",
      "     |      Build a forest of trees from the training set (X, y).\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like or sparse matrix of shape = [n_samples, n_features]\n",
      "     |          The training input samples. Internally, its dtype will be converted\n",
      "     |          to ``dtype=np.float32``. If a sparse matrix is provided, it will be\n",
      "     |          converted into a sparse ``csc_matrix``.\n",
      "     |      \n",
      "     |      y : array-like, shape = [n_samples] or [n_samples, n_outputs]\n",
      "     |          The target values (class labels in classification, real numbers in\n",
      "     |          regression).\n",
      "     |      \n",
      "     |      sample_weight : array-like, shape = [n_samples] or None\n",
      "     |          Sample weights. If None, then samples are equally weighted. Splits\n",
      "     |          that would create child nodes with net zero or negative weight are\n",
      "     |          ignored while searching for a split in each node. In the case of\n",
      "     |          classification, splits are also ignored if they would result in any\n",
      "     |          single class carrying a negative weight in either child node.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from BaseForest:\n",
      "     |  \n",
      "     |  feature_importances_\n",
      "     |      Return the feature importances (the higher, the more important the\n",
      "     |         feature).\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      feature_importances_ : array, shape = [n_features]\n",
      "     |          The values of this array sum to 1, unless all trees are single node\n",
      "     |          trees consisting of only the root node, in which case it will be an\n",
      "     |          array of zeros.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.ensemble.base.BaseEnsemble:\n",
      "     |  \n",
      "     |  __getitem__(self, index)\n",
      "     |      Returns the index'th estimator in the ensemble.\n",
      "     |  \n",
      "     |  __iter__(self)\n",
      "     |      Returns iterator over estimators in the ensemble.\n",
      "     |  \n",
      "     |  __len__(self)\n",
      "     |      Returns the number of estimators in the ensemble.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __repr__(self, N_CHAR_MAX=700)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : boolean, optional\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : mapping of string to any\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as pipelines). The latter have parameters of the form\n",
      "     |      ``<component>__<parameter>`` so that it's possible to update each\n",
      "     |      component of a nested object.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.RegressorMixin:\n",
      "     |  \n",
      "     |  score(self, X, y, sample_weight=None)\n",
      "     |      Returns the coefficient of determination R^2 of the prediction.\n",
      "     |      \n",
      "     |      The coefficient R^2 is defined as (1 - u/v), where u is the residual\n",
      "     |      sum of squares ((y_true - y_pred) ** 2).sum() and v is the total\n",
      "     |      sum of squares ((y_true - y_true.mean()) ** 2).sum().\n",
      "     |      The best possible score is 1.0 and it can be negative (because the\n",
      "     |      model can be arbitrarily worse). A constant model that always\n",
      "     |      predicts the expected value of y, disregarding the input features,\n",
      "     |      would get a R^2 score of 0.0.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like, shape = (n_samples, n_features)\n",
      "     |          Test samples. For some estimators this may be a\n",
      "     |          precomputed kernel matrix instead, shape = (n_samples,\n",
      "     |          n_samples_fitted], where n_samples_fitted is the number of\n",
      "     |          samples used in the fitting for the estimator.\n",
      "     |      \n",
      "     |      y : array-like, shape = (n_samples) or (n_samples, n_outputs)\n",
      "     |          True values for X.\n",
      "     |      \n",
      "     |      sample_weight : array-like, shape = [n_samples], optional\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      score : float\n",
      "     |          R^2 of self.predict(X) wrt. y.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      The R2 score used when calling ``score`` on a regressor will use\n",
      "     |      ``multioutput='uniform_average'`` from version 0.23 to keep consistent\n",
      "     |      with `metrics.r2_score`. This will influence the ``score`` method of\n",
      "     |      all the multioutput regressors (except for\n",
      "     |      `multioutput.MultiOutputRegressor`). To specify the default value\n",
      "     |      manually and avoid the warning, please either call `metrics.r2_score`\n",
      "     |      directly or make a custom scorer with `metrics.make_scorer` (the\n",
      "     |      built-in scorer ``'r2'`` uses ``multioutput='uniform_average'``).\n",
      "    \n",
      "    class RandomTreesEmbedding(BaseForest)\n",
      "     |  RandomTreesEmbedding(n_estimators='warn', max_depth=5, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, sparse_output=True, n_jobs=None, random_state=None, verbose=0, warm_start=False)\n",
      "     |  \n",
      "     |  An ensemble of totally random trees.\n",
      "     |  \n",
      "     |  An unsupervised transformation of a dataset to a high-dimensional\n",
      "     |  sparse representation. A datapoint is coded according to which leaf of\n",
      "     |  each tree it is sorted into. Using a one-hot encoding of the leaves,\n",
      "     |  this leads to a binary coding with as many ones as there are trees in\n",
      "     |  the forest.\n",
      "     |  \n",
      "     |  The dimensionality of the resulting representation is\n",
      "     |  ``n_out <= n_estimators * max_leaf_nodes``. If ``max_leaf_nodes == None``,\n",
      "     |  the number of leaf nodes is at most ``n_estimators * 2 ** max_depth``.\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <random_trees_embedding>`.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  n_estimators : integer, optional (default=10)\n",
      "     |      Number of trees in the forest.\n",
      "     |  \n",
      "     |      .. versionchanged:: 0.20\n",
      "     |         The default value of ``n_estimators`` will change from 10 in\n",
      "     |         version 0.20 to 100 in version 0.22.\n",
      "     |  \n",
      "     |  max_depth : integer, optional (default=5)\n",
      "     |      The maximum depth of each tree. If None, then nodes are expanded until\n",
      "     |      all leaves are pure or until all leaves contain less than\n",
      "     |      min_samples_split samples.\n",
      "     |  \n",
      "     |  min_samples_split : int, float, optional (default=2)\n",
      "     |      The minimum number of samples required to split an internal node:\n",
      "     |  \n",
      "     |      - If int, then consider `min_samples_split` as the minimum number.\n",
      "     |      - If float, then `min_samples_split` is a fraction and\n",
      "     |        `ceil(min_samples_split * n_samples)` is the minimum\n",
      "     |        number of samples for each split.\n",
      "     |  \n",
      "     |      .. versionchanged:: 0.18\n",
      "     |         Added float values for fractions.\n",
      "     |  \n",
      "     |  min_samples_leaf : int, float, optional (default=1)\n",
      "     |      The minimum number of samples required to be at a leaf node.\n",
      "     |      A split point at any depth will only be considered if it leaves at\n",
      "     |      least ``min_samples_leaf`` training samples in each of the left and\n",
      "     |      right branches.  This may have the effect of smoothing the model,\n",
      "     |      especially in regression.\n",
      "     |  \n",
      "     |      - If int, then consider `min_samples_leaf` as the minimum number.\n",
      "     |      - If float, then `min_samples_leaf` is a fraction and\n",
      "     |        `ceil(min_samples_leaf * n_samples)` is the minimum\n",
      "     |        number of samples for each node.\n",
      "     |  \n",
      "     |      .. versionchanged:: 0.18\n",
      "     |         Added float values for fractions.\n",
      "     |  \n",
      "     |  min_weight_fraction_leaf : float, optional (default=0.)\n",
      "     |      The minimum weighted fraction of the sum total of weights (of all\n",
      "     |      the input samples) required to be at a leaf node. Samples have\n",
      "     |      equal weight when sample_weight is not provided.\n",
      "     |  \n",
      "     |  max_leaf_nodes : int or None, optional (default=None)\n",
      "     |      Grow trees with ``max_leaf_nodes`` in best-first fashion.\n",
      "     |      Best nodes are defined as relative reduction in impurity.\n",
      "     |      If None then unlimited number of leaf nodes.\n",
      "     |  \n",
      "     |  min_impurity_decrease : float, optional (default=0.)\n",
      "     |      A node will be split if this split induces a decrease of the impurity\n",
      "     |      greater than or equal to this value.\n",
      "     |  \n",
      "     |      The weighted impurity decrease equation is the following::\n",
      "     |  \n",
      "     |          N_t / N * (impurity - N_t_R / N_t * right_impurity\n",
      "     |                              - N_t_L / N_t * left_impurity)\n",
      "     |  \n",
      "     |      where ``N`` is the total number of samples, ``N_t`` is the number of\n",
      "     |      samples at the current node, ``N_t_L`` is the number of samples in the\n",
      "     |      left child, and ``N_t_R`` is the number of samples in the right child.\n",
      "     |  \n",
      "     |      ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,\n",
      "     |      if ``sample_weight`` is passed.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.19\n",
      "     |  \n",
      "     |  min_impurity_split : float, (default=1e-7)\n",
      "     |      Threshold for early stopping in tree growth. A node will split\n",
      "     |      if its impurity is above the threshold, otherwise it is a leaf.\n",
      "     |  \n",
      "     |      .. deprecated:: 0.19\n",
      "     |         ``min_impurity_split`` has been deprecated in favor of\n",
      "     |         ``min_impurity_decrease`` in 0.19. The default value of\n",
      "     |         ``min_impurity_split`` will change from 1e-7 to 0 in 0.23 and it\n",
      "     |         will be removed in 0.25. Use ``min_impurity_decrease`` instead.\n",
      "     |  \n",
      "     |  sparse_output : bool, optional (default=True)\n",
      "     |      Whether or not to return a sparse CSR matrix, as default behavior,\n",
      "     |      or to return a dense array compatible with dense pipeline operators.\n",
      "     |  \n",
      "     |  n_jobs : int or None, optional (default=None)\n",
      "     |      The number of jobs to run in parallel for both `fit` and `predict`.\n",
      "     |      ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n",
      "     |      ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n",
      "     |      for more details.\n",
      "     |  \n",
      "     |  random_state : int, RandomState instance or None, optional (default=None)\n",
      "     |      If int, random_state is the seed used by the random number generator;\n",
      "     |      If RandomState instance, random_state is the random number generator;\n",
      "     |      If None, the random number generator is the RandomState instance used\n",
      "     |      by `np.random`.\n",
      "     |  \n",
      "     |  verbose : int, optional (default=0)\n",
      "     |      Controls the verbosity when fitting and predicting.\n",
      "     |  \n",
      "     |  warm_start : bool, optional (default=False)\n",
      "     |      When set to ``True``, reuse the solution of the previous call to fit\n",
      "     |      and add more estimators to the ensemble, otherwise, just fit a whole\n",
      "     |      new forest. See :term:`the Glossary <warm_start>`.\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  estimators_ : list of DecisionTreeClassifier\n",
      "     |      The collection of fitted sub-estimators.\n",
      "     |  \n",
      "     |  References\n",
      "     |  ----------\n",
      "     |  .. [1] P. Geurts, D. Ernst., and L. Wehenkel, \"Extremely randomized trees\",\n",
      "     |         Machine Learning, 63(1), 3-42, 2006.\n",
      "     |  .. [2] Moosmann, F. and Triggs, B. and Jurie, F.  \"Fast discriminative\n",
      "     |         visual codebooks using randomized clustering forests\"\n",
      "     |         NIPS 2007\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      RandomTreesEmbedding\n",
      "     |      BaseForest\n",
      "     |      sklearn.ensemble.base.BaseEnsemble\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      sklearn.base.MetaEstimatorMixin\n",
      "     |      sklearn.base.MultiOutputMixin\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, n_estimators='warn', max_depth=5, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, sparse_output=True, n_jobs=None, random_state=None, verbose=0, warm_start=False)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  fit(self, X, y=None, sample_weight=None)\n",
      "     |      Fit estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like or sparse matrix, shape=(n_samples, n_features)\n",
      "     |          The input samples. Use ``dtype=np.float32`` for maximum\n",
      "     |          efficiency. Sparse matrices are also supported, use sparse\n",
      "     |          ``csc_matrix`` for maximum efficiency.\n",
      "     |      \n",
      "     |      sample_weight : array-like, shape = [n_samples] or None\n",
      "     |          Sample weights. If None, then samples are equally weighted. Splits\n",
      "     |          that would create child nodes with net zero or negative weight are\n",
      "     |          ignored while searching for a split in each node. In the case of\n",
      "     |          classification, splits are also ignored if they would result in any\n",
      "     |          single class carrying a negative weight in either child node.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |  \n",
      "     |  fit_transform(self, X, y=None, sample_weight=None)\n",
      "     |      Fit estimator and transform dataset.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like or sparse matrix, shape=(n_samples, n_features)\n",
      "     |          Input data used to build forests. Use ``dtype=np.float32`` for\n",
      "     |          maximum efficiency.\n",
      "     |      \n",
      "     |      sample_weight : array-like, shape = [n_samples] or None\n",
      "     |          Sample weights. If None, then samples are equally weighted. Splits\n",
      "     |          that would create child nodes with net zero or negative weight are\n",
      "     |          ignored while searching for a split in each node. In the case of\n",
      "     |          classification, splits are also ignored if they would result in any\n",
      "     |          single class carrying a negative weight in either child node.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      X_transformed : sparse matrix, shape=(n_samples, n_out)\n",
      "     |          Transformed dataset.\n",
      "     |  \n",
      "     |  transform(self, X)\n",
      "     |      Transform dataset.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like or sparse matrix, shape=(n_samples, n_features)\n",
      "     |          Input data to be transformed. Use ``dtype=np.float32`` for maximum\n",
      "     |          efficiency. Sparse matrices are also supported, use sparse\n",
      "     |          ``csr_matrix`` for maximum efficiency.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      X_transformed : sparse matrix, shape=(n_samples, n_out)\n",
      "     |          Transformed dataset.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  criterion = 'mse'\n",
      "     |  \n",
      "     |  max_features = 1\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from BaseForest:\n",
      "     |  \n",
      "     |  apply(self, X)\n",
      "     |      Apply trees in the forest to X, return leaf indices.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like or sparse matrix, shape = [n_samples, n_features]\n",
      "     |          The input samples. Internally, its dtype will be converted to\n",
      "     |          ``dtype=np.float32``. If a sparse matrix is provided, it will be\n",
      "     |          converted into a sparse ``csr_matrix``.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      X_leaves : array_like, shape = [n_samples, n_estimators]\n",
      "     |          For each datapoint x in X and for each tree in the forest,\n",
      "     |          return the index of the leaf x ends up in.\n",
      "     |  \n",
      "     |  decision_path(self, X)\n",
      "     |      Return the decision path in the forest\n",
      "     |      \n",
      "     |      .. versionadded:: 0.18\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like or sparse matrix, shape = [n_samples, n_features]\n",
      "     |          The input samples. Internally, its dtype will be converted to\n",
      "     |          ``dtype=np.float32``. If a sparse matrix is provided, it will be\n",
      "     |          converted into a sparse ``csr_matrix``.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      indicator : sparse csr array, shape = [n_samples, n_nodes]\n",
      "     |          Return a node indicator matrix where non zero elements\n",
      "     |          indicates that the samples goes through the nodes.\n",
      "     |      \n",
      "     |      n_nodes_ptr : array of size (n_estimators + 1, )\n",
      "     |          The columns from indicator[n_nodes_ptr[i]:n_nodes_ptr[i+1]]\n",
      "     |          gives the indicator value for the i-th estimator.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from BaseForest:\n",
      "     |  \n",
      "     |  feature_importances_\n",
      "     |      Return the feature importances (the higher, the more important the\n",
      "     |         feature).\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      feature_importances_ : array, shape = [n_features]\n",
      "     |          The values of this array sum to 1, unless all trees are single node\n",
      "     |          trees consisting of only the root node, in which case it will be an\n",
      "     |          array of zeros.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.ensemble.base.BaseEnsemble:\n",
      "     |  \n",
      "     |  __getitem__(self, index)\n",
      "     |      Returns the index'th estimator in the ensemble.\n",
      "     |  \n",
      "     |  __iter__(self)\n",
      "     |      Returns iterator over estimators in the ensemble.\n",
      "     |  \n",
      "     |  __len__(self)\n",
      "     |      Returns the number of estimators in the ensemble.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __repr__(self, N_CHAR_MAX=700)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : boolean, optional\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : mapping of string to any\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as pipelines). The latter have parameters of the form\n",
      "     |      ``<component>__<parameter>`` so that it's possible to update each\n",
      "     |      component of a nested object.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class VotingClassifier(_BaseVoting, sklearn.base.ClassifierMixin)\n",
      "     |  VotingClassifier(estimators, voting='hard', weights=None, n_jobs=None, flatten_transform=True)\n",
      "     |  \n",
      "     |  Soft Voting/Majority Rule classifier for unfitted estimators.\n",
      "     |  \n",
      "     |  .. versionadded:: 0.17\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <voting_classifier>`.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  estimators : list of (string, estimator) tuples\n",
      "     |      Invoking the ``fit`` method on the ``VotingClassifier`` will fit clones\n",
      "     |      of those original estimators that will be stored in the class attribute\n",
      "     |      ``self.estimators_``. An estimator can be set to ``None`` or ``'drop'``\n",
      "     |      using ``set_params``.\n",
      "     |  \n",
      "     |  voting : str, {'hard', 'soft'} (default='hard')\n",
      "     |      If 'hard', uses predicted class labels for majority rule voting.\n",
      "     |      Else if 'soft', predicts the class label based on the argmax of\n",
      "     |      the sums of the predicted probabilities, which is recommended for\n",
      "     |      an ensemble of well-calibrated classifiers.\n",
      "     |  \n",
      "     |  weights : array-like, shape (n_classifiers,), optional (default=`None`)\n",
      "     |      Sequence of weights (`float` or `int`) to weight the occurrences of\n",
      "     |      predicted class labels (`hard` voting) or class probabilities\n",
      "     |      before averaging (`soft` voting). Uses uniform weights if `None`.\n",
      "     |  \n",
      "     |  n_jobs : int or None, optional (default=None)\n",
      "     |      The number of jobs to run in parallel for ``fit``.\n",
      "     |      ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n",
      "     |      ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n",
      "     |      for more details.\n",
      "     |  \n",
      "     |  flatten_transform : bool, optional (default=True)\n",
      "     |      Affects shape of transform output only when voting='soft'\n",
      "     |      If voting='soft' and flatten_transform=True, transform method returns\n",
      "     |      matrix with shape (n_samples, n_classifiers * n_classes). If\n",
      "     |      flatten_transform=False, it returns\n",
      "     |      (n_classifiers, n_samples, n_classes).\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  estimators_ : list of classifiers\n",
      "     |      The collection of fitted sub-estimators as defined in ``estimators``\n",
      "     |      that are not `None`.\n",
      "     |  \n",
      "     |  named_estimators_ : Bunch object, a dictionary with attribute access\n",
      "     |      Attribute to access any fitted sub-estimators by name.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.20\n",
      "     |  \n",
      "     |  classes_ : array-like, shape (n_predictions,)\n",
      "     |      The classes labels.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> import numpy as np\n",
      "     |  >>> from sklearn.linear_model import LogisticRegression\n",
      "     |  >>> from sklearn.naive_bayes import GaussianNB\n",
      "     |  >>> from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
      "     |  >>> clf1 = LogisticRegression(solver='lbfgs', multi_class='multinomial',\n",
      "     |  ...                           random_state=1)\n",
      "     |  >>> clf2 = RandomForestClassifier(n_estimators=50, random_state=1)\n",
      "     |  >>> clf3 = GaussianNB()\n",
      "     |  >>> X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])\n",
      "     |  >>> y = np.array([1, 1, 1, 2, 2, 2])\n",
      "     |  >>> eclf1 = VotingClassifier(estimators=[\n",
      "     |  ...         ('lr', clf1), ('rf', clf2), ('gnb', clf3)], voting='hard')\n",
      "     |  >>> eclf1 = eclf1.fit(X, y)\n",
      "     |  >>> print(eclf1.predict(X))\n",
      "     |  [1 1 1 2 2 2]\n",
      "     |  >>> np.array_equal(eclf1.named_estimators_.lr.predict(X),\n",
      "     |  ...                eclf1.named_estimators_['lr'].predict(X))\n",
      "     |  True\n",
      "     |  >>> eclf2 = VotingClassifier(estimators=[\n",
      "     |  ...         ('lr', clf1), ('rf', clf2), ('gnb', clf3)],\n",
      "     |  ...         voting='soft')\n",
      "     |  >>> eclf2 = eclf2.fit(X, y)\n",
      "     |  >>> print(eclf2.predict(X))\n",
      "     |  [1 1 1 2 2 2]\n",
      "     |  >>> eclf3 = VotingClassifier(estimators=[\n",
      "     |  ...        ('lr', clf1), ('rf', clf2), ('gnb', clf3)],\n",
      "     |  ...        voting='soft', weights=[2,1,1],\n",
      "     |  ...        flatten_transform=True)\n",
      "     |  >>> eclf3 = eclf3.fit(X, y)\n",
      "     |  >>> print(eclf3.predict(X))\n",
      "     |  [1 1 1 2 2 2]\n",
      "     |  >>> print(eclf3.transform(X).shape)\n",
      "     |  (6, 6)\n",
      "     |  \n",
      "     |  See also\n",
      "     |  --------\n",
      "     |  VotingRegressor: Prediction voting regressor.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      VotingClassifier\n",
      "     |      _BaseVoting\n",
      "     |      sklearn.utils.metaestimators._BaseComposition\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      sklearn.base.TransformerMixin\n",
      "     |      sklearn.base.ClassifierMixin\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, estimators, voting='hard', weights=None, n_jobs=None, flatten_transform=True)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  fit(self, X, y, sample_weight=None)\n",
      "     |      Fit the estimators.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix}, shape (n_samples, n_features)\n",
      "     |          Training vectors, where n_samples is the number of samples and\n",
      "     |          n_features is the number of features.\n",
      "     |      \n",
      "     |      y : array-like, shape (n_samples,)\n",
      "     |          Target values.\n",
      "     |      \n",
      "     |      sample_weight : array-like, shape (n_samples,) or None\n",
      "     |          Sample weights. If None, then samples are equally weighted.\n",
      "     |          Note that this is supported only if all underlying estimators\n",
      "     |          support sample weights.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |  \n",
      "     |  predict(self, X)\n",
      "     |      Predict class labels for X.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix}, shape (n_samples, n_features)\n",
      "     |          The input samples.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      maj : array-like, shape (n_samples,)\n",
      "     |          Predicted class labels.\n",
      "     |  \n",
      "     |  transform(self, X)\n",
      "     |      Return class labels or probabilities for X for each estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix}, shape (n_samples, n_features)\n",
      "     |          Training vectors, where n_samples is the number of samples and\n",
      "     |          n_features is the number of features.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      probabilities_or_labels\n",
      "     |          If `voting='soft'` and `flatten_transform=True`:\n",
      "     |              returns array-like of shape (n_classifiers, n_samples *\n",
      "     |              n_classes), being class probabilities calculated by each\n",
      "     |              classifier.\n",
      "     |          If `voting='soft' and `flatten_transform=False`:\n",
      "     |              array-like of shape (n_classifiers, n_samples, n_classes)\n",
      "     |          If `voting='hard'`:\n",
      "     |              array-like of shape (n_samples, n_classifiers), being\n",
      "     |              class labels predicted by each classifier.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  predict_proba\n",
      "     |      Compute probabilities of possible outcomes for samples in X.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix}, shape (n_samples, n_features)\n",
      "     |          The input samples.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      avg : array-like, shape (n_samples, n_classes)\n",
      "     |          Weighted average probability for each class per sample.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from _BaseVoting:\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get the parameters of the ensemble estimator\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : bool\n",
      "     |          Setting it to True gets the various estimators and the parameters\n",
      "     |          of the estimators as well\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Setting the parameters for the ensemble estimator\n",
      "     |      \n",
      "     |      Valid parameter keys can be listed with get_params().\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      **params : keyword arguments\n",
      "     |          Specific parameters using e.g. set_params(parameter_name=new_value)\n",
      "     |          In addition, to setting the parameters of the ensemble estimator,\n",
      "     |          the individual estimators of the ensemble estimator can also be\n",
      "     |          set or replaced by setting them to None.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      # In this example, the RandomForestClassifier is removed\n",
      "     |      clf1 = LogisticRegression()\n",
      "     |      clf2 = RandomForestClassifier()\n",
      "     |      eclf = VotingClassifier(estimators=[('lr', clf1), ('rf', clf2)]\n",
      "     |      eclf.set_params(rf=None)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from _BaseVoting:\n",
      "     |  \n",
      "     |  named_estimators\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __repr__(self, N_CHAR_MAX=700)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.TransformerMixin:\n",
      "     |  \n",
      "     |  fit_transform(self, X, y=None, **fit_params)\n",
      "     |      Fit to data, then transform it.\n",
      "     |      \n",
      "     |      Fits transformer to X and y with optional parameters fit_params\n",
      "     |      and returns a transformed version of X.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : numpy array of shape [n_samples, n_features]\n",
      "     |          Training set.\n",
      "     |      \n",
      "     |      y : numpy array of shape [n_samples]\n",
      "     |          Target values.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      X_new : numpy array of shape [n_samples, n_features_new]\n",
      "     |          Transformed array.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.ClassifierMixin:\n",
      "     |  \n",
      "     |  score(self, X, y, sample_weight=None)\n",
      "     |      Returns the mean accuracy on the given test data and labels.\n",
      "     |      \n",
      "     |      In multi-label classification, this is the subset accuracy\n",
      "     |      which is a harsh metric since you require for each sample that\n",
      "     |      each label set be correctly predicted.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like, shape = (n_samples, n_features)\n",
      "     |          Test samples.\n",
      "     |      \n",
      "     |      y : array-like, shape = (n_samples) or (n_samples, n_outputs)\n",
      "     |          True labels for X.\n",
      "     |      \n",
      "     |      sample_weight : array-like, shape = [n_samples], optional\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      score : float\n",
      "     |          Mean accuracy of self.predict(X) wrt. y.\n",
      "    \n",
      "    class VotingRegressor(_BaseVoting, sklearn.base.RegressorMixin)\n",
      "     |  VotingRegressor(estimators, weights=None, n_jobs=None)\n",
      "     |  \n",
      "     |  Prediction voting regressor for unfitted estimators.\n",
      "     |  \n",
      "     |  .. versionadded:: 0.21\n",
      "     |  \n",
      "     |  A voting regressor is an ensemble meta-estimator that fits base\n",
      "     |  regressors each on the whole dataset. It, then, averages the individual\n",
      "     |  predictions to form a final prediction.\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <voting_regressor>`.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  estimators : list of (string, estimator) tuples\n",
      "     |      Invoking the ``fit`` method on the ``VotingRegressor`` will fit clones\n",
      "     |      of those original estimators that will be stored in the class attribute\n",
      "     |      ``self.estimators_``. An estimator can be set to ``None`` or ``'drop'``\n",
      "     |      using ``set_params``.\n",
      "     |  \n",
      "     |  weights : array-like, shape (n_regressors,), optional (default=`None`)\n",
      "     |      Sequence of weights (`float` or `int`) to weight the occurrences of\n",
      "     |      predicted values before averaging. Uses uniform weights if `None`.\n",
      "     |  \n",
      "     |  n_jobs : int or None, optional (default=None)\n",
      "     |      The number of jobs to run in parallel for ``fit``.\n",
      "     |      ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n",
      "     |      ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n",
      "     |      for more details.\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  estimators_ : list of regressors\n",
      "     |      The collection of fitted sub-estimators as defined in ``estimators``\n",
      "     |      that are not `None`.\n",
      "     |  \n",
      "     |  named_estimators_ : Bunch object, a dictionary with attribute access\n",
      "     |      Attribute to access any fitted sub-estimators by name.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> import numpy as np\n",
      "     |  >>> from sklearn.linear_model import LinearRegression\n",
      "     |  >>> from sklearn.ensemble import RandomForestRegressor\n",
      "     |  >>> from sklearn.ensemble import VotingRegressor\n",
      "     |  >>> r1 = LinearRegression()\n",
      "     |  >>> r2 = RandomForestRegressor(n_estimators=10, random_state=1)\n",
      "     |  >>> X = np.array([[1, 1], [2, 4], [3, 9], [4, 16], [5, 25], [6, 36]])\n",
      "     |  >>> y = np.array([2, 6, 12, 20, 30, 42])\n",
      "     |  >>> er = VotingRegressor([('lr', r1), ('rf', r2)])\n",
      "     |  >>> print(er.fit(X, y).predict(X))\n",
      "     |  [ 3.3  5.7 11.8 19.7 28.  40.3]\n",
      "     |  \n",
      "     |  See also\n",
      "     |  --------\n",
      "     |  VotingClassifier: Soft Voting/Majority Rule classifier.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      VotingRegressor\n",
      "     |      _BaseVoting\n",
      "     |      sklearn.utils.metaestimators._BaseComposition\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      sklearn.base.TransformerMixin\n",
      "     |      sklearn.base.RegressorMixin\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, estimators, weights=None, n_jobs=None)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  fit(self, X, y, sample_weight=None)\n",
      "     |      Fit the estimators.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix}, shape (n_samples, n_features)\n",
      "     |          Training vectors, where n_samples is the number of samples and\n",
      "     |          n_features is the number of features.\n",
      "     |      \n",
      "     |      y : array-like, shape (n_samples,)\n",
      "     |          Target values.\n",
      "     |      \n",
      "     |      sample_weight : array-like, shape (n_samples,) or None\n",
      "     |          Sample weights. If None, then samples are equally weighted.\n",
      "     |          Note that this is supported only if all underlying estimators\n",
      "     |          support sample weights.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |  \n",
      "     |  predict(self, X)\n",
      "     |      Predict regression target for X.\n",
      "     |      \n",
      "     |      The predicted regression target of an input sample is computed as the\n",
      "     |      mean predicted regression targets of the estimators in the ensemble.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          The input samples.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      y : array of shape (n_samples,)\n",
      "     |          The predicted values.\n",
      "     |  \n",
      "     |  transform(self, X)\n",
      "     |      Return predictions for X for each estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix}, shape (n_samples, n_features)\n",
      "     |          The input samples.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      predictions\n",
      "     |          array-like of shape (n_samples, n_classifiers), being\n",
      "     |          values predicted by each regressor.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from _BaseVoting:\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get the parameters of the ensemble estimator\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : bool\n",
      "     |          Setting it to True gets the various estimators and the parameters\n",
      "     |          of the estimators as well\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Setting the parameters for the ensemble estimator\n",
      "     |      \n",
      "     |      Valid parameter keys can be listed with get_params().\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      **params : keyword arguments\n",
      "     |          Specific parameters using e.g. set_params(parameter_name=new_value)\n",
      "     |          In addition, to setting the parameters of the ensemble estimator,\n",
      "     |          the individual estimators of the ensemble estimator can also be\n",
      "     |          set or replaced by setting them to None.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      # In this example, the RandomForestClassifier is removed\n",
      "     |      clf1 = LogisticRegression()\n",
      "     |      clf2 = RandomForestClassifier()\n",
      "     |      eclf = VotingClassifier(estimators=[('lr', clf1), ('rf', clf2)]\n",
      "     |      eclf.set_params(rf=None)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from _BaseVoting:\n",
      "     |  \n",
      "     |  named_estimators\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __repr__(self, N_CHAR_MAX=700)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.TransformerMixin:\n",
      "     |  \n",
      "     |  fit_transform(self, X, y=None, **fit_params)\n",
      "     |      Fit to data, then transform it.\n",
      "     |      \n",
      "     |      Fits transformer to X and y with optional parameters fit_params\n",
      "     |      and returns a transformed version of X.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : numpy array of shape [n_samples, n_features]\n",
      "     |          Training set.\n",
      "     |      \n",
      "     |      y : numpy array of shape [n_samples]\n",
      "     |          Target values.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      X_new : numpy array of shape [n_samples, n_features_new]\n",
      "     |          Transformed array.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.RegressorMixin:\n",
      "     |  \n",
      "     |  score(self, X, y, sample_weight=None)\n",
      "     |      Returns the coefficient of determination R^2 of the prediction.\n",
      "     |      \n",
      "     |      The coefficient R^2 is defined as (1 - u/v), where u is the residual\n",
      "     |      sum of squares ((y_true - y_pred) ** 2).sum() and v is the total\n",
      "     |      sum of squares ((y_true - y_true.mean()) ** 2).sum().\n",
      "     |      The best possible score is 1.0 and it can be negative (because the\n",
      "     |      model can be arbitrarily worse). A constant model that always\n",
      "     |      predicts the expected value of y, disregarding the input features,\n",
      "     |      would get a R^2 score of 0.0.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like, shape = (n_samples, n_features)\n",
      "     |          Test samples. For some estimators this may be a\n",
      "     |          precomputed kernel matrix instead, shape = (n_samples,\n",
      "     |          n_samples_fitted], where n_samples_fitted is the number of\n",
      "     |          samples used in the fitting for the estimator.\n",
      "     |      \n",
      "     |      y : array-like, shape = (n_samples) or (n_samples, n_outputs)\n",
      "     |          True values for X.\n",
      "     |      \n",
      "     |      sample_weight : array-like, shape = [n_samples], optional\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      score : float\n",
      "     |          R^2 of self.predict(X) wrt. y.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      The R2 score used when calling ``score`` on a regressor will use\n",
      "     |      ``multioutput='uniform_average'`` from version 0.23 to keep consistent\n",
      "     |      with `metrics.r2_score`. This will influence the ``score`` method of\n",
      "     |      all the multioutput regressors (except for\n",
      "     |      `multioutput.MultiOutputRegressor`). To specify the default value\n",
      "     |      manually and avoid the warning, please either call `metrics.r2_score`\n",
      "     |      directly or make a custom scorer with `metrics.make_scorer` (the\n",
      "     |      built-in scorer ``'r2'`` uses ``multioutput='uniform_average'``).\n",
      "\n",
      "DATA\n",
      "    __all__ = ['BaseEnsemble', 'RandomForestClassifier', 'RandomForestRegr...\n",
      "\n",
      "FILE\n",
      "    /home/dir0417/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/__init__.py\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import sklearn.ensemble\n",
    "help(sklearn.ensemble)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input contains NaN, infinity or a value too large for dtype('float32').",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-4ce536d1ecbd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensemble\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRandomForestClassifier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mclf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRandomForestClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_estimators\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_depth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m8675309\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/forest.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    247\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    248\u001b[0m         \u001b[0;31m# Validate or convert input data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 249\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"csc\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mDTYPE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    250\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'csc'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mensure_2d\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msample_weight\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mforce_all_finite\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    541\u001b[0m             _assert_all_finite(array,\n\u001b[0;32m--> 542\u001b[0;31m                                allow_nan=force_all_finite == 'allow-nan')\n\u001b[0m\u001b[1;32m    543\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mensure_min_samples\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36m_assert_all_finite\u001b[0;34m(X, allow_nan)\u001b[0m\n\u001b[1;32m     54\u001b[0m                 not allow_nan and not np.isfinite(X).all()):\n\u001b[1;32m     55\u001b[0m             \u001b[0mtype_err\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'infinity'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mallow_nan\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m'NaN, infinity'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg_err\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype_err\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m     \u001b[0;31m# for object dtype data, we only check for NaNs (GH-13254)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'object'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mallow_nan\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Input contains NaN, infinity or a value too large for dtype('float32')."
     ]
    }
   ],
   "source": [
    "#Build a model to classify the Label based on the other columns\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "clf = RandomForestClassifier(n_estimators=100, max_depth=2, random_state=8675309)\n",
    "clf.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the model\n",
    "predictions = clf.predict(x_test)\n",
    "reality = y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'predictions' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-3b95c952b756>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#######################################  BEGIN STUDENT CODE  #####################################################\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mclassification_report\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclassification_report\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreality\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'predictions' is not defined"
     ]
    }
   ],
   "source": [
    "# YouDo:\n",
    "#  1) Evaluate the \"goodness\" of the predictions and write a paragraph describing the results as a markdown.\n",
    "#\n",
    "#######################################  BEGIN STUDENT CODE  #####################################################\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(predictions, reality))\n",
    "#one of them is sensitivity and specificity\n",
    "\n",
    "#######################################   END STUDENT CODE   #####################################################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on package sklearn.metrics in sklearn:\n",
      "\n",
      "NAME\n",
      "    sklearn.metrics\n",
      "\n",
      "DESCRIPTION\n",
      "    The :mod:`sklearn.metrics` module includes score functions, performance metrics\n",
      "    and pairwise metrics and distance computations.\n",
      "\n",
      "PACKAGE CONTENTS\n",
      "    base\n",
      "    classification\n",
      "    cluster (package)\n",
      "    pairwise\n",
      "    pairwise_fast\n",
      "    ranking\n",
      "    regression\n",
      "    scorer\n",
      "    setup\n",
      "    tests (package)\n",
      "\n",
      "FUNCTIONS\n",
      "    accuracy_score(y_true, y_pred, normalize=True, sample_weight=None)\n",
      "        Accuracy classification score.\n",
      "        \n",
      "        In multilabel classification, this function computes subset accuracy:\n",
      "        the set of labels predicted for a sample must *exactly* match the\n",
      "        corresponding set of labels in y_true.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <accuracy_score>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y_true : 1d array-like, or label indicator array / sparse matrix\n",
      "            Ground truth (correct) labels.\n",
      "        \n",
      "        y_pred : 1d array-like, or label indicator array / sparse matrix\n",
      "            Predicted labels, as returned by a classifier.\n",
      "        \n",
      "        normalize : bool, optional (default=True)\n",
      "            If ``False``, return the number of correctly classified samples.\n",
      "            Otherwise, return the fraction of correctly classified samples.\n",
      "        \n",
      "        sample_weight : array-like of shape = [n_samples], optional\n",
      "            Sample weights.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        score : float\n",
      "            If ``normalize == True``, return the fraction of correctly\n",
      "            classified samples (float), else returns the number of correctly\n",
      "            classified samples (int).\n",
      "        \n",
      "            The best performance is 1 with ``normalize == True`` and the number\n",
      "            of samples with ``normalize == False``.\n",
      "        \n",
      "        See also\n",
      "        --------\n",
      "        jaccard_score, hamming_loss, zero_one_loss\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        In binary and multiclass classification, this function is equal\n",
      "        to the ``jaccard_score`` function.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from sklearn.metrics import accuracy_score\n",
      "        >>> y_pred = [0, 2, 1, 3]\n",
      "        >>> y_true = [0, 1, 2, 3]\n",
      "        >>> accuracy_score(y_true, y_pred)\n",
      "        0.5\n",
      "        >>> accuracy_score(y_true, y_pred, normalize=False)\n",
      "        2\n",
      "        \n",
      "        In the multilabel case with binary label indicators:\n",
      "        \n",
      "        >>> import numpy as np\n",
      "        >>> accuracy_score(np.array([[0, 1], [1, 1]]), np.ones((2, 2)))\n",
      "        0.5\n",
      "    \n",
      "    adjusted_mutual_info_score(labels_true, labels_pred, average_method='warn')\n",
      "        Adjusted Mutual Information between two clusterings.\n",
      "        \n",
      "        Adjusted Mutual Information (AMI) is an adjustment of the Mutual\n",
      "        Information (MI) score to account for chance. It accounts for the fact that\n",
      "        the MI is generally higher for two clusterings with a larger number of\n",
      "        clusters, regardless of whether there is actually more information shared.\n",
      "        For two clusterings :math:`U` and :math:`V`, the AMI is given as::\n",
      "        \n",
      "            AMI(U, V) = [MI(U, V) - E(MI(U, V))] / [avg(H(U), H(V)) - E(MI(U, V))]\n",
      "        \n",
      "        This metric is independent of the absolute values of the labels:\n",
      "        a permutation of the class or cluster label values won't change the\n",
      "        score value in any way.\n",
      "        \n",
      "        This metric is furthermore symmetric: switching ``label_true`` with\n",
      "        ``label_pred`` will return the same score value. This can be useful to\n",
      "        measure the agreement of two independent label assignments strategies\n",
      "        on the same dataset when the real ground truth is not known.\n",
      "        \n",
      "        Be mindful that this function is an order of magnitude slower than other\n",
      "        metrics, such as the Adjusted Rand Index.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <mutual_info_score>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        labels_true : int array, shape = [n_samples]\n",
      "            A clustering of the data into disjoint subsets.\n",
      "        \n",
      "        labels_pred : array, shape = [n_samples]\n",
      "            A clustering of the data into disjoint subsets.\n",
      "        \n",
      "        average_method : string, optional (default: 'warn')\n",
      "            How to compute the normalizer in the denominator. Possible options\n",
      "            are 'min', 'geometric', 'arithmetic', and 'max'.\n",
      "            If 'warn', 'max' will be used. The default will change to\n",
      "            'arithmetic' in version 0.22.\n",
      "        \n",
      "            .. versionadded:: 0.20\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        ami: float (upperlimited by 1.0)\n",
      "           The AMI returns a value of 1 when the two partitions are identical\n",
      "           (ie perfectly matched). Random partitions (independent labellings) have\n",
      "           an expected AMI around 0 on average hence can be negative.\n",
      "        \n",
      "        See also\n",
      "        --------\n",
      "        adjusted_rand_score: Adjusted Rand Index\n",
      "        mutual_info_score: Mutual Information (not adjusted for chance)\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        \n",
      "        Perfect labelings are both homogeneous and complete, hence have\n",
      "        score 1.0::\n",
      "        \n",
      "          >>> from sklearn.metrics.cluster import adjusted_mutual_info_score\n",
      "          >>> adjusted_mutual_info_score([0, 0, 1, 1], [0, 0, 1, 1])\n",
      "          ... # doctest: +SKIP\n",
      "          1.0\n",
      "          >>> adjusted_mutual_info_score([0, 0, 1, 1], [1, 1, 0, 0])\n",
      "          ... # doctest: +SKIP\n",
      "          1.0\n",
      "        \n",
      "        If classes members are completely split across different clusters,\n",
      "        the assignment is totally in-complete, hence the AMI is null::\n",
      "        \n",
      "          >>> adjusted_mutual_info_score([0, 0, 0, 0], [0, 1, 2, 3])\n",
      "          ... # doctest: +SKIP\n",
      "          0.0\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] `Vinh, Epps, and Bailey, (2010). Information Theoretic Measures for\n",
      "           Clusterings Comparison: Variants, Properties, Normalization and\n",
      "           Correction for Chance, JMLR\n",
      "           <http://jmlr.csail.mit.edu/papers/volume11/vinh10a/vinh10a.pdf>`_\n",
      "        \n",
      "        .. [2] `Wikipedia entry for the Adjusted Mutual Information\n",
      "           <https://en.wikipedia.org/wiki/Adjusted_Mutual_Information>`_\n",
      "    \n",
      "    adjusted_rand_score(labels_true, labels_pred)\n",
      "        Rand index adjusted for chance.\n",
      "        \n",
      "        The Rand Index computes a similarity measure between two clusterings\n",
      "        by considering all pairs of samples and counting pairs that are\n",
      "        assigned in the same or different clusters in the predicted and\n",
      "        true clusterings.\n",
      "        \n",
      "        The raw RI score is then \"adjusted for chance\" into the ARI score\n",
      "        using the following scheme::\n",
      "        \n",
      "            ARI = (RI - Expected_RI) / (max(RI) - Expected_RI)\n",
      "        \n",
      "        The adjusted Rand index is thus ensured to have a value close to\n",
      "        0.0 for random labeling independently of the number of clusters and\n",
      "        samples and exactly 1.0 when the clusterings are identical (up to\n",
      "        a permutation).\n",
      "        \n",
      "        ARI is a symmetric measure::\n",
      "        \n",
      "            adjusted_rand_score(a, b) == adjusted_rand_score(b, a)\n",
      "        \n",
      "        Read more in the :ref:`User Guide <adjusted_rand_score>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        labels_true : int array, shape = [n_samples]\n",
      "            Ground truth class labels to be used as a reference\n",
      "        \n",
      "        labels_pred : array, shape = [n_samples]\n",
      "            Cluster labels to evaluate\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        ari : float\n",
      "           Similarity score between -1.0 and 1.0. Random labelings have an ARI\n",
      "           close to 0.0. 1.0 stands for perfect match.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        \n",
      "        Perfectly matching labelings have a score of 1 even\n",
      "        \n",
      "          >>> from sklearn.metrics.cluster import adjusted_rand_score\n",
      "          >>> adjusted_rand_score([0, 0, 1, 1], [0, 0, 1, 1])\n",
      "          1.0\n",
      "          >>> adjusted_rand_score([0, 0, 1, 1], [1, 1, 0, 0])\n",
      "          1.0\n",
      "        \n",
      "        Labelings that assign all classes members to the same clusters\n",
      "        are complete be not always pure, hence penalized::\n",
      "        \n",
      "          >>> adjusted_rand_score([0, 0, 1, 2], [0, 0, 1, 1])  # doctest: +ELLIPSIS\n",
      "          0.57...\n",
      "        \n",
      "        ARI is symmetric, so labelings that have pure clusters with members\n",
      "        coming from the same classes but unnecessary splits are penalized::\n",
      "        \n",
      "          >>> adjusted_rand_score([0, 0, 1, 1], [0, 0, 1, 2])  # doctest: +ELLIPSIS\n",
      "          0.57...\n",
      "        \n",
      "        If classes members are completely split across different clusters, the\n",
      "        assignment is totally incomplete, hence the ARI is very low::\n",
      "        \n",
      "          >>> adjusted_rand_score([0, 0, 0, 0], [0, 1, 2, 3])\n",
      "          0.0\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        \n",
      "        .. [Hubert1985] L. Hubert and P. Arabie, Comparing Partitions,\n",
      "          Journal of Classification 1985\n",
      "          https://link.springer.com/article/10.1007%2FBF01908075\n",
      "        \n",
      "        .. [wk] https://en.wikipedia.org/wiki/Rand_index#Adjusted_Rand_index\n",
      "        \n",
      "        See also\n",
      "        --------\n",
      "        adjusted_mutual_info_score: Adjusted Mutual Information\n",
      "    \n",
      "    auc(x, y, reorder='deprecated')\n",
      "        Compute Area Under the Curve (AUC) using the trapezoidal rule\n",
      "        \n",
      "        This is a general function, given points on a curve.  For computing the\n",
      "        area under the ROC-curve, see :func:`roc_auc_score`.  For an alternative\n",
      "        way to summarize a precision-recall curve, see\n",
      "        :func:`average_precision_score`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        x : array, shape = [n]\n",
      "            x coordinates. These must be either monotonic increasing or monotonic\n",
      "            decreasing.\n",
      "        y : array, shape = [n]\n",
      "            y coordinates.\n",
      "        reorder : boolean, optional (default='deprecated')\n",
      "            Whether to sort x before computing. If False, assume that x must be\n",
      "            either monotonic increasing or monotonic decreasing. If True, y is\n",
      "            used to break ties when sorting x. Make sure that y has a monotonic\n",
      "            relation to x when setting reorder to True.\n",
      "        \n",
      "            .. deprecated:: 0.20\n",
      "               Parameter ``reorder`` has been deprecated in version 0.20 and will\n",
      "               be removed in 0.22. It's introduced for roc_auc_score (not for\n",
      "               general use) and is no longer used there. What's more, the result\n",
      "               from auc will be significantly influenced if x is sorted\n",
      "               unexpectedly due to slight floating point error (See issue #9786).\n",
      "               Future (and default) behavior is equivalent to ``reorder=False``.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        auc : float\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> import numpy as np\n",
      "        >>> from sklearn import metrics\n",
      "        >>> y = np.array([1, 1, 2, 2])\n",
      "        >>> pred = np.array([0.1, 0.4, 0.35, 0.8])\n",
      "        >>> fpr, tpr, thresholds = metrics.roc_curve(y, pred, pos_label=2)\n",
      "        >>> metrics.auc(fpr, tpr)\n",
      "        0.75\n",
      "        \n",
      "        See also\n",
      "        --------\n",
      "        roc_auc_score : Compute the area under the ROC curve\n",
      "        average_precision_score : Compute average precision from prediction scores\n",
      "        precision_recall_curve :\n",
      "            Compute precision-recall pairs for different probability thresholds\n",
      "    \n",
      "    average_precision_score(y_true, y_score, average='macro', pos_label=1, sample_weight=None)\n",
      "        Compute average precision (AP) from prediction scores\n",
      "        \n",
      "        AP summarizes a precision-recall curve as the weighted mean of precisions\n",
      "        achieved at each threshold, with the increase in recall from the previous\n",
      "        threshold used as the weight:\n",
      "        \n",
      "        .. math::\n",
      "            \\text{AP} = \\sum_n (R_n - R_{n-1}) P_n\n",
      "        \n",
      "        where :math:`P_n` and :math:`R_n` are the precision and recall at the nth\n",
      "        threshold [1]_. This implementation is not interpolated and is different\n",
      "        from computing the area under the precision-recall curve with the\n",
      "        trapezoidal rule, which uses linear interpolation and can be too\n",
      "        optimistic.\n",
      "        \n",
      "        Note: this implementation is restricted to the binary classification task\n",
      "        or multilabel classification task.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <precision_recall_f_measure_metrics>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y_true : array, shape = [n_samples] or [n_samples, n_classes]\n",
      "            True binary labels or binary label indicators.\n",
      "        \n",
      "        y_score : array, shape = [n_samples] or [n_samples, n_classes]\n",
      "            Target scores, can either be probability estimates of the positive\n",
      "            class, confidence values, or non-thresholded measure of decisions\n",
      "            (as returned by \"decision_function\" on some classifiers).\n",
      "        \n",
      "        average : string, [None, 'micro', 'macro' (default), 'samples', 'weighted']\n",
      "            If ``None``, the scores for each class are returned. Otherwise,\n",
      "            this determines the type of averaging performed on the data:\n",
      "        \n",
      "            ``'micro'``:\n",
      "                Calculate metrics globally by considering each element of the label\n",
      "                indicator matrix as a label.\n",
      "            ``'macro'``:\n",
      "                Calculate metrics for each label, and find their unweighted\n",
      "                mean.  This does not take label imbalance into account.\n",
      "            ``'weighted'``:\n",
      "                Calculate metrics for each label, and find their average, weighted\n",
      "                by support (the number of true instances for each label).\n",
      "            ``'samples'``:\n",
      "                Calculate metrics for each instance, and find their average.\n",
      "        \n",
      "            Will be ignored when ``y_true`` is binary.\n",
      "        \n",
      "        pos_label : int or str (default=1)\n",
      "            The label of the positive class. Only applied to binary ``y_true``.\n",
      "            For multilabel-indicator ``y_true``, ``pos_label`` is fixed to 1.\n",
      "        \n",
      "        sample_weight : array-like of shape = [n_samples], optional\n",
      "            Sample weights.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        average_precision : float\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] `Wikipedia entry for the Average precision\n",
      "               <https://en.wikipedia.org/w/index.php?title=Information_retrieval&\n",
      "               oldid=793358396#Average_precision>`_\n",
      "        \n",
      "        See also\n",
      "        --------\n",
      "        roc_auc_score : Compute the area under the ROC curve\n",
      "        \n",
      "        precision_recall_curve :\n",
      "            Compute precision-recall pairs for different probability thresholds\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> import numpy as np\n",
      "        >>> from sklearn.metrics import average_precision_score\n",
      "        >>> y_true = np.array([0, 0, 1, 1])\n",
      "        >>> y_scores = np.array([0.1, 0.4, 0.35, 0.8])\n",
      "        >>> average_precision_score(y_true, y_scores)  # doctest: +ELLIPSIS\n",
      "        0.83...\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        .. versionchanged:: 0.19\n",
      "          Instead of linearly interpolating between operating points, precisions\n",
      "          are weighted by the change in recall since the last operating point.\n",
      "    \n",
      "    balanced_accuracy_score(y_true, y_pred, sample_weight=None, adjusted=False)\n",
      "        Compute the balanced accuracy\n",
      "        \n",
      "        The balanced accuracy in binary and multiclass classification problems to\n",
      "        deal with imbalanced datasets. It is defined as the average of recall\n",
      "        obtained on each class.\n",
      "        \n",
      "        The best value is 1 and the worst value is 0 when ``adjusted=False``.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <balanced_accuracy_score>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y_true : 1d array-like\n",
      "            Ground truth (correct) target values.\n",
      "        \n",
      "        y_pred : 1d array-like\n",
      "            Estimated targets as returned by a classifier.\n",
      "        \n",
      "        sample_weight : array-like of shape = [n_samples], optional\n",
      "            Sample weights.\n",
      "        \n",
      "        adjusted : bool, default=False\n",
      "            When true, the result is adjusted for chance, so that random\n",
      "            performance would score 0, and perfect performance scores 1.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        balanced_accuracy : float\n",
      "        \n",
      "        See also\n",
      "        --------\n",
      "        recall_score, roc_auc_score\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        Some literature promotes alternative definitions of balanced accuracy. Our\n",
      "        definition is equivalent to :func:`accuracy_score` with class-balanced\n",
      "        sample weights, and shares desirable properties with the binary case.\n",
      "        See the :ref:`User Guide <balanced_accuracy_score>`.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] Brodersen, K.H.; Ong, C.S.; Stephan, K.E.; Buhmann, J.M. (2010).\n",
      "               The balanced accuracy and its posterior distribution.\n",
      "               Proceedings of the 20th International Conference on Pattern\n",
      "               Recognition, 3121-24.\n",
      "        .. [2] John. D. Kelleher, Brian Mac Namee, Aoife D'Arcy, (2015).\n",
      "               `Fundamentals of Machine Learning for Predictive Data Analytics:\n",
      "               Algorithms, Worked Examples, and Case Studies\n",
      "               <https://mitpress.mit.edu/books/fundamentals-machine-learning-predictive-data-analytics>`_.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from sklearn.metrics import balanced_accuracy_score\n",
      "        >>> y_true = [0, 1, 0, 0, 1, 0]\n",
      "        >>> y_pred = [0, 1, 0, 0, 0, 1]\n",
      "        >>> balanced_accuracy_score(y_true, y_pred)\n",
      "        0.625\n",
      "    \n",
      "    brier_score_loss(y_true, y_prob, sample_weight=None, pos_label=None)\n",
      "        Compute the Brier score.\n",
      "        The smaller the Brier score, the better, hence the naming with \"loss\".\n",
      "        Across all items in a set N predictions, the Brier score measures the\n",
      "        mean squared difference between (1) the predicted probability assigned\n",
      "        to the possible outcomes for item i, and (2) the actual outcome.\n",
      "        Therefore, the lower the Brier score is for a set of predictions, the\n",
      "        better the predictions are calibrated. Note that the Brier score always\n",
      "        takes on a value between zero and one, since this is the largest\n",
      "        possible difference between a predicted probability (which must be\n",
      "        between zero and one) and the actual outcome (which can take on values\n",
      "        of only 0 and 1). The Brier loss is composed of refinement loss and\n",
      "        calibration loss.\n",
      "        The Brier score is appropriate for binary and categorical outcomes that\n",
      "        can be structured as true or false, but is inappropriate for ordinal\n",
      "        variables which can take on three or more values (this is because the\n",
      "        Brier score assumes that all possible outcomes are equivalently\n",
      "        \"distant\" from one another). Which label is considered to be the positive\n",
      "        label is controlled via the parameter pos_label, which defaults to 1.\n",
      "        Read more in the :ref:`User Guide <calibration>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y_true : array, shape (n_samples,)\n",
      "            True targets.\n",
      "        \n",
      "        y_prob : array, shape (n_samples,)\n",
      "            Probabilities of the positive class.\n",
      "        \n",
      "        sample_weight : array-like of shape = [n_samples], optional\n",
      "            Sample weights.\n",
      "        \n",
      "        pos_label : int or str, default=None\n",
      "            Label of the positive class.\n",
      "            Defaults to the greater label unless y_true is all 0 or all -1\n",
      "            in which case pos_label defaults to 1.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        score : float\n",
      "            Brier score\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> import numpy as np\n",
      "        >>> from sklearn.metrics import brier_score_loss\n",
      "        >>> y_true = np.array([0, 1, 1, 0])\n",
      "        >>> y_true_categorical = np.array([\"spam\", \"ham\", \"ham\", \"spam\"])\n",
      "        >>> y_prob = np.array([0.1, 0.9, 0.8, 0.3])\n",
      "        >>> brier_score_loss(y_true, y_prob)  # doctest: +ELLIPSIS\n",
      "        0.037...\n",
      "        >>> brier_score_loss(y_true, 1-y_prob, pos_label=0)  # doctest: +ELLIPSIS\n",
      "        0.037...\n",
      "        >>> brier_score_loss(y_true_categorical, y_prob,                          pos_label=\"ham\")  # doctest: +ELLIPSIS\n",
      "        0.037...\n",
      "        >>> brier_score_loss(y_true, np.array(y_prob) > 0.5)\n",
      "        0.0\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] `Wikipedia entry for the Brier score.\n",
      "                <https://en.wikipedia.org/wiki/Brier_score>`_\n",
      "    \n",
      "    calinski_harabasz_score(X, labels)\n",
      "        Compute the Calinski and Harabasz score.\n",
      "        \n",
      "        It is also known as the Variance Ratio Criterion.\n",
      "        \n",
      "        The score is defined as ratio between the within-cluster dispersion and\n",
      "        the between-cluster dispersion.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <calinski_harabasz_index>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        X : array-like, shape (``n_samples``, ``n_features``)\n",
      "            List of ``n_features``-dimensional data points. Each row corresponds\n",
      "            to a single data point.\n",
      "        \n",
      "        labels : array-like, shape (``n_samples``,)\n",
      "            Predicted labels for each sample.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        score : float\n",
      "            The resulting Calinski-Harabasz score.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] `T. Calinski and J. Harabasz, 1974. \"A dendrite method for cluster\n",
      "           analysis\". Communications in Statistics\n",
      "           <https://www.tandfonline.com/doi/abs/10.1080/03610927408827101>`_\n",
      "    \n",
      "    calinski_harabaz_score(X, labels)\n",
      "        DEPRECATED: Function 'calinski_harabaz_score' has been renamed to 'calinski_harabasz_score' and will be removed in version 0.23.\n",
      "    \n",
      "    check_scoring(estimator, scoring=None, allow_none=False)\n",
      "        Determine scorer from user options.\n",
      "        \n",
      "        A TypeError will be thrown if the estimator cannot be scored.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        estimator : estimator object implementing 'fit'\n",
      "            The object to use to fit the data.\n",
      "        \n",
      "        scoring : string, callable or None, optional, default: None\n",
      "            A string (see model evaluation documentation) or\n",
      "            a scorer callable object / function with signature\n",
      "            ``scorer(estimator, X, y)``.\n",
      "        \n",
      "        allow_none : boolean, optional, default: False\n",
      "            If no scoring is specified and the estimator has no score function, we\n",
      "            can either return None or raise an exception.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        scoring : callable\n",
      "            A scorer callable object / function with signature\n",
      "            ``scorer(estimator, X, y)``.\n",
      "    \n",
      "    classification_report(y_true, y_pred, labels=None, target_names=None, sample_weight=None, digits=2, output_dict=False)\n",
      "        Build a text report showing the main classification metrics\n",
      "        \n",
      "        Read more in the :ref:`User Guide <classification_report>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y_true : 1d array-like, or label indicator array / sparse matrix\n",
      "            Ground truth (correct) target values.\n",
      "        \n",
      "        y_pred : 1d array-like, or label indicator array / sparse matrix\n",
      "            Estimated targets as returned by a classifier.\n",
      "        \n",
      "        labels : array, shape = [n_labels]\n",
      "            Optional list of label indices to include in the report.\n",
      "        \n",
      "        target_names : list of strings\n",
      "            Optional display names matching the labels (same order).\n",
      "        \n",
      "        sample_weight : array-like of shape = [n_samples], optional\n",
      "            Sample weights.\n",
      "        \n",
      "        digits : int\n",
      "            Number of digits for formatting output floating point values.\n",
      "            When ``output_dict`` is ``True``, this will be ignored and the\n",
      "            returned values will not be rounded.\n",
      "        \n",
      "        output_dict : bool (default = False)\n",
      "            If True, return output as dict\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        report : string / dict\n",
      "            Text summary of the precision, recall, F1 score for each class.\n",
      "            Dictionary returned if output_dict is True. Dictionary has the\n",
      "            following structure::\n",
      "        \n",
      "                {'label 1': {'precision':0.5,\n",
      "                             'recall':1.0,\n",
      "                             'f1-score':0.67,\n",
      "                             'support':1},\n",
      "                 'label 2': { ... },\n",
      "                  ...\n",
      "                }\n",
      "        \n",
      "            The reported averages include macro average (averaging the unweighted\n",
      "            mean per label), weighted average (averaging the support-weighted mean\n",
      "            per label), sample average (only for multilabel classification) and\n",
      "            micro average (averaging the total true positives, false negatives and\n",
      "            false positives) it is only shown for multi-label or multi-class\n",
      "            with a subset of classes because it is accuracy otherwise.\n",
      "            See also:func:`precision_recall_fscore_support` for more details\n",
      "            on averages.\n",
      "        \n",
      "            Note that in binary classification, recall of the positive class\n",
      "            is also known as \"sensitivity\"; recall of the negative class is\n",
      "            \"specificity\".\n",
      "        \n",
      "        See also\n",
      "        --------\n",
      "        precision_recall_fscore_support, confusion_matrix,\n",
      "        multilabel_confusion_matrix\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from sklearn.metrics import classification_report\n",
      "        >>> y_true = [0, 1, 2, 2, 2]\n",
      "        >>> y_pred = [0, 0, 2, 2, 1]\n",
      "        >>> target_names = ['class 0', 'class 1', 'class 2']\n",
      "        >>> print(classification_report(y_true, y_pred, target_names=target_names))\n",
      "                      precision    recall  f1-score   support\n",
      "        <BLANKLINE>\n",
      "             class 0       0.50      1.00      0.67         1\n",
      "             class 1       0.00      0.00      0.00         1\n",
      "             class 2       1.00      0.67      0.80         3\n",
      "        <BLANKLINE>\n",
      "            accuracy                           0.60         5\n",
      "           macro avg       0.50      0.56      0.49         5\n",
      "        weighted avg       0.70      0.60      0.61         5\n",
      "        <BLANKLINE>\n",
      "        >>> y_pred = [1, 1, 0]\n",
      "        >>> y_true = [1, 1, 1]\n",
      "        >>> print(classification_report(y_true, y_pred, labels=[1, 2, 3]))\n",
      "                      precision    recall  f1-score   support\n",
      "        <BLANKLINE>\n",
      "                   1       1.00      0.67      0.80         3\n",
      "                   2       0.00      0.00      0.00         0\n",
      "                   3       0.00      0.00      0.00         0\n",
      "        <BLANKLINE>\n",
      "           micro avg       1.00      0.67      0.80         3\n",
      "           macro avg       0.33      0.22      0.27         3\n",
      "        weighted avg       1.00      0.67      0.80         3\n",
      "        <BLANKLINE>\n",
      "    \n",
      "    cohen_kappa_score(y1, y2, labels=None, weights=None, sample_weight=None)\n",
      "        Cohen's kappa: a statistic that measures inter-annotator agreement.\n",
      "        \n",
      "        This function computes Cohen's kappa [1]_, a score that expresses the level\n",
      "        of agreement between two annotators on a classification problem. It is\n",
      "        defined as\n",
      "        \n",
      "        .. math::\n",
      "            \\kappa = (p_o - p_e) / (1 - p_e)\n",
      "        \n",
      "        where :math:`p_o` is the empirical probability of agreement on the label\n",
      "        assigned to any sample (the observed agreement ratio), and :math:`p_e` is\n",
      "        the expected agreement when both annotators assign labels randomly.\n",
      "        :math:`p_e` is estimated using a per-annotator empirical prior over the\n",
      "        class labels [2]_.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <cohen_kappa>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y1 : array, shape = [n_samples]\n",
      "            Labels assigned by the first annotator.\n",
      "        \n",
      "        y2 : array, shape = [n_samples]\n",
      "            Labels assigned by the second annotator. The kappa statistic is\n",
      "            symmetric, so swapping ``y1`` and ``y2`` doesn't change the value.\n",
      "        \n",
      "        labels : array, shape = [n_classes], optional\n",
      "            List of labels to index the matrix. This may be used to select a\n",
      "            subset of labels. If None, all labels that appear at least once in\n",
      "            ``y1`` or ``y2`` are used.\n",
      "        \n",
      "        weights : str, optional\n",
      "            List of weighting type to calculate the score. None means no weighted;\n",
      "            \"linear\" means linear weighted; \"quadratic\" means quadratic weighted.\n",
      "        \n",
      "        sample_weight : array-like of shape = [n_samples], optional\n",
      "            Sample weights.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        kappa : float\n",
      "            The kappa statistic, which is a number between -1 and 1. The maximum\n",
      "            value means complete agreement; zero or lower means chance agreement.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] J. Cohen (1960). \"A coefficient of agreement for nominal scales\".\n",
      "               Educational and Psychological Measurement 20(1):37-46.\n",
      "               doi:10.1177/001316446002000104.\n",
      "        .. [2] `R. Artstein and M. Poesio (2008). \"Inter-coder agreement for\n",
      "               computational linguistics\". Computational Linguistics 34(4):555-596.\n",
      "               <https://www.mitpressjournals.org/doi/pdf/10.1162/coli.07-034-R2>`_\n",
      "        .. [3] `Wikipedia entry for the Cohen's kappa.\n",
      "                <https://en.wikipedia.org/wiki/Cohen%27s_kappa>`_\n",
      "    \n",
      "    completeness_score(labels_true, labels_pred)\n",
      "        Completeness metric of a cluster labeling given a ground truth.\n",
      "        \n",
      "        A clustering result satisfies completeness if all the data points\n",
      "        that are members of a given class are elements of the same cluster.\n",
      "        \n",
      "        This metric is independent of the absolute values of the labels:\n",
      "        a permutation of the class or cluster label values won't change the\n",
      "        score value in any way.\n",
      "        \n",
      "        This metric is not symmetric: switching ``label_true`` with ``label_pred``\n",
      "        will return the :func:`homogeneity_score` which will be different in\n",
      "        general.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <homogeneity_completeness>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        labels_true : int array, shape = [n_samples]\n",
      "            ground truth class labels to be used as a reference\n",
      "        \n",
      "        labels_pred : array, shape = [n_samples]\n",
      "            cluster labels to evaluate\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        completeness : float\n",
      "           score between 0.0 and 1.0. 1.0 stands for perfectly complete labeling\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        \n",
      "        .. [1] `Andrew Rosenberg and Julia Hirschberg, 2007. V-Measure: A\n",
      "           conditional entropy-based external cluster evaluation measure\n",
      "           <https://aclweb.org/anthology/D/D07/D07-1043.pdf>`_\n",
      "        \n",
      "        See also\n",
      "        --------\n",
      "        homogeneity_score\n",
      "        v_measure_score\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        \n",
      "        Perfect labelings are complete::\n",
      "        \n",
      "          >>> from sklearn.metrics.cluster import completeness_score\n",
      "          >>> completeness_score([0, 0, 1, 1], [1, 1, 0, 0])\n",
      "          1.0\n",
      "        \n",
      "        Non-perfect labelings that assign all classes members to the same clusters\n",
      "        are still complete::\n",
      "        \n",
      "          >>> print(completeness_score([0, 0, 1, 1], [0, 0, 0, 0]))\n",
      "          1.0\n",
      "          >>> print(completeness_score([0, 1, 2, 3], [0, 0, 1, 1]))\n",
      "          0.999...\n",
      "        \n",
      "        If classes members are split across different clusters, the\n",
      "        assignment cannot be complete::\n",
      "        \n",
      "          >>> print(completeness_score([0, 0, 1, 1], [0, 1, 0, 1]))\n",
      "          0.0\n",
      "          >>> print(completeness_score([0, 0, 0, 0], [0, 1, 2, 3]))\n",
      "          0.0\n",
      "    \n",
      "    confusion_matrix(y_true, y_pred, labels=None, sample_weight=None)\n",
      "        Compute confusion matrix to evaluate the accuracy of a classification\n",
      "        \n",
      "        By definition a confusion matrix :math:`C` is such that :math:`C_{i, j}`\n",
      "        is equal to the number of observations known to be in group :math:`i` but\n",
      "        predicted to be in group :math:`j`.\n",
      "        \n",
      "        Thus in binary classification, the count of true negatives is\n",
      "        :math:`C_{0,0}`, false negatives is :math:`C_{1,0}`, true positives is\n",
      "        :math:`C_{1,1}` and false positives is :math:`C_{0,1}`.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <confusion_matrix>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y_true : array, shape = [n_samples]\n",
      "            Ground truth (correct) target values.\n",
      "        \n",
      "        y_pred : array, shape = [n_samples]\n",
      "            Estimated targets as returned by a classifier.\n",
      "        \n",
      "        labels : array, shape = [n_classes], optional\n",
      "            List of labels to index the matrix. This may be used to reorder\n",
      "            or select a subset of labels.\n",
      "            If none is given, those that appear at least once\n",
      "            in ``y_true`` or ``y_pred`` are used in sorted order.\n",
      "        \n",
      "        sample_weight : array-like of shape = [n_samples], optional\n",
      "            Sample weights.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        C : array, shape = [n_classes, n_classes]\n",
      "            Confusion matrix\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] `Wikipedia entry for the Confusion matrix\n",
      "               <https://en.wikipedia.org/wiki/Confusion_matrix>`_\n",
      "               (Wikipedia and other references may use a different\n",
      "               convention for axes)\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from sklearn.metrics import confusion_matrix\n",
      "        >>> y_true = [2, 0, 2, 2, 0, 1]\n",
      "        >>> y_pred = [0, 0, 2, 2, 0, 2]\n",
      "        >>> confusion_matrix(y_true, y_pred)\n",
      "        array([[2, 0, 0],\n",
      "               [0, 0, 1],\n",
      "               [1, 0, 2]])\n",
      "        \n",
      "        >>> y_true = [\"cat\", \"ant\", \"cat\", \"cat\", \"ant\", \"bird\"]\n",
      "        >>> y_pred = [\"ant\", \"ant\", \"cat\", \"cat\", \"ant\", \"cat\"]\n",
      "        >>> confusion_matrix(y_true, y_pred, labels=[\"ant\", \"bird\", \"cat\"])\n",
      "        array([[2, 0, 0],\n",
      "               [0, 0, 1],\n",
      "               [1, 0, 2]])\n",
      "        \n",
      "        In the binary case, we can extract true positives, etc as follows:\n",
      "        \n",
      "        >>> tn, fp, fn, tp = confusion_matrix([0, 1, 0, 1], [1, 1, 1, 0]).ravel()\n",
      "        >>> (tn, fp, fn, tp)\n",
      "        (0, 2, 1, 1)\n",
      "    \n",
      "    consensus_score(a, b, similarity='jaccard')\n",
      "        The similarity of two sets of biclusters.\n",
      "        \n",
      "        Similarity between individual biclusters is computed. Then the\n",
      "        best matching between sets is found using the Hungarian algorithm.\n",
      "        The final score is the sum of similarities divided by the size of\n",
      "        the larger set.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <biclustering>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        a : (rows, columns)\n",
      "            Tuple of row and column indicators for a set of biclusters.\n",
      "        \n",
      "        b : (rows, columns)\n",
      "            Another set of biclusters like ``a``.\n",
      "        \n",
      "        similarity : string or function, optional, default: \"jaccard\"\n",
      "            May be the string \"jaccard\" to use the Jaccard coefficient, or\n",
      "            any function that takes four arguments, each of which is a 1d\n",
      "            indicator vector: (a_rows, a_columns, b_rows, b_columns).\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        \n",
      "        * Hochreiter, Bodenhofer, et. al., 2010. `FABIA: factor analysis\n",
      "          for bicluster acquisition\n",
      "          <https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2881408/>`__.\n",
      "    \n",
      "    coverage_error(y_true, y_score, sample_weight=None)\n",
      "        Coverage error measure\n",
      "        \n",
      "        Compute how far we need to go through the ranked scores to cover all\n",
      "        true labels. The best value is equal to the average number\n",
      "        of labels in ``y_true`` per sample.\n",
      "        \n",
      "        Ties in ``y_scores`` are broken by giving maximal rank that would have\n",
      "        been assigned to all tied values.\n",
      "        \n",
      "        Note: Our implementation's score is 1 greater than the one given in\n",
      "        Tsoumakas et al., 2010. This extends it to handle the degenerate case\n",
      "        in which an instance has 0 true labels.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <coverage_error>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y_true : array, shape = [n_samples, n_labels]\n",
      "            True binary labels in binary indicator format.\n",
      "        \n",
      "        y_score : array, shape = [n_samples, n_labels]\n",
      "            Target scores, can either be probability estimates of the positive\n",
      "            class, confidence values, or non-thresholded measure of decisions\n",
      "            (as returned by \"decision_function\" on some classifiers).\n",
      "        \n",
      "        sample_weight : array-like of shape = [n_samples], optional\n",
      "            Sample weights.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        coverage_error : float\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] Tsoumakas, G., Katakis, I., & Vlahavas, I. (2010).\n",
      "               Mining multi-label data. In Data mining and knowledge discovery\n",
      "               handbook (pp. 667-685). Springer US.\n",
      "    \n",
      "    davies_bouldin_score(X, labels)\n",
      "        Computes the Davies-Bouldin score.\n",
      "        \n",
      "        The score is defined as the average similarity measure of each cluster with\n",
      "        its most similar cluster, where similarity is the ratio of within-cluster\n",
      "        distances to between-cluster distances. Thus, clusters which are farther\n",
      "        apart and less dispersed will result in a better score.\n",
      "        \n",
      "        The minimum score is zero, with lower values indicating better clustering.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <davies-bouldin_index>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        X : array-like, shape (``n_samples``, ``n_features``)\n",
      "            List of ``n_features``-dimensional data points. Each row corresponds\n",
      "            to a single data point.\n",
      "        \n",
      "        labels : array-like, shape (``n_samples``,)\n",
      "            Predicted labels for each sample.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        score: float\n",
      "            The resulting Davies-Bouldin score.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] Davies, David L.; Bouldin, Donald W. (1979).\n",
      "           `\"A Cluster Separation Measure\"\n",
      "           <https://ieeexplore.ieee.org/document/4766909>`__.\n",
      "           IEEE Transactions on Pattern Analysis and Machine Intelligence.\n",
      "           PAMI-1 (2): 224-227\n",
      "    \n",
      "    euclidean_distances(X, Y=None, Y_norm_squared=None, squared=False, X_norm_squared=None)\n",
      "        Considering the rows of X (and Y=X) as vectors, compute the\n",
      "        distance matrix between each pair of vectors.\n",
      "        \n",
      "        For efficiency reasons, the euclidean distance between a pair of row\n",
      "        vector x and y is computed as::\n",
      "        \n",
      "            dist(x, y) = sqrt(dot(x, x) - 2 * dot(x, y) + dot(y, y))\n",
      "        \n",
      "        This formulation has two advantages over other ways of computing distances.\n",
      "        First, it is computationally efficient when dealing with sparse data.\n",
      "        Second, if one argument varies but the other remains unchanged, then\n",
      "        `dot(x, x)` and/or `dot(y, y)` can be pre-computed.\n",
      "        \n",
      "        However, this is not the most precise way of doing this computation, and\n",
      "        the distance matrix returned by this function may not be exactly\n",
      "        symmetric as required by, e.g., ``scipy.spatial.distance`` functions.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <metrics>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        X : {array-like, sparse matrix}, shape (n_samples_1, n_features)\n",
      "        \n",
      "        Y : {array-like, sparse matrix}, shape (n_samples_2, n_features)\n",
      "        \n",
      "        Y_norm_squared : array-like, shape (n_samples_2, ), optional\n",
      "            Pre-computed dot-products of vectors in Y (e.g.,\n",
      "            ``(Y**2).sum(axis=1)``)\n",
      "            May be ignored in some cases, see the note below.\n",
      "        \n",
      "        squared : boolean, optional\n",
      "            Return squared Euclidean distances.\n",
      "        \n",
      "        X_norm_squared : array-like, shape = [n_samples_1], optional\n",
      "            Pre-computed dot-products of vectors in X (e.g.,\n",
      "            ``(X**2).sum(axis=1)``)\n",
      "            May be ignored in some cases, see the note below.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        To achieve better accuracy, `X_norm_squared` and `Y_norm_squared` may be\n",
      "        unused if they are passed as ``float32``.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        distances : array, shape (n_samples_1, n_samples_2)\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from sklearn.metrics.pairwise import euclidean_distances\n",
      "        >>> X = [[0, 1], [1, 1]]\n",
      "        >>> # distance between rows of X\n",
      "        >>> euclidean_distances(X, X)\n",
      "        array([[0., 1.],\n",
      "               [1., 0.]])\n",
      "        >>> # get distance to origin\n",
      "        >>> euclidean_distances(X, [[0, 0]])\n",
      "        array([[1.        ],\n",
      "               [1.41421356]])\n",
      "        \n",
      "        See also\n",
      "        --------\n",
      "        paired_distances : distances betweens pairs of elements of X and Y.\n",
      "    \n",
      "    explained_variance_score(y_true, y_pred, sample_weight=None, multioutput='uniform_average')\n",
      "        Explained variance regression score function\n",
      "        \n",
      "        Best possible score is 1.0, lower values are worse.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <explained_variance_score>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y_true : array-like of shape = (n_samples) or (n_samples, n_outputs)\n",
      "            Ground truth (correct) target values.\n",
      "        \n",
      "        y_pred : array-like of shape = (n_samples) or (n_samples, n_outputs)\n",
      "            Estimated target values.\n",
      "        \n",
      "        sample_weight : array-like of shape = (n_samples), optional\n",
      "            Sample weights.\n",
      "        \n",
      "        multioutput : string in ['raw_values', 'uniform_average',                 'variance_weighted'] or array-like of shape (n_outputs)\n",
      "            Defines aggregating of multiple output scores.\n",
      "            Array-like value defines weights used to average scores.\n",
      "        \n",
      "            'raw_values' :\n",
      "                Returns a full set of scores in case of multioutput input.\n",
      "        \n",
      "            'uniform_average' :\n",
      "                Scores of all outputs are averaged with uniform weight.\n",
      "        \n",
      "            'variance_weighted' :\n",
      "                Scores of all outputs are averaged, weighted by the variances\n",
      "                of each individual output.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        score : float or ndarray of floats\n",
      "            The explained variance or ndarray if 'multioutput' is 'raw_values'.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        This is not a symmetric function.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from sklearn.metrics import explained_variance_score\n",
      "        >>> y_true = [3, -0.5, 2, 7]\n",
      "        >>> y_pred = [2.5, 0.0, 2, 8]\n",
      "        >>> explained_variance_score(y_true, y_pred)  # doctest: +ELLIPSIS\n",
      "        0.957...\n",
      "        >>> y_true = [[0.5, 1], [-1, 1], [7, -6]]\n",
      "        >>> y_pred = [[0, 2], [-1, 2], [8, -5]]\n",
      "        >>> explained_variance_score(y_true, y_pred, multioutput='uniform_average')\n",
      "        ... # doctest: +ELLIPSIS\n",
      "        0.983...\n",
      "    \n",
      "    f1_score(y_true, y_pred, labels=None, pos_label=1, average='binary', sample_weight=None)\n",
      "        Compute the F1 score, also known as balanced F-score or F-measure\n",
      "        \n",
      "        The F1 score can be interpreted as a weighted average of the precision and\n",
      "        recall, where an F1 score reaches its best value at 1 and worst score at 0.\n",
      "        The relative contribution of precision and recall to the F1 score are\n",
      "        equal. The formula for the F1 score is::\n",
      "        \n",
      "            F1 = 2 * (precision * recall) / (precision + recall)\n",
      "        \n",
      "        In the multi-class and multi-label case, this is the average of\n",
      "        the F1 score of each class with weighting depending on the ``average``\n",
      "        parameter.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <precision_recall_f_measure_metrics>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y_true : 1d array-like, or label indicator array / sparse matrix\n",
      "            Ground truth (correct) target values.\n",
      "        \n",
      "        y_pred : 1d array-like, or label indicator array / sparse matrix\n",
      "            Estimated targets as returned by a classifier.\n",
      "        \n",
      "        labels : list, optional\n",
      "            The set of labels to include when ``average != 'binary'``, and their\n",
      "            order if ``average is None``. Labels present in the data can be\n",
      "            excluded, for example to calculate a multiclass average ignoring a\n",
      "            majority negative class, while labels not present in the data will\n",
      "            result in 0 components in a macro average. For multilabel targets,\n",
      "            labels are column indices. By default, all labels in ``y_true`` and\n",
      "            ``y_pred`` are used in sorted order.\n",
      "        \n",
      "            .. versionchanged:: 0.17\n",
      "               parameter *labels* improved for multiclass problem.\n",
      "        \n",
      "        pos_label : str or int, 1 by default\n",
      "            The class to report if ``average='binary'`` and the data is binary.\n",
      "            If the data are multiclass or multilabel, this will be ignored;\n",
      "            setting ``labels=[pos_label]`` and ``average != 'binary'`` will report\n",
      "            scores for that label only.\n",
      "        \n",
      "        average : string, [None, 'binary' (default), 'micro', 'macro', 'samples',                        'weighted']\n",
      "            This parameter is required for multiclass/multilabel targets.\n",
      "            If ``None``, the scores for each class are returned. Otherwise, this\n",
      "            determines the type of averaging performed on the data:\n",
      "        \n",
      "            ``'binary'``:\n",
      "                Only report results for the class specified by ``pos_label``.\n",
      "                This is applicable only if targets (``y_{true,pred}``) are binary.\n",
      "            ``'micro'``:\n",
      "                Calculate metrics globally by counting the total true positives,\n",
      "                false negatives and false positives.\n",
      "            ``'macro'``:\n",
      "                Calculate metrics for each label, and find their unweighted\n",
      "                mean.  This does not take label imbalance into account.\n",
      "            ``'weighted'``:\n",
      "                Calculate metrics for each label, and find their average weighted\n",
      "                by support (the number of true instances for each label). This\n",
      "                alters 'macro' to account for label imbalance; it can result in an\n",
      "                F-score that is not between precision and recall.\n",
      "            ``'samples'``:\n",
      "                Calculate metrics for each instance, and find their average (only\n",
      "                meaningful for multilabel classification where this differs from\n",
      "                :func:`accuracy_score`).\n",
      "        \n",
      "        sample_weight : array-like of shape = [n_samples], optional\n",
      "            Sample weights.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        f1_score : float or array of float, shape = [n_unique_labels]\n",
      "            F1 score of the positive class in binary classification or weighted\n",
      "            average of the F1 scores of each class for the multiclass task.\n",
      "        \n",
      "        See also\n",
      "        --------\n",
      "        fbeta_score, precision_recall_fscore_support, jaccard_score,\n",
      "        multilabel_confusion_matrix\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] `Wikipedia entry for the F1-score\n",
      "               <https://en.wikipedia.org/wiki/F1_score>`_\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from sklearn.metrics import f1_score\n",
      "        >>> y_true = [0, 1, 2, 0, 1, 2]\n",
      "        >>> y_pred = [0, 2, 1, 0, 0, 1]\n",
      "        >>> f1_score(y_true, y_pred, average='macro')  # doctest: +ELLIPSIS\n",
      "        0.26...\n",
      "        >>> f1_score(y_true, y_pred, average='micro')  # doctest: +ELLIPSIS\n",
      "        0.33...\n",
      "        >>> f1_score(y_true, y_pred, average='weighted')  # doctest: +ELLIPSIS\n",
      "        0.26...\n",
      "        >>> f1_score(y_true, y_pred, average=None)\n",
      "        array([0.8, 0. , 0. ])\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        When ``true positive + false positive == 0`` or\n",
      "        ``true positive + false negative == 0``, f-score returns 0 and raises\n",
      "        ``UndefinedMetricWarning``.\n",
      "    \n",
      "    fbeta_score(y_true, y_pred, beta, labels=None, pos_label=1, average='binary', sample_weight=None)\n",
      "        Compute the F-beta score\n",
      "        \n",
      "        The F-beta score is the weighted harmonic mean of precision and recall,\n",
      "        reaching its optimal value at 1 and its worst value at 0.\n",
      "        \n",
      "        The `beta` parameter determines the weight of recall in the combined\n",
      "        score. ``beta < 1`` lends more weight to precision, while ``beta > 1``\n",
      "        favors recall (``beta -> 0`` considers only precision, ``beta -> inf``\n",
      "        only recall).\n",
      "        \n",
      "        Read more in the :ref:`User Guide <precision_recall_f_measure_metrics>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y_true : 1d array-like, or label indicator array / sparse matrix\n",
      "            Ground truth (correct) target values.\n",
      "        \n",
      "        y_pred : 1d array-like, or label indicator array / sparse matrix\n",
      "            Estimated targets as returned by a classifier.\n",
      "        \n",
      "        beta : float\n",
      "            Weight of precision in harmonic mean.\n",
      "        \n",
      "        labels : list, optional\n",
      "            The set of labels to include when ``average != 'binary'``, and their\n",
      "            order if ``average is None``. Labels present in the data can be\n",
      "            excluded, for example to calculate a multiclass average ignoring a\n",
      "            majority negative class, while labels not present in the data will\n",
      "            result in 0 components in a macro average. For multilabel targets,\n",
      "            labels are column indices. By default, all labels in ``y_true`` and\n",
      "            ``y_pred`` are used in sorted order.\n",
      "        \n",
      "            .. versionchanged:: 0.17\n",
      "               parameter *labels* improved for multiclass problem.\n",
      "        \n",
      "        pos_label : str or int, 1 by default\n",
      "            The class to report if ``average='binary'`` and the data is binary.\n",
      "            If the data are multiclass or multilabel, this will be ignored;\n",
      "            setting ``labels=[pos_label]`` and ``average != 'binary'`` will report\n",
      "            scores for that label only.\n",
      "        \n",
      "        average : string, [None, 'binary' (default), 'micro', 'macro', 'samples',                        'weighted']\n",
      "            This parameter is required for multiclass/multilabel targets.\n",
      "            If ``None``, the scores for each class are returned. Otherwise, this\n",
      "            determines the type of averaging performed on the data:\n",
      "        \n",
      "            ``'binary'``:\n",
      "                Only report results for the class specified by ``pos_label``.\n",
      "                This is applicable only if targets (``y_{true,pred}``) are binary.\n",
      "            ``'micro'``:\n",
      "                Calculate metrics globally by counting the total true positives,\n",
      "                false negatives and false positives.\n",
      "            ``'macro'``:\n",
      "                Calculate metrics for each label, and find their unweighted\n",
      "                mean.  This does not take label imbalance into account.\n",
      "            ``'weighted'``:\n",
      "                Calculate metrics for each label, and find their average weighted\n",
      "                by support (the number of true instances for each label). This\n",
      "                alters 'macro' to account for label imbalance; it can result in an\n",
      "                F-score that is not between precision and recall.\n",
      "            ``'samples'``:\n",
      "                Calculate metrics for each instance, and find their average (only\n",
      "                meaningful for multilabel classification where this differs from\n",
      "                :func:`accuracy_score`).\n",
      "        \n",
      "        sample_weight : array-like of shape = [n_samples], optional\n",
      "            Sample weights.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        fbeta_score : float (if average is not None) or array of float, shape =        [n_unique_labels]\n",
      "            F-beta score of the positive class in binary classification or weighted\n",
      "            average of the F-beta score of each class for the multiclass task.\n",
      "        \n",
      "        See also\n",
      "        --------\n",
      "        precision_recall_fscore_support, multilabel_confusion_matrix\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] R. Baeza-Yates and B. Ribeiro-Neto (2011).\n",
      "               Modern Information Retrieval. Addison Wesley, pp. 327-328.\n",
      "        \n",
      "        .. [2] `Wikipedia entry for the F1-score\n",
      "               <https://en.wikipedia.org/wiki/F1_score>`_\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from sklearn.metrics import fbeta_score\n",
      "        >>> y_true = [0, 1, 2, 0, 1, 2]\n",
      "        >>> y_pred = [0, 2, 1, 0, 0, 1]\n",
      "        >>> fbeta_score(y_true, y_pred, average='macro', beta=0.5)\n",
      "        ... # doctest: +ELLIPSIS\n",
      "        0.23...\n",
      "        >>> fbeta_score(y_true, y_pred, average='micro', beta=0.5)\n",
      "        ... # doctest: +ELLIPSIS\n",
      "        0.33...\n",
      "        >>> fbeta_score(y_true, y_pred, average='weighted', beta=0.5)\n",
      "        ... # doctest: +ELLIPSIS\n",
      "        0.23...\n",
      "        >>> fbeta_score(y_true, y_pred, average=None, beta=0.5)\n",
      "        ... # doctest: +ELLIPSIS\n",
      "        array([0.71..., 0.        , 0.        ])\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        When ``true positive + false positive == 0`` or\n",
      "        ``true positive + false negative == 0``, f-score returns 0 and raises\n",
      "        ``UndefinedMetricWarning``.\n",
      "    \n",
      "    fowlkes_mallows_score(labels_true, labels_pred, sparse=False)\n",
      "        Measure the similarity of two clusterings of a set of points.\n",
      "        \n",
      "        The Fowlkes-Mallows index (FMI) is defined as the geometric mean between of\n",
      "        the precision and recall::\n",
      "        \n",
      "            FMI = TP / sqrt((TP + FP) * (TP + FN))\n",
      "        \n",
      "        Where ``TP`` is the number of **True Positive** (i.e. the number of pair of\n",
      "        points that belongs in the same clusters in both ``labels_true`` and\n",
      "        ``labels_pred``), ``FP`` is the number of **False Positive** (i.e. the\n",
      "        number of pair of points that belongs in the same clusters in\n",
      "        ``labels_true`` and not in ``labels_pred``) and ``FN`` is the number of\n",
      "        **False Negative** (i.e the number of pair of points that belongs in the\n",
      "        same clusters in ``labels_pred`` and not in ``labels_True``).\n",
      "        \n",
      "        The score ranges from 0 to 1. A high value indicates a good similarity\n",
      "        between two clusters.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <fowlkes_mallows_scores>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        labels_true : int array, shape = (``n_samples``,)\n",
      "            A clustering of the data into disjoint subsets.\n",
      "        \n",
      "        labels_pred : array, shape = (``n_samples``, )\n",
      "            A clustering of the data into disjoint subsets.\n",
      "        \n",
      "        sparse : bool\n",
      "            Compute contingency matrix internally with sparse matrix.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        score : float\n",
      "           The resulting Fowlkes-Mallows score.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        \n",
      "        Perfect labelings are both homogeneous and complete, hence have\n",
      "        score 1.0::\n",
      "        \n",
      "          >>> from sklearn.metrics.cluster import fowlkes_mallows_score\n",
      "          >>> fowlkes_mallows_score([0, 0, 1, 1], [0, 0, 1, 1])\n",
      "          1.0\n",
      "          >>> fowlkes_mallows_score([0, 0, 1, 1], [1, 1, 0, 0])\n",
      "          1.0\n",
      "        \n",
      "        If classes members are completely split across different clusters,\n",
      "        the assignment is totally random, hence the FMI is null::\n",
      "        \n",
      "          >>> fowlkes_mallows_score([0, 0, 0, 0], [0, 1, 2, 3])\n",
      "          0.0\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] `E. B. Fowkles and C. L. Mallows, 1983. \"A method for comparing two\n",
      "           hierarchical clusterings\". Journal of the American Statistical\n",
      "           Association\n",
      "           <http://wildfire.stat.ucla.edu/pdflibrary/fowlkes.pdf>`_\n",
      "        \n",
      "        .. [2] `Wikipedia entry for the Fowlkes-Mallows Index\n",
      "               <https://en.wikipedia.org/wiki/Fowlkes-Mallows_index>`_\n",
      "    \n",
      "    get_scorer(scoring)\n",
      "        Get a scorer from string\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        scoring : str | callable\n",
      "            scoring method as string. If callable it is returned as is.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        scorer : callable\n",
      "            The scorer.\n",
      "    \n",
      "    hamming_loss(y_true, y_pred, labels=None, sample_weight=None)\n",
      "        Compute the average Hamming loss.\n",
      "        \n",
      "        The Hamming loss is the fraction of labels that are incorrectly predicted.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <hamming_loss>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y_true : 1d array-like, or label indicator array / sparse matrix\n",
      "            Ground truth (correct) labels.\n",
      "        \n",
      "        y_pred : 1d array-like, or label indicator array / sparse matrix\n",
      "            Predicted labels, as returned by a classifier.\n",
      "        \n",
      "        labels : array, shape = [n_labels], optional (default='deprecated')\n",
      "            Integer array of labels. If not provided, labels will be inferred\n",
      "            from y_true and y_pred.\n",
      "        \n",
      "            .. versionadded:: 0.18\n",
      "            .. deprecated:: 0.21\n",
      "               This parameter ``labels`` is deprecated in version 0.21 and will\n",
      "               be removed in version 0.23. Hamming loss uses ``y_true.shape[1]``\n",
      "               for the number of labels when y_true is binary label indicators,\n",
      "               so it is unnecessary for the user to specify.\n",
      "        \n",
      "        sample_weight : array-like of shape = [n_samples], optional\n",
      "            Sample weights.\n",
      "        \n",
      "            .. versionadded:: 0.18\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        loss : float or int,\n",
      "            Return the average Hamming loss between element of ``y_true`` and\n",
      "            ``y_pred``.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        accuracy_score, jaccard_score, zero_one_loss\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        In multiclass classification, the Hamming loss corresponds to the Hamming\n",
      "        distance between ``y_true`` and ``y_pred`` which is equivalent to the\n",
      "        subset ``zero_one_loss`` function, when `normalize` parameter is set to\n",
      "        True.\n",
      "        \n",
      "        In multilabel classification, the Hamming loss is different from the\n",
      "        subset zero-one loss. The zero-one loss considers the entire set of labels\n",
      "        for a given sample incorrect if it does not entirely match the true set of\n",
      "        labels. Hamming loss is more forgiving in that it penalizes only the\n",
      "        individual labels.\n",
      "        \n",
      "        The Hamming loss is upperbounded by the subset zero-one loss, when\n",
      "        `normalize` parameter is set to True. It is always between 0 and 1,\n",
      "        lower being better.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] Grigorios Tsoumakas, Ioannis Katakis. Multi-Label Classification:\n",
      "               An Overview. International Journal of Data Warehousing & Mining,\n",
      "               3(3), 1-13, July-September 2007.\n",
      "        \n",
      "        .. [2] `Wikipedia entry on the Hamming distance\n",
      "               <https://en.wikipedia.org/wiki/Hamming_distance>`_\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from sklearn.metrics import hamming_loss\n",
      "        >>> y_pred = [1, 2, 3, 4]\n",
      "        >>> y_true = [2, 2, 3, 4]\n",
      "        >>> hamming_loss(y_true, y_pred)\n",
      "        0.25\n",
      "        \n",
      "        In the multilabel case with binary label indicators:\n",
      "        \n",
      "        >>> import numpy as np\n",
      "        >>> hamming_loss(np.array([[0, 1], [1, 1]]), np.zeros((2, 2)))\n",
      "        0.75\n",
      "    \n",
      "    hinge_loss(y_true, pred_decision, labels=None, sample_weight=None)\n",
      "        Average hinge loss (non-regularized)\n",
      "        \n",
      "        In binary class case, assuming labels in y_true are encoded with +1 and -1,\n",
      "        when a prediction mistake is made, ``margin = y_true * pred_decision`` is\n",
      "        always negative (since the signs disagree), implying ``1 - margin`` is\n",
      "        always greater than 1.  The cumulated hinge loss is therefore an upper\n",
      "        bound of the number of mistakes made by the classifier.\n",
      "        \n",
      "        In multiclass case, the function expects that either all the labels are\n",
      "        included in y_true or an optional labels argument is provided which\n",
      "        contains all the labels. The multilabel margin is calculated according\n",
      "        to Crammer-Singer's method. As in the binary case, the cumulated hinge loss\n",
      "        is an upper bound of the number of mistakes made by the classifier.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <hinge_loss>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y_true : array, shape = [n_samples]\n",
      "            True target, consisting of integers of two values. The positive label\n",
      "            must be greater than the negative label.\n",
      "        \n",
      "        pred_decision : array, shape = [n_samples] or [n_samples, n_classes]\n",
      "            Predicted decisions, as output by decision_function (floats).\n",
      "        \n",
      "        labels : array, optional, default None\n",
      "            Contains all the labels for the problem. Used in multiclass hinge loss.\n",
      "        \n",
      "        sample_weight : array-like of shape = [n_samples], optional\n",
      "            Sample weights.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        loss : float\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] `Wikipedia entry on the Hinge loss\n",
      "               <https://en.wikipedia.org/wiki/Hinge_loss>`_\n",
      "        \n",
      "        .. [2] Koby Crammer, Yoram Singer. On the Algorithmic\n",
      "               Implementation of Multiclass Kernel-based Vector\n",
      "               Machines. Journal of Machine Learning Research 2,\n",
      "               (2001), 265-292\n",
      "        \n",
      "        .. [3] `L1 AND L2 Regularization for Multiclass Hinge Loss Models\n",
      "               by Robert C. Moore, John DeNero.\n",
      "               <http://www.ttic.edu/sigml/symposium2011/papers/\n",
      "               Moore+DeNero_Regularization.pdf>`_\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from sklearn import svm\n",
      "        >>> from sklearn.metrics import hinge_loss\n",
      "        >>> X = [[0], [1]]\n",
      "        >>> y = [-1, 1]\n",
      "        >>> est = svm.LinearSVC(random_state=0)\n",
      "        >>> est.fit(X, y)  # doctest: +NORMALIZE_WHITESPACE\n",
      "        LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
      "             intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
      "             multi_class='ovr', penalty='l2', random_state=0, tol=0.0001,\n",
      "             verbose=0)\n",
      "        >>> pred_decision = est.decision_function([[-2], [3], [0.5]])\n",
      "        >>> pred_decision  # doctest: +ELLIPSIS\n",
      "        array([-2.18...,  2.36...,  0.09...])\n",
      "        >>> hinge_loss([-1, 1, 1], pred_decision)  # doctest: +ELLIPSIS\n",
      "        0.30...\n",
      "        \n",
      "        In the multiclass case:\n",
      "        \n",
      "        >>> import numpy as np\n",
      "        >>> X = np.array([[0], [1], [2], [3]])\n",
      "        >>> Y = np.array([0, 1, 2, 3])\n",
      "        >>> labels = np.array([0, 1, 2, 3])\n",
      "        >>> est = svm.LinearSVC()\n",
      "        >>> est.fit(X, Y)  # doctest: +NORMALIZE_WHITESPACE\n",
      "        LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
      "             intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
      "             multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
      "             verbose=0)\n",
      "        >>> pred_decision = est.decision_function([[-1], [2], [3]])\n",
      "        >>> y_true = [0, 2, 3]\n",
      "        >>> hinge_loss(y_true, pred_decision, labels)  #doctest: +ELLIPSIS\n",
      "        0.56...\n",
      "    \n",
      "    homogeneity_completeness_v_measure(labels_true, labels_pred, beta=1.0)\n",
      "        Compute the homogeneity and completeness and V-Measure scores at once.\n",
      "        \n",
      "        Those metrics are based on normalized conditional entropy measures of\n",
      "        the clustering labeling to evaluate given the knowledge of a Ground\n",
      "        Truth class labels of the same samples.\n",
      "        \n",
      "        A clustering result satisfies homogeneity if all of its clusters\n",
      "        contain only data points which are members of a single class.\n",
      "        \n",
      "        A clustering result satisfies completeness if all the data points\n",
      "        that are members of a given class are elements of the same cluster.\n",
      "        \n",
      "        Both scores have positive values between 0.0 and 1.0, larger values\n",
      "        being desirable.\n",
      "        \n",
      "        Those 3 metrics are independent of the absolute values of the labels:\n",
      "        a permutation of the class or cluster label values won't change the\n",
      "        score values in any way.\n",
      "        \n",
      "        V-Measure is furthermore symmetric: swapping ``labels_true`` and\n",
      "        ``label_pred`` will give the same score. This does not hold for\n",
      "        homogeneity and completeness. V-Measure is identical to\n",
      "        :func:`normalized_mutual_info_score` with the arithmetic averaging\n",
      "        method.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <homogeneity_completeness>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        labels_true : int array, shape = [n_samples]\n",
      "            ground truth class labels to be used as a reference\n",
      "        \n",
      "        labels_pred : array, shape = [n_samples]\n",
      "            cluster labels to evaluate\n",
      "        \n",
      "        beta : float\n",
      "            Ratio of weight attributed to ``homogeneity`` vs ``completeness``.\n",
      "            If ``beta`` is greater than 1, ``completeness`` is weighted more\n",
      "            strongly in the calculation. If ``beta`` is less than 1,\n",
      "            ``homogeneity`` is weighted more strongly.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        homogeneity : float\n",
      "           score between 0.0 and 1.0. 1.0 stands for perfectly homogeneous labeling\n",
      "        \n",
      "        completeness : float\n",
      "           score between 0.0 and 1.0. 1.0 stands for perfectly complete labeling\n",
      "        \n",
      "        v_measure : float\n",
      "            harmonic mean of the first two\n",
      "        \n",
      "        See also\n",
      "        --------\n",
      "        homogeneity_score\n",
      "        completeness_score\n",
      "        v_measure_score\n",
      "    \n",
      "    homogeneity_score(labels_true, labels_pred)\n",
      "        Homogeneity metric of a cluster labeling given a ground truth.\n",
      "        \n",
      "        A clustering result satisfies homogeneity if all of its clusters\n",
      "        contain only data points which are members of a single class.\n",
      "        \n",
      "        This metric is independent of the absolute values of the labels:\n",
      "        a permutation of the class or cluster label values won't change the\n",
      "        score value in any way.\n",
      "        \n",
      "        This metric is not symmetric: switching ``label_true`` with ``label_pred``\n",
      "        will return the :func:`completeness_score` which will be different in\n",
      "        general.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <homogeneity_completeness>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        labels_true : int array, shape = [n_samples]\n",
      "            ground truth class labels to be used as a reference\n",
      "        \n",
      "        labels_pred : array, shape = [n_samples]\n",
      "            cluster labels to evaluate\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        homogeneity : float\n",
      "           score between 0.0 and 1.0. 1.0 stands for perfectly homogeneous labeling\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        \n",
      "        .. [1] `Andrew Rosenberg and Julia Hirschberg, 2007. V-Measure: A\n",
      "           conditional entropy-based external cluster evaluation measure\n",
      "           <https://aclweb.org/anthology/D/D07/D07-1043.pdf>`_\n",
      "        \n",
      "        See also\n",
      "        --------\n",
      "        completeness_score\n",
      "        v_measure_score\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        \n",
      "        Perfect labelings are homogeneous::\n",
      "        \n",
      "          >>> from sklearn.metrics.cluster import homogeneity_score\n",
      "          >>> homogeneity_score([0, 0, 1, 1], [1, 1, 0, 0])\n",
      "          1.0\n",
      "        \n",
      "        Non-perfect labelings that further split classes into more clusters can be\n",
      "        perfectly homogeneous::\n",
      "        \n",
      "          >>> print(\"%.6f\" % homogeneity_score([0, 0, 1, 1], [0, 0, 1, 2]))\n",
      "          ...                                                  # doctest: +ELLIPSIS\n",
      "          1.000000\n",
      "          >>> print(\"%.6f\" % homogeneity_score([0, 0, 1, 1], [0, 1, 2, 3]))\n",
      "          ...                                                  # doctest: +ELLIPSIS\n",
      "          1.000000\n",
      "        \n",
      "        Clusters that include samples from different classes do not make for an\n",
      "        homogeneous labeling::\n",
      "        \n",
      "          >>> print(\"%.6f\" % homogeneity_score([0, 0, 1, 1], [0, 1, 0, 1]))\n",
      "          ...                                                  # doctest: +ELLIPSIS\n",
      "          0.0...\n",
      "          >>> print(\"%.6f\" % homogeneity_score([0, 0, 1, 1], [0, 0, 0, 0]))\n",
      "          ...                                                  # doctest: +ELLIPSIS\n",
      "          0.0...\n",
      "    \n",
      "    jaccard_score(y_true, y_pred, labels=None, pos_label=1, average='binary', sample_weight=None)\n",
      "        Jaccard similarity coefficient score\n",
      "        \n",
      "        The Jaccard index [1], or Jaccard similarity coefficient, defined as\n",
      "        the size of the intersection divided by the size of the union of two label\n",
      "        sets, is used to compare set of predicted labels for a sample to the\n",
      "        corresponding set of labels in ``y_true``.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <jaccard_score>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y_true : 1d array-like, or label indicator array / sparse matrix\n",
      "            Ground truth (correct) labels.\n",
      "        \n",
      "        y_pred : 1d array-like, or label indicator array / sparse matrix\n",
      "            Predicted labels, as returned by a classifier.\n",
      "        \n",
      "        labels : list, optional\n",
      "            The set of labels to include when ``average != 'binary'``, and their\n",
      "            order if ``average is None``. Labels present in the data can be\n",
      "            excluded, for example to calculate a multiclass average ignoring a\n",
      "            majority negative class, while labels not present in the data will\n",
      "            result in 0 components in a macro average. For multilabel targets,\n",
      "            labels are column indices. By default, all labels in ``y_true`` and\n",
      "            ``y_pred`` are used in sorted order.\n",
      "        \n",
      "        pos_label : str or int, 1 by default\n",
      "            The class to report if ``average='binary'`` and the data is binary.\n",
      "            If the data are multiclass or multilabel, this will be ignored;\n",
      "            setting ``labels=[pos_label]`` and ``average != 'binary'`` will report\n",
      "            scores for that label only.\n",
      "        \n",
      "        average : string, [None, 'binary' (default), 'micro', 'macro', 'samples',                        'weighted']\n",
      "            If ``None``, the scores for each class are returned. Otherwise, this\n",
      "            determines the type of averaging performed on the data:\n",
      "        \n",
      "            ``'binary'``:\n",
      "                Only report results for the class specified by ``pos_label``.\n",
      "                This is applicable only if targets (``y_{true,pred}``) are binary.\n",
      "            ``'micro'``:\n",
      "                Calculate metrics globally by counting the total true positives,\n",
      "                false negatives and false positives.\n",
      "            ``'macro'``:\n",
      "                Calculate metrics for each label, and find their unweighted\n",
      "                mean.  This does not take label imbalance into account.\n",
      "            ``'weighted'``:\n",
      "                Calculate metrics for each label, and find their average, weighted\n",
      "                by support (the number of true instances for each label). This\n",
      "                alters 'macro' to account for label imbalance.\n",
      "            ``'samples'``:\n",
      "                Calculate metrics for each instance, and find their average (only\n",
      "                meaningful for multilabel classification).\n",
      "        \n",
      "        sample_weight : array-like of shape = [n_samples], optional\n",
      "            Sample weights.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        score : float (if average is not None) or array of floats, shape =            [n_unique_labels]\n",
      "        \n",
      "        See also\n",
      "        --------\n",
      "        accuracy_score, f_score, multilabel_confusion_matrix\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        :func:`jaccard_score` may be a poor metric if there are no\n",
      "        positives for some samples or classes. Jaccard is undefined if there are\n",
      "        no true or predicted labels, and our implementation will return a score\n",
      "        of 0 with a warning.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] `Wikipedia entry for the Jaccard index\n",
      "               <https://en.wikipedia.org/wiki/Jaccard_index>`_\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> import numpy as np\n",
      "        >>> from sklearn.metrics import jaccard_score\n",
      "        >>> y_true = np.array([[0, 1, 1],\n",
      "        ...                    [1, 1, 0]])\n",
      "        >>> y_pred = np.array([[1, 1, 1],\n",
      "        ...                    [1, 0, 0]])\n",
      "        \n",
      "        In the binary case:\n",
      "        \n",
      "        >>> jaccard_score(y_true[0], y_pred[0])  # doctest: +ELLIPSIS\n",
      "        0.6666...\n",
      "        \n",
      "        In the multilabel case:\n",
      "        \n",
      "        >>> jaccard_score(y_true, y_pred, average='samples')\n",
      "        0.5833...\n",
      "        >>> jaccard_score(y_true, y_pred, average='macro')\n",
      "        0.6666...\n",
      "        >>> jaccard_score(y_true, y_pred, average=None)\n",
      "        array([0.5, 0.5, 1. ])\n",
      "        \n",
      "        In the multiclass case:\n",
      "        \n",
      "        >>> y_pred = [0, 2, 1, 2]\n",
      "        >>> y_true = [0, 1, 2, 2]\n",
      "        >>> jaccard_score(y_true, y_pred, average=None)\n",
      "        ... # doctest: +NORMALIZE_WHITESPACE +ELLIPSIS\n",
      "        array([1. , 0. , 0.33...])\n",
      "    \n",
      "    jaccard_similarity_score(y_true, y_pred, normalize=True, sample_weight=None)\n",
      "        Jaccard similarity coefficient score\n",
      "        \n",
      "        .. deprecated:: 0.21\n",
      "            This is deprecated to be removed in 0.23, since its handling of\n",
      "            binary and multiclass inputs was broken. `jaccard_score` has an API\n",
      "            that is consistent with precision_score, f_score, etc.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <jaccard_similarity_score>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y_true : 1d array-like, or label indicator array / sparse matrix\n",
      "            Ground truth (correct) labels.\n",
      "        \n",
      "        y_pred : 1d array-like, or label indicator array / sparse matrix\n",
      "            Predicted labels, as returned by a classifier.\n",
      "        \n",
      "        normalize : bool, optional (default=True)\n",
      "            If ``False``, return the sum of the Jaccard similarity coefficient\n",
      "            over the sample set. Otherwise, return the average of Jaccard\n",
      "            similarity coefficient.\n",
      "        \n",
      "        sample_weight : array-like of shape = [n_samples], optional\n",
      "            Sample weights.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        score : float\n",
      "            If ``normalize == True``, return the average Jaccard similarity\n",
      "            coefficient, else it returns the sum of the Jaccard similarity\n",
      "            coefficient over the sample set.\n",
      "        \n",
      "            The best performance is 1 with ``normalize == True`` and the number\n",
      "            of samples with ``normalize == False``.\n",
      "        \n",
      "        See also\n",
      "        --------\n",
      "        accuracy_score, hamming_loss, zero_one_loss\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        In binary and multiclass classification, this function is equivalent\n",
      "        to the ``accuracy_score``. It differs in the multilabel classification\n",
      "        problem.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] `Wikipedia entry for the Jaccard index\n",
      "               <https://en.wikipedia.org/wiki/Jaccard_index>`_\n",
      "    \n",
      "    label_ranking_average_precision_score(y_true, y_score, sample_weight=None)\n",
      "        Compute ranking-based average precision\n",
      "        \n",
      "        Label ranking average precision (LRAP) is the average over each ground\n",
      "        truth label assigned to each sample, of the ratio of true vs. total\n",
      "        labels with lower score.\n",
      "        \n",
      "        This metric is used in multilabel ranking problem, where the goal\n",
      "        is to give better rank to the labels associated to each sample.\n",
      "        \n",
      "        The obtained score is always strictly greater than 0 and\n",
      "        the best value is 1.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <label_ranking_average_precision>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y_true : array or sparse matrix, shape = [n_samples, n_labels]\n",
      "            True binary labels in binary indicator format.\n",
      "        \n",
      "        y_score : array, shape = [n_samples, n_labels]\n",
      "            Target scores, can either be probability estimates of the positive\n",
      "            class, confidence values, or non-thresholded measure of decisions\n",
      "            (as returned by \"decision_function\" on some classifiers).\n",
      "        \n",
      "        sample_weight : array-like of shape = [n_samples], optional\n",
      "            Sample weights.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        score : float\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> import numpy as np\n",
      "        >>> from sklearn.metrics import label_ranking_average_precision_score\n",
      "        >>> y_true = np.array([[1, 0, 0], [0, 0, 1]])\n",
      "        >>> y_score = np.array([[0.75, 0.5, 1], [1, 0.2, 0.1]])\n",
      "        >>> label_ranking_average_precision_score(y_true, y_score)         # doctest: +ELLIPSIS\n",
      "        0.416...\n",
      "    \n",
      "    label_ranking_loss(y_true, y_score, sample_weight=None)\n",
      "        Compute Ranking loss measure\n",
      "        \n",
      "        Compute the average number of label pairs that are incorrectly ordered\n",
      "        given y_score weighted by the size of the label set and the number of\n",
      "        labels not in the label set.\n",
      "        \n",
      "        This is similar to the error set size, but weighted by the number of\n",
      "        relevant and irrelevant labels. The best performance is achieved with\n",
      "        a ranking loss of zero.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <label_ranking_loss>`.\n",
      "        \n",
      "        .. versionadded:: 0.17\n",
      "           A function *label_ranking_loss*\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y_true : array or sparse matrix, shape = [n_samples, n_labels]\n",
      "            True binary labels in binary indicator format.\n",
      "        \n",
      "        y_score : array, shape = [n_samples, n_labels]\n",
      "            Target scores, can either be probability estimates of the positive\n",
      "            class, confidence values, or non-thresholded measure of decisions\n",
      "            (as returned by \"decision_function\" on some classifiers).\n",
      "        \n",
      "        sample_weight : array-like of shape = [n_samples], optional\n",
      "            Sample weights.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        loss : float\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] Tsoumakas, G., Katakis, I., & Vlahavas, I. (2010).\n",
      "               Mining multi-label data. In Data mining and knowledge discovery\n",
      "               handbook (pp. 667-685). Springer US.\n",
      "    \n",
      "    log_loss(y_true, y_pred, eps=1e-15, normalize=True, sample_weight=None, labels=None)\n",
      "        Log loss, aka logistic loss or cross-entropy loss.\n",
      "        \n",
      "        This is the loss function used in (multinomial) logistic regression\n",
      "        and extensions of it such as neural networks, defined as the negative\n",
      "        log-likelihood of the true labels given a probabilistic classifier's\n",
      "        predictions. The log loss is only defined for two or more labels.\n",
      "        For a single sample with true label yt in {0,1} and\n",
      "        estimated probability yp that yt = 1, the log loss is\n",
      "        \n",
      "            -log P(yt|yp) = -(yt log(yp) + (1 - yt) log(1 - yp))\n",
      "        \n",
      "        Read more in the :ref:`User Guide <log_loss>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y_true : array-like or label indicator matrix\n",
      "            Ground truth (correct) labels for n_samples samples.\n",
      "        \n",
      "        y_pred : array-like of float, shape = (n_samples, n_classes) or (n_samples,)\n",
      "            Predicted probabilities, as returned by a classifier's\n",
      "            predict_proba method. If ``y_pred.shape = (n_samples,)``\n",
      "            the probabilities provided are assumed to be that of the\n",
      "            positive class. The labels in ``y_pred`` are assumed to be\n",
      "            ordered alphabetically, as done by\n",
      "            :class:`preprocessing.LabelBinarizer`.\n",
      "        \n",
      "        eps : float\n",
      "            Log loss is undefined for p=0 or p=1, so probabilities are\n",
      "            clipped to max(eps, min(1 - eps, p)).\n",
      "        \n",
      "        normalize : bool, optional (default=True)\n",
      "            If true, return the mean loss per sample.\n",
      "            Otherwise, return the sum of the per-sample losses.\n",
      "        \n",
      "        sample_weight : array-like of shape = [n_samples], optional\n",
      "            Sample weights.\n",
      "        \n",
      "        labels : array-like, optional (default=None)\n",
      "            If not provided, labels will be inferred from y_true. If ``labels``\n",
      "            is ``None`` and ``y_pred`` has shape (n_samples,) the labels are\n",
      "            assumed to be binary and are inferred from ``y_true``.\n",
      "            .. versionadded:: 0.18\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        loss : float\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from sklearn.metrics import log_loss\n",
      "        >>> log_loss([\"spam\", \"ham\", \"ham\", \"spam\"],  # doctest: +ELLIPSIS\n",
      "        ...          [[.1, .9], [.9, .1], [.8, .2], [.35, .65]])\n",
      "        0.21616...\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        C.M. Bishop (2006). Pattern Recognition and Machine Learning. Springer,\n",
      "        p. 209.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        The logarithm used is the natural logarithm (base-e).\n",
      "    \n",
      "    make_scorer(score_func, greater_is_better=True, needs_proba=False, needs_threshold=False, **kwargs)\n",
      "        Make a scorer from a performance metric or loss function.\n",
      "        \n",
      "        This factory function wraps scoring functions for use in GridSearchCV\n",
      "        and cross_val_score. It takes a score function, such as ``accuracy_score``,\n",
      "        ``mean_squared_error``, ``adjusted_rand_index`` or ``average_precision``\n",
      "        and returns a callable that scores an estimator's output.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <scoring>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        score_func : callable,\n",
      "            Score function (or loss function) with signature\n",
      "            ``score_func(y, y_pred, **kwargs)``.\n",
      "        \n",
      "        greater_is_better : boolean, default=True\n",
      "            Whether score_func is a score function (default), meaning high is good,\n",
      "            or a loss function, meaning low is good. In the latter case, the\n",
      "            scorer object will sign-flip the outcome of the score_func.\n",
      "        \n",
      "        needs_proba : boolean, default=False\n",
      "            Whether score_func requires predict_proba to get probability estimates\n",
      "            out of a classifier.\n",
      "        \n",
      "        needs_threshold : boolean, default=False\n",
      "            Whether score_func takes a continuous decision certainty.\n",
      "            This only works for binary classification using estimators that\n",
      "            have either a decision_function or predict_proba method.\n",
      "        \n",
      "            For example ``average_precision`` or the area under the roc curve\n",
      "            can not be computed using discrete predictions alone.\n",
      "        \n",
      "        **kwargs : additional arguments\n",
      "            Additional parameters to be passed to score_func.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        scorer : callable\n",
      "            Callable object that returns a scalar score; greater is better.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from sklearn.metrics import fbeta_score, make_scorer\n",
      "        >>> ftwo_scorer = make_scorer(fbeta_score, beta=2)\n",
      "        >>> ftwo_scorer\n",
      "        make_scorer(fbeta_score, beta=2)\n",
      "        >>> from sklearn.model_selection import GridSearchCV\n",
      "        >>> from sklearn.svm import LinearSVC\n",
      "        >>> grid = GridSearchCV(LinearSVC(), param_grid={'C': [1, 10]},\n",
      "        ...                     scoring=ftwo_scorer)\n",
      "    \n",
      "    matthews_corrcoef(y_true, y_pred, sample_weight=None)\n",
      "        Compute the Matthews correlation coefficient (MCC)\n",
      "        \n",
      "        The Matthews correlation coefficient is used in machine learning as a\n",
      "        measure of the quality of binary and multiclass classifications. It takes\n",
      "        into account true and false positives and negatives and is generally\n",
      "        regarded as a balanced measure which can be used even if the classes are of\n",
      "        very different sizes. The MCC is in essence a correlation coefficient value\n",
      "        between -1 and +1. A coefficient of +1 represents a perfect prediction, 0\n",
      "        an average random prediction and -1 an inverse prediction.  The statistic\n",
      "        is also known as the phi coefficient. [source: Wikipedia]\n",
      "        \n",
      "        Binary and multiclass labels are supported.  Only in the binary case does\n",
      "        this relate to information about true and false positives and negatives.\n",
      "        See references below.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <matthews_corrcoef>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y_true : array, shape = [n_samples]\n",
      "            Ground truth (correct) target values.\n",
      "        \n",
      "        y_pred : array, shape = [n_samples]\n",
      "            Estimated targets as returned by a classifier.\n",
      "        \n",
      "        sample_weight : array-like of shape = [n_samples], default None\n",
      "            Sample weights.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        mcc : float\n",
      "            The Matthews correlation coefficient (+1 represents a perfect\n",
      "            prediction, 0 an average random prediction and -1 and inverse\n",
      "            prediction).\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] `Baldi, Brunak, Chauvin, Andersen and Nielsen, (2000). Assessing the\n",
      "           accuracy of prediction algorithms for classification: an overview\n",
      "           <https://doi.org/10.1093/bioinformatics/16.5.412>`_\n",
      "        \n",
      "        .. [2] `Wikipedia entry for the Matthews Correlation Coefficient\n",
      "           <https://en.wikipedia.org/wiki/Matthews_correlation_coefficient>`_\n",
      "        \n",
      "        .. [3] `Gorodkin, (2004). Comparing two K-category assignments by a\n",
      "            K-category correlation coefficient\n",
      "            <https://www.sciencedirect.com/science/article/pii/S1476927104000799>`_\n",
      "        \n",
      "        .. [4] `Jurman, Riccadonna, Furlanello, (2012). A Comparison of MCC and CEN\n",
      "            Error Measures in MultiClass Prediction\n",
      "            <https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0041882>`_\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from sklearn.metrics import matthews_corrcoef\n",
      "        >>> y_true = [+1, +1, +1, -1]\n",
      "        >>> y_pred = [+1, -1, +1, +1]\n",
      "        >>> matthews_corrcoef(y_true, y_pred)  # doctest: +ELLIPSIS\n",
      "        -0.33...\n",
      "    \n",
      "    max_error(y_true, y_pred)\n",
      "        max_error metric calculates the maximum residual error.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <max_error>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y_true : array-like of shape = (n_samples)\n",
      "            Ground truth (correct) target values.\n",
      "        \n",
      "        y_pred : array-like of shape = (n_samples)\n",
      "            Estimated target values.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        max_error : float\n",
      "            A positive floating point value (the best value is 0.0).\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from sklearn.metrics import max_error\n",
      "        >>> y_true = [3, 2, 7, 1]\n",
      "        >>> y_pred = [4, 2, 7, 1]\n",
      "        >>> max_error(y_true, y_pred)\n",
      "        1\n",
      "    \n",
      "    mean_absolute_error(y_true, y_pred, sample_weight=None, multioutput='uniform_average')\n",
      "        Mean absolute error regression loss\n",
      "        \n",
      "        Read more in the :ref:`User Guide <mean_absolute_error>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y_true : array-like of shape = (n_samples) or (n_samples, n_outputs)\n",
      "            Ground truth (correct) target values.\n",
      "        \n",
      "        y_pred : array-like of shape = (n_samples) or (n_samples, n_outputs)\n",
      "            Estimated target values.\n",
      "        \n",
      "        sample_weight : array-like of shape = (n_samples), optional\n",
      "            Sample weights.\n",
      "        \n",
      "        multioutput : string in ['raw_values', 'uniform_average']\n",
      "            or array-like of shape (n_outputs)\n",
      "            Defines aggregating of multiple output values.\n",
      "            Array-like value defines weights used to average errors.\n",
      "        \n",
      "            'raw_values' :\n",
      "                Returns a full set of errors in case of multioutput input.\n",
      "        \n",
      "            'uniform_average' :\n",
      "                Errors of all outputs are averaged with uniform weight.\n",
      "        \n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        loss : float or ndarray of floats\n",
      "            If multioutput is 'raw_values', then mean absolute error is returned\n",
      "            for each output separately.\n",
      "            If multioutput is 'uniform_average' or an ndarray of weights, then the\n",
      "            weighted average of all output errors is returned.\n",
      "        \n",
      "            MAE output is non-negative floating point. The best value is 0.0.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from sklearn.metrics import mean_absolute_error\n",
      "        >>> y_true = [3, -0.5, 2, 7]\n",
      "        >>> y_pred = [2.5, 0.0, 2, 8]\n",
      "        >>> mean_absolute_error(y_true, y_pred)\n",
      "        0.5\n",
      "        >>> y_true = [[0.5, 1], [-1, 1], [7, -6]]\n",
      "        >>> y_pred = [[0, 2], [-1, 2], [8, -5]]\n",
      "        >>> mean_absolute_error(y_true, y_pred)\n",
      "        0.75\n",
      "        >>> mean_absolute_error(y_true, y_pred, multioutput='raw_values')\n",
      "        array([0.5, 1. ])\n",
      "        >>> mean_absolute_error(y_true, y_pred, multioutput=[0.3, 0.7])\n",
      "        ... # doctest: +ELLIPSIS\n",
      "        0.85...\n",
      "    \n",
      "    mean_squared_error(y_true, y_pred, sample_weight=None, multioutput='uniform_average')\n",
      "        Mean squared error regression loss\n",
      "        \n",
      "        Read more in the :ref:`User Guide <mean_squared_error>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y_true : array-like of shape = (n_samples) or (n_samples, n_outputs)\n",
      "            Ground truth (correct) target values.\n",
      "        \n",
      "        y_pred : array-like of shape = (n_samples) or (n_samples, n_outputs)\n",
      "            Estimated target values.\n",
      "        \n",
      "        sample_weight : array-like of shape = (n_samples), optional\n",
      "            Sample weights.\n",
      "        \n",
      "        multioutput : string in ['raw_values', 'uniform_average']\n",
      "            or array-like of shape (n_outputs)\n",
      "            Defines aggregating of multiple output values.\n",
      "            Array-like value defines weights used to average errors.\n",
      "        \n",
      "            'raw_values' :\n",
      "                Returns a full set of errors in case of multioutput input.\n",
      "        \n",
      "            'uniform_average' :\n",
      "                Errors of all outputs are averaged with uniform weight.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        loss : float or ndarray of floats\n",
      "            A non-negative floating point value (the best value is 0.0), or an\n",
      "            array of floating point values, one for each individual target.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from sklearn.metrics import mean_squared_error\n",
      "        >>> y_true = [3, -0.5, 2, 7]\n",
      "        >>> y_pred = [2.5, 0.0, 2, 8]\n",
      "        >>> mean_squared_error(y_true, y_pred)\n",
      "        0.375\n",
      "        >>> y_true = [[0.5, 1],[-1, 1],[7, -6]]\n",
      "        >>> y_pred = [[0, 2],[-1, 2],[8, -5]]\n",
      "        >>> mean_squared_error(y_true, y_pred)  # doctest: +ELLIPSIS\n",
      "        0.708...\n",
      "        >>> mean_squared_error(y_true, y_pred, multioutput='raw_values')\n",
      "        ... # doctest: +ELLIPSIS\n",
      "        array([0.41666667, 1.        ])\n",
      "        >>> mean_squared_error(y_true, y_pred, multioutput=[0.3, 0.7])\n",
      "        ... # doctest: +ELLIPSIS\n",
      "        0.825...\n",
      "    \n",
      "    mean_squared_log_error(y_true, y_pred, sample_weight=None, multioutput='uniform_average')\n",
      "        Mean squared logarithmic error regression loss\n",
      "        \n",
      "        Read more in the :ref:`User Guide <mean_squared_log_error>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y_true : array-like of shape = (n_samples) or (n_samples, n_outputs)\n",
      "            Ground truth (correct) target values.\n",
      "        \n",
      "        y_pred : array-like of shape = (n_samples) or (n_samples, n_outputs)\n",
      "            Estimated target values.\n",
      "        \n",
      "        sample_weight : array-like of shape = (n_samples), optional\n",
      "            Sample weights.\n",
      "        \n",
      "        multioutput : string in ['raw_values', 'uniform_average']             or array-like of shape = (n_outputs)\n",
      "        \n",
      "            Defines aggregating of multiple output values.\n",
      "            Array-like value defines weights used to average errors.\n",
      "        \n",
      "            'raw_values' :\n",
      "                Returns a full set of errors when the input is of multioutput\n",
      "                format.\n",
      "        \n",
      "            'uniform_average' :\n",
      "                Errors of all outputs are averaged with uniform weight.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        loss : float or ndarray of floats\n",
      "            A non-negative floating point value (the best value is 0.0), or an\n",
      "            array of floating point values, one for each individual target.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from sklearn.metrics import mean_squared_log_error\n",
      "        >>> y_true = [3, 5, 2.5, 7]\n",
      "        >>> y_pred = [2.5, 5, 4, 8]\n",
      "        >>> mean_squared_log_error(y_true, y_pred)  # doctest: +ELLIPSIS\n",
      "        0.039...\n",
      "        >>> y_true = [[0.5, 1], [1, 2], [7, 6]]\n",
      "        >>> y_pred = [[0.5, 2], [1, 2.5], [8, 8]]\n",
      "        >>> mean_squared_log_error(y_true, y_pred)  # doctest: +ELLIPSIS\n",
      "        0.044...\n",
      "        >>> mean_squared_log_error(y_true, y_pred, multioutput='raw_values')\n",
      "        ... # doctest: +ELLIPSIS\n",
      "        array([0.00462428, 0.08377444])\n",
      "        >>> mean_squared_log_error(y_true, y_pred, multioutput=[0.3, 0.7])\n",
      "        ... # doctest: +ELLIPSIS\n",
      "        0.060...\n",
      "    \n",
      "    median_absolute_error(y_true, y_pred)\n",
      "        Median absolute error regression loss\n",
      "        \n",
      "        Read more in the :ref:`User Guide <median_absolute_error>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y_true : array-like of shape = (n_samples)\n",
      "            Ground truth (correct) target values.\n",
      "        \n",
      "        y_pred : array-like of shape = (n_samples)\n",
      "            Estimated target values.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        loss : float\n",
      "            A positive floating point value (the best value is 0.0).\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from sklearn.metrics import median_absolute_error\n",
      "        >>> y_true = [3, -0.5, 2, 7]\n",
      "        >>> y_pred = [2.5, 0.0, 2, 8]\n",
      "        >>> median_absolute_error(y_true, y_pred)\n",
      "        0.5\n",
      "    \n",
      "    multilabel_confusion_matrix(y_true, y_pred, sample_weight=None, labels=None, samplewise=False)\n",
      "        Compute a confusion matrix for each class or sample\n",
      "        \n",
      "        .. versionadded:: 0.21\n",
      "        \n",
      "        Compute class-wise (default) or sample-wise (samplewise=True) multilabel\n",
      "        confusion matrix to evaluate the accuracy of a classification, and output\n",
      "        confusion matrices for each class or sample.\n",
      "        \n",
      "        In multilabel confusion matrix :math:`MCM`, the count of true negatives\n",
      "        is :math:`MCM_{:,0,0}`, false negatives is :math:`MCM_{:,1,0}`,\n",
      "        true positives is :math:`MCM_{:,1,1}` and false positives is\n",
      "        :math:`MCM_{:,0,1}`.\n",
      "        \n",
      "        Multiclass data will be treated as if binarized under a one-vs-rest\n",
      "        transformation. Returned confusion matrices will be in the order of\n",
      "        sorted unique labels in the union of (y_true, y_pred).\n",
      "        \n",
      "        Read more in the :ref:`User Guide <multilabel_confusion_matrix>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y_true : 1d array-like, or label indicator array / sparse matrix\n",
      "            of shape (n_samples, n_outputs) or (n_samples,)\n",
      "            Ground truth (correct) target values.\n",
      "        \n",
      "        y_pred : 1d array-like, or label indicator array / sparse matrix\n",
      "            of shape (n_samples, n_outputs) or (n_samples,)\n",
      "            Estimated targets as returned by a classifier\n",
      "        \n",
      "        sample_weight : array-like of shape = (n_samples,), optional\n",
      "            Sample weights\n",
      "        \n",
      "        labels : array-like\n",
      "            A list of classes or column indices to select some (or to force\n",
      "            inclusion of classes absent from the data)\n",
      "        \n",
      "        samplewise : bool, default=False\n",
      "            In the multilabel case, this calculates a confusion matrix per sample\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        multi_confusion : array, shape (n_outputs, 2, 2)\n",
      "            A 2x2 confusion matrix corresponding to each output in the input.\n",
      "            When calculating class-wise multi_confusion (default), then\n",
      "            n_outputs = n_labels; when calculating sample-wise multi_confusion\n",
      "            (samplewise=True), n_outputs = n_samples. If ``labels`` is defined,\n",
      "            the results will be returned in the order specified in ``labels``,\n",
      "            otherwise the results will be returned in sorted order by default.\n",
      "        \n",
      "        See also\n",
      "        --------\n",
      "        confusion_matrix\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        The multilabel_confusion_matrix calculates class-wise or sample-wise\n",
      "        multilabel confusion matrices, and in multiclass tasks, labels are\n",
      "        binarized under a one-vs-rest way; while confusion_matrix calculates\n",
      "        one confusion matrix for confusion between every two classes.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        \n",
      "        Multilabel-indicator case:\n",
      "        \n",
      "        >>> import numpy as np\n",
      "        >>> from sklearn.metrics import multilabel_confusion_matrix\n",
      "        >>> y_true = np.array([[1, 0, 1],\n",
      "        ...                    [0, 1, 0]])\n",
      "        >>> y_pred = np.array([[1, 0, 0],\n",
      "        ...                    [0, 1, 1]])\n",
      "        >>> multilabel_confusion_matrix(y_true, y_pred)\n",
      "        array([[[1, 0],\n",
      "                [0, 1]],\n",
      "        <BLANKLINE>\n",
      "               [[1, 0],\n",
      "                [0, 1]],\n",
      "        <BLANKLINE>\n",
      "               [[0, 1],\n",
      "                [1, 0]]])\n",
      "        \n",
      "        Multiclass case:\n",
      "        \n",
      "        >>> y_true = [\"cat\", \"ant\", \"cat\", \"cat\", \"ant\", \"bird\"]\n",
      "        >>> y_pred = [\"ant\", \"ant\", \"cat\", \"cat\", \"ant\", \"cat\"]\n",
      "        >>> multilabel_confusion_matrix(y_true, y_pred,\n",
      "        ...                             labels=[\"ant\", \"bird\", \"cat\"])\n",
      "        array([[[3, 1],\n",
      "                [0, 2]],\n",
      "        <BLANKLINE>\n",
      "               [[5, 0],\n",
      "                [1, 0]],\n",
      "        <BLANKLINE>\n",
      "               [[2, 1],\n",
      "                [1, 2]]])\n",
      "    \n",
      "    mutual_info_score(labels_true, labels_pred, contingency=None)\n",
      "        Mutual Information between two clusterings.\n",
      "        \n",
      "        The Mutual Information is a measure of the similarity between two labels of\n",
      "        the same data. Where :math:`|U_i|` is the number of the samples\n",
      "        in cluster :math:`U_i` and :math:`|V_j|` is the number of the\n",
      "        samples in cluster :math:`V_j`, the Mutual Information\n",
      "        between clusterings :math:`U` and :math:`V` is given as:\n",
      "        \n",
      "        .. math::\n",
      "        \n",
      "            MI(U,V)=\\sum_{i=1}^{|U|} \\sum_{j=1}^{|V|} \\frac{|U_i\\cap V_j|}{N}\n",
      "            \\log\\frac{N|U_i \\cap V_j|}{|U_i||V_j|}\n",
      "        \n",
      "        This metric is independent of the absolute values of the labels:\n",
      "        a permutation of the class or cluster label values won't change the\n",
      "        score value in any way.\n",
      "        \n",
      "        This metric is furthermore symmetric: switching ``label_true`` with\n",
      "        ``label_pred`` will return the same score value. This can be useful to\n",
      "        measure the agreement of two independent label assignments strategies\n",
      "        on the same dataset when the real ground truth is not known.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <mutual_info_score>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        labels_true : int array, shape = [n_samples]\n",
      "            A clustering of the data into disjoint subsets.\n",
      "        \n",
      "        labels_pred : array, shape = [n_samples]\n",
      "            A clustering of the data into disjoint subsets.\n",
      "        \n",
      "        contingency : {None, array, sparse matrix},                   shape = [n_classes_true, n_classes_pred]\n",
      "            A contingency matrix given by the :func:`contingency_matrix` function.\n",
      "            If value is ``None``, it will be computed, otherwise the given value is\n",
      "            used, with ``labels_true`` and ``labels_pred`` ignored.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        mi : float\n",
      "           Mutual information, a non-negative value\n",
      "        \n",
      "        See also\n",
      "        --------\n",
      "        adjusted_mutual_info_score: Adjusted against chance Mutual Information\n",
      "        normalized_mutual_info_score: Normalized Mutual Information\n",
      "    \n",
      "    normalized_mutual_info_score(labels_true, labels_pred, average_method='warn')\n",
      "        Normalized Mutual Information between two clusterings.\n",
      "        \n",
      "        Normalized Mutual Information (NMI) is a normalization of the Mutual\n",
      "        Information (MI) score to scale the results between 0 (no mutual\n",
      "        information) and 1 (perfect correlation). In this function, mutual\n",
      "        information is normalized by some generalized mean of ``H(labels_true)``\n",
      "        and ``H(labels_pred))``, defined by the `average_method`.\n",
      "        \n",
      "        This measure is not adjusted for chance. Therefore\n",
      "        :func:`adjusted_mutual_info_score` might be preferred.\n",
      "        \n",
      "        This metric is independent of the absolute values of the labels:\n",
      "        a permutation of the class or cluster label values won't change the\n",
      "        score value in any way.\n",
      "        \n",
      "        This metric is furthermore symmetric: switching ``label_true`` with\n",
      "        ``label_pred`` will return the same score value. This can be useful to\n",
      "        measure the agreement of two independent label assignments strategies\n",
      "        on the same dataset when the real ground truth is not known.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <mutual_info_score>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        labels_true : int array, shape = [n_samples]\n",
      "            A clustering of the data into disjoint subsets.\n",
      "        \n",
      "        labels_pred : array, shape = [n_samples]\n",
      "            A clustering of the data into disjoint subsets.\n",
      "        \n",
      "        average_method : string, optional (default: 'warn')\n",
      "            How to compute the normalizer in the denominator. Possible options\n",
      "            are 'min', 'geometric', 'arithmetic', and 'max'.\n",
      "            If 'warn', 'geometric' will be used. The default will change to\n",
      "            'arithmetic' in version 0.22.\n",
      "        \n",
      "            .. versionadded:: 0.20\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        nmi : float\n",
      "           score between 0.0 and 1.0. 1.0 stands for perfectly complete labeling\n",
      "        \n",
      "        See also\n",
      "        --------\n",
      "        v_measure_score: V-Measure (NMI with arithmetic mean option.)\n",
      "        adjusted_rand_score: Adjusted Rand Index\n",
      "        adjusted_mutual_info_score: Adjusted Mutual Information (adjusted\n",
      "            against chance)\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        \n",
      "        Perfect labelings are both homogeneous and complete, hence have\n",
      "        score 1.0::\n",
      "        \n",
      "          >>> from sklearn.metrics.cluster import normalized_mutual_info_score\n",
      "          >>> normalized_mutual_info_score([0, 0, 1, 1], [0, 0, 1, 1])\n",
      "          ... # doctest: +SKIP\n",
      "          1.0\n",
      "          >>> normalized_mutual_info_score([0, 0, 1, 1], [1, 1, 0, 0])\n",
      "          ... # doctest: +SKIP\n",
      "          1.0\n",
      "        \n",
      "        If classes members are completely split across different clusters,\n",
      "        the assignment is totally in-complete, hence the NMI is null::\n",
      "        \n",
      "          >>> normalized_mutual_info_score([0, 0, 0, 0], [0, 1, 2, 3])\n",
      "          ... # doctest: +SKIP\n",
      "          0.0\n",
      "    \n",
      "    pairwise_distances(X, Y=None, metric='euclidean', n_jobs=None, **kwds)\n",
      "        Compute the distance matrix from a vector array X and optional Y.\n",
      "        \n",
      "        This method takes either a vector array or a distance matrix, and returns\n",
      "        a distance matrix. If the input is a vector array, the distances are\n",
      "        computed. If the input is a distances matrix, it is returned instead.\n",
      "        \n",
      "        This method provides a safe way to take a distance matrix as input, while\n",
      "        preserving compatibility with many other algorithms that take a vector\n",
      "        array.\n",
      "        \n",
      "        If Y is given (default is None), then the returned matrix is the pairwise\n",
      "        distance between the arrays from both X and Y.\n",
      "        \n",
      "        Valid values for metric are:\n",
      "        \n",
      "        - From scikit-learn: ['cityblock', 'cosine', 'euclidean', 'l1', 'l2',\n",
      "          'manhattan']. These metrics support sparse matrix inputs.\n",
      "        \n",
      "        - From scipy.spatial.distance: ['braycurtis', 'canberra', 'chebyshev',\n",
      "          'correlation', 'dice', 'hamming', 'jaccard', 'kulsinski', 'mahalanobis',\n",
      "          'minkowski', 'rogerstanimoto', 'russellrao', 'seuclidean',\n",
      "          'sokalmichener', 'sokalsneath', 'sqeuclidean', 'yule']\n",
      "          See the documentation for scipy.spatial.distance for details on these\n",
      "          metrics. These metrics do not support sparse matrix inputs.\n",
      "        \n",
      "        Note that in the case of 'cityblock', 'cosine' and 'euclidean' (which are\n",
      "        valid scipy.spatial.distance metrics), the scikit-learn implementation\n",
      "        will be used, which is faster and has support for sparse matrices (except\n",
      "        for 'cityblock'). For a verbose description of the metrics from\n",
      "        scikit-learn, see the __doc__ of the sklearn.pairwise.distance_metrics\n",
      "        function.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <metrics>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        X : array [n_samples_a, n_samples_a] if metric == \"precomputed\", or,              [n_samples_a, n_features] otherwise\n",
      "            Array of pairwise distances between samples, or a feature array.\n",
      "        \n",
      "        Y : array [n_samples_b, n_features], optional\n",
      "            An optional second feature array. Only allowed if\n",
      "            metric != \"precomputed\".\n",
      "        \n",
      "        metric : string, or callable\n",
      "            The metric to use when calculating distance between instances in a\n",
      "            feature array. If metric is a string, it must be one of the options\n",
      "            allowed by scipy.spatial.distance.pdist for its metric parameter, or\n",
      "            a metric listed in pairwise.PAIRWISE_DISTANCE_FUNCTIONS.\n",
      "            If metric is \"precomputed\", X is assumed to be a distance matrix.\n",
      "            Alternatively, if metric is a callable function, it is called on each\n",
      "            pair of instances (rows) and the resulting value recorded. The callable\n",
      "            should take two arrays from X as input and return a value indicating\n",
      "            the distance between them.\n",
      "        \n",
      "        n_jobs : int or None, optional (default=None)\n",
      "            The number of jobs to use for the computation. This works by breaking\n",
      "            down the pairwise matrix into n_jobs even slices and computing them in\n",
      "            parallel.\n",
      "        \n",
      "            ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n",
      "            ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n",
      "            for more details.\n",
      "        \n",
      "        **kwds : optional keyword parameters\n",
      "            Any further parameters are passed directly to the distance function.\n",
      "            If using a scipy.spatial.distance metric, the parameters are still\n",
      "            metric dependent. See the scipy docs for usage examples.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        D : array [n_samples_a, n_samples_a] or [n_samples_a, n_samples_b]\n",
      "            A distance matrix D such that D_{i, j} is the distance between the\n",
      "            ith and jth vectors of the given matrix X, if Y is None.\n",
      "            If Y is not None, then D_{i, j} is the distance between the ith array\n",
      "            from X and the jth array from Y.\n",
      "        \n",
      "        See also\n",
      "        --------\n",
      "        pairwise_distances_chunked : performs the same calculation as this\n",
      "            function, but returns a generator of chunks of the distance matrix, in\n",
      "            order to limit memory usage.\n",
      "        paired_distances : Computes the distances between corresponding\n",
      "                           elements of two arrays\n",
      "    \n",
      "    pairwise_distances_argmin(X, Y, axis=1, metric='euclidean', batch_size=None, metric_kwargs=None)\n",
      "        Compute minimum distances between one point and a set of points.\n",
      "        \n",
      "        This function computes for each row in X, the index of the row of Y which\n",
      "        is closest (according to the specified distance).\n",
      "        \n",
      "        This is mostly equivalent to calling:\n",
      "        \n",
      "            pairwise_distances(X, Y=Y, metric=metric).argmin(axis=axis)\n",
      "        \n",
      "        but uses much less memory, and is faster for large arrays.\n",
      "        \n",
      "        This function works with dense 2D arrays only.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        X : array-like\n",
      "            Arrays containing points. Respective shapes (n_samples1, n_features)\n",
      "            and (n_samples2, n_features)\n",
      "        \n",
      "        Y : array-like\n",
      "            Arrays containing points. Respective shapes (n_samples1, n_features)\n",
      "            and (n_samples2, n_features)\n",
      "        \n",
      "        axis : int, optional, default 1\n",
      "            Axis along which the argmin and distances are to be computed.\n",
      "        \n",
      "        metric : string or callable\n",
      "            metric to use for distance computation. Any metric from scikit-learn\n",
      "            or scipy.spatial.distance can be used.\n",
      "        \n",
      "            If metric is a callable function, it is called on each\n",
      "            pair of instances (rows) and the resulting value recorded. The callable\n",
      "            should take two arrays as input and return one value indicating the\n",
      "            distance between them. This works for Scipy's metrics, but is less\n",
      "            efficient than passing the metric name as a string.\n",
      "        \n",
      "            Distance matrices are not supported.\n",
      "        \n",
      "            Valid values for metric are:\n",
      "        \n",
      "            - from scikit-learn: ['cityblock', 'cosine', 'euclidean', 'l1', 'l2',\n",
      "              'manhattan']\n",
      "        \n",
      "            - from scipy.spatial.distance: ['braycurtis', 'canberra', 'chebyshev',\n",
      "              'correlation', 'dice', 'hamming', 'jaccard', 'kulsinski',\n",
      "              'mahalanobis', 'minkowski', 'rogerstanimoto', 'russellrao',\n",
      "              'seuclidean', 'sokalmichener', 'sokalsneath', 'sqeuclidean',\n",
      "              'yule']\n",
      "        \n",
      "            See the documentation for scipy.spatial.distance for details on these\n",
      "            metrics.\n",
      "        \n",
      "        batch_size : integer\n",
      "            .. deprecated:: 0.20\n",
      "                Deprecated for removal in 0.22.\n",
      "                Use sklearn.set_config(working_memory=...) instead.\n",
      "        \n",
      "        metric_kwargs : dict\n",
      "            keyword arguments to pass to specified metric function.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        argmin : numpy.ndarray\n",
      "            Y[argmin[i], :] is the row in Y that is closest to X[i, :].\n",
      "        \n",
      "        See also\n",
      "        --------\n",
      "        sklearn.metrics.pairwise_distances\n",
      "        sklearn.metrics.pairwise_distances_argmin_min\n",
      "    \n",
      "    pairwise_distances_argmin_min(X, Y, axis=1, metric='euclidean', batch_size=None, metric_kwargs=None)\n",
      "        Compute minimum distances between one point and a set of points.\n",
      "        \n",
      "        This function computes for each row in X, the index of the row of Y which\n",
      "        is closest (according to the specified distance). The minimal distances are\n",
      "        also returned.\n",
      "        \n",
      "        This is mostly equivalent to calling:\n",
      "        \n",
      "            (pairwise_distances(X, Y=Y, metric=metric).argmin(axis=axis),\n",
      "             pairwise_distances(X, Y=Y, metric=metric).min(axis=axis))\n",
      "        \n",
      "        but uses much less memory, and is faster for large arrays.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        X : {array-like, sparse matrix}, shape (n_samples1, n_features)\n",
      "            Array containing points.\n",
      "        \n",
      "        Y : {array-like, sparse matrix}, shape (n_samples2, n_features)\n",
      "            Arrays containing points.\n",
      "        \n",
      "        axis : int, optional, default 1\n",
      "            Axis along which the argmin and distances are to be computed.\n",
      "        \n",
      "        metric : string or callable, default 'euclidean'\n",
      "            metric to use for distance computation. Any metric from scikit-learn\n",
      "            or scipy.spatial.distance can be used.\n",
      "        \n",
      "            If metric is a callable function, it is called on each\n",
      "            pair of instances (rows) and the resulting value recorded. The callable\n",
      "            should take two arrays as input and return one value indicating the\n",
      "            distance between them. This works for Scipy's metrics, but is less\n",
      "            efficient than passing the metric name as a string.\n",
      "        \n",
      "            Distance matrices are not supported.\n",
      "        \n",
      "            Valid values for metric are:\n",
      "        \n",
      "            - from scikit-learn: ['cityblock', 'cosine', 'euclidean', 'l1', 'l2',\n",
      "              'manhattan']\n",
      "        \n",
      "            - from scipy.spatial.distance: ['braycurtis', 'canberra', 'chebyshev',\n",
      "              'correlation', 'dice', 'hamming', 'jaccard', 'kulsinski',\n",
      "              'mahalanobis', 'minkowski', 'rogerstanimoto', 'russellrao',\n",
      "              'seuclidean', 'sokalmichener', 'sokalsneath', 'sqeuclidean',\n",
      "              'yule']\n",
      "        \n",
      "            See the documentation for scipy.spatial.distance for details on these\n",
      "            metrics.\n",
      "        \n",
      "        batch_size : integer\n",
      "            .. deprecated:: 0.20\n",
      "                Deprecated for removal in 0.22.\n",
      "                Use sklearn.set_config(working_memory=...) instead.\n",
      "        \n",
      "        metric_kwargs : dict, optional\n",
      "            Keyword arguments to pass to specified metric function.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        argmin : numpy.ndarray\n",
      "            Y[argmin[i], :] is the row in Y that is closest to X[i, :].\n",
      "        \n",
      "        distances : numpy.ndarray\n",
      "            distances[i] is the distance between the i-th row in X and the\n",
      "            argmin[i]-th row in Y.\n",
      "        \n",
      "        See also\n",
      "        --------\n",
      "        sklearn.metrics.pairwise_distances\n",
      "        sklearn.metrics.pairwise_distances_argmin\n",
      "    \n",
      "    pairwise_distances_chunked(X, Y=None, reduce_func=None, metric='euclidean', n_jobs=None, working_memory=None, **kwds)\n",
      "        Generate a distance matrix chunk by chunk with optional reduction\n",
      "        \n",
      "        In cases where not all of a pairwise distance matrix needs to be stored at\n",
      "        once, this is used to calculate pairwise distances in\n",
      "        ``working_memory``-sized chunks.  If ``reduce_func`` is given, it is run\n",
      "        on each chunk and its return values are concatenated into lists, arrays\n",
      "        or sparse matrices.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        X : array [n_samples_a, n_samples_a] if metric == \"precomputed\", or,\n",
      "            [n_samples_a, n_features] otherwise\n",
      "            Array of pairwise distances between samples, or a feature array.\n",
      "        \n",
      "        Y : array [n_samples_b, n_features], optional\n",
      "            An optional second feature array. Only allowed if\n",
      "            metric != \"precomputed\".\n",
      "        \n",
      "        reduce_func : callable, optional\n",
      "            The function which is applied on each chunk of the distance matrix,\n",
      "            reducing it to needed values.  ``reduce_func(D_chunk, start)``\n",
      "            is called repeatedly, where ``D_chunk`` is a contiguous vertical\n",
      "            slice of the pairwise distance matrix, starting at row ``start``.\n",
      "            It should return an array, a list, or a sparse matrix of length\n",
      "            ``D_chunk.shape[0]``, or a tuple of such objects.\n",
      "        \n",
      "            If None, pairwise_distances_chunked returns a generator of vertical\n",
      "            chunks of the distance matrix.\n",
      "        \n",
      "        metric : string, or callable\n",
      "            The metric to use when calculating distance between instances in a\n",
      "            feature array. If metric is a string, it must be one of the options\n",
      "            allowed by scipy.spatial.distance.pdist for its metric parameter, or\n",
      "            a metric listed in pairwise.PAIRWISE_DISTANCE_FUNCTIONS.\n",
      "            If metric is \"precomputed\", X is assumed to be a distance matrix.\n",
      "            Alternatively, if metric is a callable function, it is called on each\n",
      "            pair of instances (rows) and the resulting value recorded. The callable\n",
      "            should take two arrays from X as input and return a value indicating\n",
      "            the distance between them.\n",
      "        \n",
      "        n_jobs : int or None, optional (default=None)\n",
      "            The number of jobs to use for the computation. This works by breaking\n",
      "            down the pairwise matrix into n_jobs even slices and computing them in\n",
      "            parallel.\n",
      "        \n",
      "            ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n",
      "            ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n",
      "            for more details.\n",
      "        \n",
      "        working_memory : int, optional\n",
      "            The sought maximum memory for temporary distance matrix chunks.\n",
      "            When None (default), the value of\n",
      "            ``sklearn.get_config()['working_memory']`` is used.\n",
      "        \n",
      "        `**kwds` : optional keyword parameters\n",
      "            Any further parameters are passed directly to the distance function.\n",
      "            If using a scipy.spatial.distance metric, the parameters are still\n",
      "            metric dependent. See the scipy docs for usage examples.\n",
      "        \n",
      "        Yields\n",
      "        ------\n",
      "        D_chunk : array or sparse matrix\n",
      "            A contiguous slice of distance matrix, optionally processed by\n",
      "            ``reduce_func``.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        Without reduce_func:\n",
      "        \n",
      "        >>> import numpy as np\n",
      "        >>> from sklearn.metrics import pairwise_distances_chunked\n",
      "        >>> X = np.random.RandomState(0).rand(5, 3)\n",
      "        >>> D_chunk = next(pairwise_distances_chunked(X))\n",
      "        >>> D_chunk  # doctest: +ELLIPSIS\n",
      "        array([[0.  ..., 0.29..., 0.41..., 0.19..., 0.57...],\n",
      "               [0.29..., 0.  ..., 0.57..., 0.41..., 0.76...],\n",
      "               [0.41..., 0.57..., 0.  ..., 0.44..., 0.90...],\n",
      "               [0.19..., 0.41..., 0.44..., 0.  ..., 0.51...],\n",
      "               [0.57..., 0.76..., 0.90..., 0.51..., 0.  ...]])\n",
      "        \n",
      "        Retrieve all neighbors and average distance within radius r:\n",
      "        \n",
      "        >>> r = .2\n",
      "        >>> def reduce_func(D_chunk, start):\n",
      "        ...     neigh = [np.flatnonzero(d < r) for d in D_chunk]\n",
      "        ...     avg_dist = (D_chunk * (D_chunk < r)).mean(axis=1)\n",
      "        ...     return neigh, avg_dist\n",
      "        >>> gen = pairwise_distances_chunked(X, reduce_func=reduce_func)\n",
      "        >>> neigh, avg_dist = next(gen)\n",
      "        >>> neigh\n",
      "        [array([0, 3]), array([1]), array([2]), array([0, 3]), array([4])]\n",
      "        >>> avg_dist  # doctest: +ELLIPSIS\n",
      "        array([0.039..., 0.        , 0.        , 0.039..., 0.        ])\n",
      "        \n",
      "        Where r is defined per sample, we need to make use of ``start``:\n",
      "        \n",
      "        >>> r = [.2, .4, .4, .3, .1]\n",
      "        >>> def reduce_func(D_chunk, start):\n",
      "        ...     neigh = [np.flatnonzero(d < r[i])\n",
      "        ...              for i, d in enumerate(D_chunk, start)]\n",
      "        ...     return neigh\n",
      "        >>> neigh = next(pairwise_distances_chunked(X, reduce_func=reduce_func))\n",
      "        >>> neigh\n",
      "        [array([0, 3]), array([0, 1]), array([2]), array([0, 3]), array([4])]\n",
      "        \n",
      "        Force row-by-row generation by reducing ``working_memory``:\n",
      "        \n",
      "        >>> gen = pairwise_distances_chunked(X, reduce_func=reduce_func,\n",
      "        ...                                  working_memory=0)\n",
      "        >>> next(gen)\n",
      "        [array([0, 3])]\n",
      "        >>> next(gen)\n",
      "        [array([0, 1])]\n",
      "    \n",
      "    pairwise_kernels(X, Y=None, metric='linear', filter_params=False, n_jobs=None, **kwds)\n",
      "        Compute the kernel between arrays X and optional array Y.\n",
      "        \n",
      "        This method takes either a vector array or a kernel matrix, and returns\n",
      "        a kernel matrix. If the input is a vector array, the kernels are\n",
      "        computed. If the input is a kernel matrix, it is returned instead.\n",
      "        \n",
      "        This method provides a safe way to take a kernel matrix as input, while\n",
      "        preserving compatibility with many other algorithms that take a vector\n",
      "        array.\n",
      "        \n",
      "        If Y is given (default is None), then the returned matrix is the pairwise\n",
      "        kernel between the arrays from both X and Y.\n",
      "        \n",
      "        Valid values for metric are::\n",
      "            ['additive_chi2', 'chi2', 'linear', 'poly', 'polynomial', 'rbf',\n",
      "             'laplacian', 'sigmoid', 'cosine']\n",
      "        \n",
      "        Read more in the :ref:`User Guide <metrics>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        X : array [n_samples_a, n_samples_a] if metric == \"precomputed\", or,              [n_samples_a, n_features] otherwise\n",
      "            Array of pairwise kernels between samples, or a feature array.\n",
      "        \n",
      "        Y : array [n_samples_b, n_features]\n",
      "            A second feature array only if X has shape [n_samples_a, n_features].\n",
      "        \n",
      "        metric : string, or callable\n",
      "            The metric to use when calculating kernel between instances in a\n",
      "            feature array. If metric is a string, it must be one of the metrics\n",
      "            in pairwise.PAIRWISE_KERNEL_FUNCTIONS.\n",
      "            If metric is \"precomputed\", X is assumed to be a kernel matrix.\n",
      "            Alternatively, if metric is a callable function, it is called on each\n",
      "            pair of instances (rows) and the resulting value recorded. The callable\n",
      "            should take two arrays from X as input and return a value indicating\n",
      "            the distance between them.\n",
      "        \n",
      "        filter_params : boolean\n",
      "            Whether to filter invalid parameters or not.\n",
      "        \n",
      "        n_jobs : int or None, optional (default=None)\n",
      "            The number of jobs to use for the computation. This works by breaking\n",
      "            down the pairwise matrix into n_jobs even slices and computing them in\n",
      "            parallel.\n",
      "        \n",
      "            ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n",
      "            ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n",
      "            for more details.\n",
      "        \n",
      "        **kwds : optional keyword parameters\n",
      "            Any further parameters are passed directly to the kernel function.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        K : array [n_samples_a, n_samples_a] or [n_samples_a, n_samples_b]\n",
      "            A kernel matrix K such that K_{i, j} is the kernel between the\n",
      "            ith and jth vectors of the given matrix X, if Y is None.\n",
      "            If Y is not None, then K_{i, j} is the kernel between the ith array\n",
      "            from X and the jth array from Y.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        If metric is 'precomputed', Y is ignored and X is returned.\n",
      "    \n",
      "    precision_recall_curve(y_true, probas_pred, pos_label=None, sample_weight=None)\n",
      "        Compute precision-recall pairs for different probability thresholds\n",
      "        \n",
      "        Note: this implementation is restricted to the binary classification task.\n",
      "        \n",
      "        The precision is the ratio ``tp / (tp + fp)`` where ``tp`` is the number of\n",
      "        true positives and ``fp`` the number of false positives. The precision is\n",
      "        intuitively the ability of the classifier not to label as positive a sample\n",
      "        that is negative.\n",
      "        \n",
      "        The recall is the ratio ``tp / (tp + fn)`` where ``tp`` is the number of\n",
      "        true positives and ``fn`` the number of false negatives. The recall is\n",
      "        intuitively the ability of the classifier to find all the positive samples.\n",
      "        \n",
      "        The last precision and recall values are 1. and 0. respectively and do not\n",
      "        have a corresponding threshold.  This ensures that the graph starts on the\n",
      "        y axis.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <precision_recall_f_measure_metrics>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y_true : array, shape = [n_samples]\n",
      "            True binary labels. If labels are not either {-1, 1} or {0, 1}, then\n",
      "            pos_label should be explicitly given.\n",
      "        \n",
      "        probas_pred : array, shape = [n_samples]\n",
      "            Estimated probabilities or decision function.\n",
      "        \n",
      "        pos_label : int or str, default=None\n",
      "            The label of the positive class.\n",
      "            When ``pos_label=None``, if y_true is in {-1, 1} or {0, 1},\n",
      "            ``pos_label`` is set to 1, otherwise an error will be raised.\n",
      "        \n",
      "        sample_weight : array-like of shape = [n_samples], optional\n",
      "            Sample weights.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        precision : array, shape = [n_thresholds + 1]\n",
      "            Precision values such that element i is the precision of\n",
      "            predictions with score >= thresholds[i] and the last element is 1.\n",
      "        \n",
      "        recall : array, shape = [n_thresholds + 1]\n",
      "            Decreasing recall values such that element i is the recall of\n",
      "            predictions with score >= thresholds[i] and the last element is 0.\n",
      "        \n",
      "        thresholds : array, shape = [n_thresholds <= len(np.unique(probas_pred))]\n",
      "            Increasing thresholds on the decision function used to compute\n",
      "            precision and recall.\n",
      "        \n",
      "        See also\n",
      "        --------\n",
      "        average_precision_score : Compute average precision from prediction scores\n",
      "        \n",
      "        roc_curve : Compute Receiver operating characteristic (ROC) curve\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> import numpy as np\n",
      "        >>> from sklearn.metrics import precision_recall_curve\n",
      "        >>> y_true = np.array([0, 0, 1, 1])\n",
      "        >>> y_scores = np.array([0.1, 0.4, 0.35, 0.8])\n",
      "        >>> precision, recall, thresholds = precision_recall_curve(\n",
      "        ...     y_true, y_scores)\n",
      "        >>> precision  # doctest: +ELLIPSIS\n",
      "        array([0.66666667, 0.5       , 1.        , 1.        ])\n",
      "        >>> recall\n",
      "        array([1. , 0.5, 0.5, 0. ])\n",
      "        >>> thresholds\n",
      "        array([0.35, 0.4 , 0.8 ])\n",
      "    \n",
      "    precision_recall_fscore_support(y_true, y_pred, beta=1.0, labels=None, pos_label=1, average=None, warn_for=('precision', 'recall', 'f-score'), sample_weight=None)\n",
      "        Compute precision, recall, F-measure and support for each class\n",
      "        \n",
      "        The precision is the ratio ``tp / (tp + fp)`` where ``tp`` is the number of\n",
      "        true positives and ``fp`` the number of false positives. The precision is\n",
      "        intuitively the ability of the classifier not to label as positive a sample\n",
      "        that is negative.\n",
      "        \n",
      "        The recall is the ratio ``tp / (tp + fn)`` where ``tp`` is the number of\n",
      "        true positives and ``fn`` the number of false negatives. The recall is\n",
      "        intuitively the ability of the classifier to find all the positive samples.\n",
      "        \n",
      "        The F-beta score can be interpreted as a weighted harmonic mean of\n",
      "        the precision and recall, where an F-beta score reaches its best\n",
      "        value at 1 and worst score at 0.\n",
      "        \n",
      "        The F-beta score weights recall more than precision by a factor of\n",
      "        ``beta``. ``beta == 1.0`` means recall and precision are equally important.\n",
      "        \n",
      "        The support is the number of occurrences of each class in ``y_true``.\n",
      "        \n",
      "        If ``pos_label is None`` and in binary classification, this function\n",
      "        returns the average precision, recall and F-measure if ``average``\n",
      "        is one of ``'micro'``, ``'macro'``, ``'weighted'`` or ``'samples'``.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <precision_recall_f_measure_metrics>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y_true : 1d array-like, or label indicator array / sparse matrix\n",
      "            Ground truth (correct) target values.\n",
      "        \n",
      "        y_pred : 1d array-like, or label indicator array / sparse matrix\n",
      "            Estimated targets as returned by a classifier.\n",
      "        \n",
      "        beta : float, 1.0 by default\n",
      "            The strength of recall versus precision in the F-score.\n",
      "        \n",
      "        labels : list, optional\n",
      "            The set of labels to include when ``average != 'binary'``, and their\n",
      "            order if ``average is None``. Labels present in the data can be\n",
      "            excluded, for example to calculate a multiclass average ignoring a\n",
      "            majority negative class, while labels not present in the data will\n",
      "            result in 0 components in a macro average. For multilabel targets,\n",
      "            labels are column indices. By default, all labels in ``y_true`` and\n",
      "            ``y_pred`` are used in sorted order.\n",
      "        \n",
      "        pos_label : str or int, 1 by default\n",
      "            The class to report if ``average='binary'`` and the data is binary.\n",
      "            If the data are multiclass or multilabel, this will be ignored;\n",
      "            setting ``labels=[pos_label]`` and ``average != 'binary'`` will report\n",
      "            scores for that label only.\n",
      "        \n",
      "        average : string, [None (default), 'binary', 'micro', 'macro', 'samples',                        'weighted']\n",
      "            If ``None``, the scores for each class are returned. Otherwise, this\n",
      "            determines the type of averaging performed on the data:\n",
      "        \n",
      "            ``'binary'``:\n",
      "                Only report results for the class specified by ``pos_label``.\n",
      "                This is applicable only if targets (``y_{true,pred}``) are binary.\n",
      "            ``'micro'``:\n",
      "                Calculate metrics globally by counting the total true positives,\n",
      "                false negatives and false positives.\n",
      "            ``'macro'``:\n",
      "                Calculate metrics for each label, and find their unweighted\n",
      "                mean.  This does not take label imbalance into account.\n",
      "            ``'weighted'``:\n",
      "                Calculate metrics for each label, and find their average weighted\n",
      "                by support (the number of true instances for each label). This\n",
      "                alters 'macro' to account for label imbalance; it can result in an\n",
      "                F-score that is not between precision and recall.\n",
      "            ``'samples'``:\n",
      "                Calculate metrics for each instance, and find their average (only\n",
      "                meaningful for multilabel classification where this differs from\n",
      "                :func:`accuracy_score`).\n",
      "        \n",
      "        warn_for : tuple or set, for internal use\n",
      "            This determines which warnings will be made in the case that this\n",
      "            function is being used to return only one of its metrics.\n",
      "        \n",
      "        sample_weight : array-like of shape = [n_samples], optional\n",
      "            Sample weights.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        precision : float (if average is not None) or array of float, shape =        [n_unique_labels]\n",
      "        \n",
      "        recall : float (if average is not None) or array of float, , shape =        [n_unique_labels]\n",
      "        \n",
      "        fbeta_score : float (if average is not None) or array of float, shape =        [n_unique_labels]\n",
      "        \n",
      "        support : int (if average is not None) or array of int, shape =        [n_unique_labels]\n",
      "            The number of occurrences of each label in ``y_true``.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] `Wikipedia entry for the Precision and recall\n",
      "               <https://en.wikipedia.org/wiki/Precision_and_recall>`_\n",
      "        \n",
      "        .. [2] `Wikipedia entry for the F1-score\n",
      "               <https://en.wikipedia.org/wiki/F1_score>`_\n",
      "        \n",
      "        .. [3] `Discriminative Methods for Multi-labeled Classification Advances\n",
      "               in Knowledge Discovery and Data Mining (2004), pp. 22-30 by Shantanu\n",
      "               Godbole, Sunita Sarawagi\n",
      "               <http://www.godbole.net/shantanu/pubs/multilabelsvm-pakdd04.pdf>`_\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> import numpy as np\n",
      "        >>> from sklearn.metrics import precision_recall_fscore_support\n",
      "        >>> y_true = np.array(['cat', 'dog', 'pig', 'cat', 'dog', 'pig'])\n",
      "        >>> y_pred = np.array(['cat', 'pig', 'dog', 'cat', 'cat', 'dog'])\n",
      "        >>> precision_recall_fscore_support(y_true, y_pred, average='macro')\n",
      "        ... # doctest: +ELLIPSIS\n",
      "        (0.22..., 0.33..., 0.26..., None)\n",
      "        >>> precision_recall_fscore_support(y_true, y_pred, average='micro')\n",
      "        ... # doctest: +ELLIPSIS\n",
      "        (0.33..., 0.33..., 0.33..., None)\n",
      "        >>> precision_recall_fscore_support(y_true, y_pred, average='weighted')\n",
      "        ... # doctest: +ELLIPSIS\n",
      "        (0.22..., 0.33..., 0.26..., None)\n",
      "        \n",
      "        It is possible to compute per-label precisions, recalls, F1-scores and\n",
      "        supports instead of averaging:\n",
      "        \n",
      "        >>> precision_recall_fscore_support(y_true, y_pred, average=None,\n",
      "        ... labels=['pig', 'dog', 'cat'])\n",
      "        ... # doctest: +ELLIPSIS,+NORMALIZE_WHITESPACE\n",
      "        (array([0.        , 0.        , 0.66...]),\n",
      "         array([0., 0., 1.]), array([0. , 0. , 0.8]),\n",
      "         array([2, 2, 2]))\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        When ``true positive + false positive == 0``, precision is undefined;\n",
      "        When ``true positive + false negative == 0``, recall is undefined.\n",
      "        In such cases, the metric will be set to 0, as will f-score, and\n",
      "        ``UndefinedMetricWarning`` will be raised.\n",
      "    \n",
      "    precision_score(y_true, y_pred, labels=None, pos_label=1, average='binary', sample_weight=None)\n",
      "        Compute the precision\n",
      "        \n",
      "        The precision is the ratio ``tp / (tp + fp)`` where ``tp`` is the number of\n",
      "        true positives and ``fp`` the number of false positives. The precision is\n",
      "        intuitively the ability of the classifier not to label as positive a sample\n",
      "        that is negative.\n",
      "        \n",
      "        The best value is 1 and the worst value is 0.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <precision_recall_f_measure_metrics>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y_true : 1d array-like, or label indicator array / sparse matrix\n",
      "            Ground truth (correct) target values.\n",
      "        \n",
      "        y_pred : 1d array-like, or label indicator array / sparse matrix\n",
      "            Estimated targets as returned by a classifier.\n",
      "        \n",
      "        labels : list, optional\n",
      "            The set of labels to include when ``average != 'binary'``, and their\n",
      "            order if ``average is None``. Labels present in the data can be\n",
      "            excluded, for example to calculate a multiclass average ignoring a\n",
      "            majority negative class, while labels not present in the data will\n",
      "            result in 0 components in a macro average. For multilabel targets,\n",
      "            labels are column indices. By default, all labels in ``y_true`` and\n",
      "            ``y_pred`` are used in sorted order.\n",
      "        \n",
      "            .. versionchanged:: 0.17\n",
      "               parameter *labels* improved for multiclass problem.\n",
      "        \n",
      "        pos_label : str or int, 1 by default\n",
      "            The class to report if ``average='binary'`` and the data is binary.\n",
      "            If the data are multiclass or multilabel, this will be ignored;\n",
      "            setting ``labels=[pos_label]`` and ``average != 'binary'`` will report\n",
      "            scores for that label only.\n",
      "        \n",
      "        average : string, [None, 'binary' (default), 'micro', 'macro', 'samples',                        'weighted']\n",
      "            This parameter is required for multiclass/multilabel targets.\n",
      "            If ``None``, the scores for each class are returned. Otherwise, this\n",
      "            determines the type of averaging performed on the data:\n",
      "        \n",
      "            ``'binary'``:\n",
      "                Only report results for the class specified by ``pos_label``.\n",
      "                This is applicable only if targets (``y_{true,pred}``) are binary.\n",
      "            ``'micro'``:\n",
      "                Calculate metrics globally by counting the total true positives,\n",
      "                false negatives and false positives.\n",
      "            ``'macro'``:\n",
      "                Calculate metrics for each label, and find their unweighted\n",
      "                mean.  This does not take label imbalance into account.\n",
      "            ``'weighted'``:\n",
      "                Calculate metrics for each label, and find their average weighted\n",
      "                by support (the number of true instances for each label). This\n",
      "                alters 'macro' to account for label imbalance; it can result in an\n",
      "                F-score that is not between precision and recall.\n",
      "            ``'samples'``:\n",
      "                Calculate metrics for each instance, and find their average (only\n",
      "                meaningful for multilabel classification where this differs from\n",
      "                :func:`accuracy_score`).\n",
      "        \n",
      "        sample_weight : array-like of shape = [n_samples], optional\n",
      "            Sample weights.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        precision : float (if average is not None) or array of float, shape =        [n_unique_labels]\n",
      "            Precision of the positive class in binary classification or weighted\n",
      "            average of the precision of each class for the multiclass task.\n",
      "        \n",
      "        See also\n",
      "        --------\n",
      "        precision_recall_fscore_support, multilabel_confusion_matrix\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from sklearn.metrics import precision_score\n",
      "        >>> y_true = [0, 1, 2, 0, 1, 2]\n",
      "        >>> y_pred = [0, 2, 1, 0, 0, 1]\n",
      "        >>> precision_score(y_true, y_pred, average='macro')  # doctest: +ELLIPSIS\n",
      "        0.22...\n",
      "        >>> precision_score(y_true, y_pred, average='micro')  # doctest: +ELLIPSIS\n",
      "        0.33...\n",
      "        >>> precision_score(y_true, y_pred, average='weighted')\n",
      "        ... # doctest: +ELLIPSIS\n",
      "        0.22...\n",
      "        >>> precision_score(y_true, y_pred, average=None)  # doctest: +ELLIPSIS\n",
      "        array([0.66..., 0.        , 0.        ])\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        When ``true positive + false positive == 0``, precision returns 0 and\n",
      "        raises ``UndefinedMetricWarning``.\n",
      "    \n",
      "    r2_score(y_true, y_pred, sample_weight=None, multioutput='uniform_average')\n",
      "        R^2 (coefficient of determination) regression score function.\n",
      "        \n",
      "        Best possible score is 1.0 and it can be negative (because the\n",
      "        model can be arbitrarily worse). A constant model that always\n",
      "        predicts the expected value of y, disregarding the input features,\n",
      "        would get a R^2 score of 0.0.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <r2_score>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y_true : array-like of shape = (n_samples) or (n_samples, n_outputs)\n",
      "            Ground truth (correct) target values.\n",
      "        \n",
      "        y_pred : array-like of shape = (n_samples) or (n_samples, n_outputs)\n",
      "            Estimated target values.\n",
      "        \n",
      "        sample_weight : array-like of shape = (n_samples), optional\n",
      "            Sample weights.\n",
      "        \n",
      "        multioutput : string in ['raw_values', 'uniform_average', 'variance_weighted'] or None or array-like of shape (n_outputs)\n",
      "        \n",
      "            Defines aggregating of multiple output scores.\n",
      "            Array-like value defines weights used to average scores.\n",
      "            Default is \"uniform_average\".\n",
      "        \n",
      "            'raw_values' :\n",
      "                Returns a full set of scores in case of multioutput input.\n",
      "        \n",
      "            'uniform_average' :\n",
      "                Scores of all outputs are averaged with uniform weight.\n",
      "        \n",
      "            'variance_weighted' :\n",
      "                Scores of all outputs are averaged, weighted by the variances\n",
      "                of each individual output.\n",
      "        \n",
      "            .. versionchanged:: 0.19\n",
      "                Default value of multioutput is 'uniform_average'.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        z : float or ndarray of floats\n",
      "            The R^2 score or ndarray of scores if 'multioutput' is\n",
      "            'raw_values'.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        This is not a symmetric function.\n",
      "        \n",
      "        Unlike most other scores, R^2 score may be negative (it need not actually\n",
      "        be the square of a quantity R).\n",
      "        \n",
      "        This metric is not well-defined for single samples and will return a NaN\n",
      "        value if n_samples is less than two.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] `Wikipedia entry on the Coefficient of determination\n",
      "                <https://en.wikipedia.org/wiki/Coefficient_of_determination>`_\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from sklearn.metrics import r2_score\n",
      "        >>> y_true = [3, -0.5, 2, 7]\n",
      "        >>> y_pred = [2.5, 0.0, 2, 8]\n",
      "        >>> r2_score(y_true, y_pred)  # doctest: +ELLIPSIS\n",
      "        0.948...\n",
      "        >>> y_true = [[0.5, 1], [-1, 1], [7, -6]]\n",
      "        >>> y_pred = [[0, 2], [-1, 2], [8, -5]]\n",
      "        >>> r2_score(y_true, y_pred,\n",
      "        ...          multioutput='variance_weighted') # doctest: +ELLIPSIS\n",
      "        0.938...\n",
      "        >>> y_true = [1, 2, 3]\n",
      "        >>> y_pred = [1, 2, 3]\n",
      "        >>> r2_score(y_true, y_pred)\n",
      "        1.0\n",
      "        >>> y_true = [1, 2, 3]\n",
      "        >>> y_pred = [2, 2, 2]\n",
      "        >>> r2_score(y_true, y_pred)\n",
      "        0.0\n",
      "        >>> y_true = [1, 2, 3]\n",
      "        >>> y_pred = [3, 2, 1]\n",
      "        >>> r2_score(y_true, y_pred)\n",
      "        -3.0\n",
      "    \n",
      "    recall_score(y_true, y_pred, labels=None, pos_label=1, average='binary', sample_weight=None)\n",
      "        Compute the recall\n",
      "        \n",
      "        The recall is the ratio ``tp / (tp + fn)`` where ``tp`` is the number of\n",
      "        true positives and ``fn`` the number of false negatives. The recall is\n",
      "        intuitively the ability of the classifier to find all the positive samples.\n",
      "        \n",
      "        The best value is 1 and the worst value is 0.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <precision_recall_f_measure_metrics>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y_true : 1d array-like, or label indicator array / sparse matrix\n",
      "            Ground truth (correct) target values.\n",
      "        \n",
      "        y_pred : 1d array-like, or label indicator array / sparse matrix\n",
      "            Estimated targets as returned by a classifier.\n",
      "        \n",
      "        labels : list, optional\n",
      "            The set of labels to include when ``average != 'binary'``, and their\n",
      "            order if ``average is None``. Labels present in the data can be\n",
      "            excluded, for example to calculate a multiclass average ignoring a\n",
      "            majority negative class, while labels not present in the data will\n",
      "            result in 0 components in a macro average. For multilabel targets,\n",
      "            labels are column indices. By default, all labels in ``y_true`` and\n",
      "            ``y_pred`` are used in sorted order.\n",
      "        \n",
      "            .. versionchanged:: 0.17\n",
      "               parameter *labels* improved for multiclass problem.\n",
      "        \n",
      "        pos_label : str or int, 1 by default\n",
      "            The class to report if ``average='binary'`` and the data is binary.\n",
      "            If the data are multiclass or multilabel, this will be ignored;\n",
      "            setting ``labels=[pos_label]`` and ``average != 'binary'`` will report\n",
      "            scores for that label only.\n",
      "        \n",
      "        average : string, [None, 'binary' (default), 'micro', 'macro', 'samples',                        'weighted']\n",
      "            This parameter is required for multiclass/multilabel targets.\n",
      "            If ``None``, the scores for each class are returned. Otherwise, this\n",
      "            determines the type of averaging performed on the data:\n",
      "        \n",
      "            ``'binary'``:\n",
      "                Only report results for the class specified by ``pos_label``.\n",
      "                This is applicable only if targets (``y_{true,pred}``) are binary.\n",
      "            ``'micro'``:\n",
      "                Calculate metrics globally by counting the total true positives,\n",
      "                false negatives and false positives.\n",
      "            ``'macro'``:\n",
      "                Calculate metrics for each label, and find their unweighted\n",
      "                mean.  This does not take label imbalance into account.\n",
      "            ``'weighted'``:\n",
      "                Calculate metrics for each label, and find their average weighted\n",
      "                by support (the number of true instances for each label). This\n",
      "                alters 'macro' to account for label imbalance; it can result in an\n",
      "                F-score that is not between precision and recall.\n",
      "            ``'samples'``:\n",
      "                Calculate metrics for each instance, and find their average (only\n",
      "                meaningful for multilabel classification where this differs from\n",
      "                :func:`accuracy_score`).\n",
      "        \n",
      "        sample_weight : array-like of shape = [n_samples], optional\n",
      "            Sample weights.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        recall : float (if average is not None) or array of float, shape =        [n_unique_labels]\n",
      "            Recall of the positive class in binary classification or weighted\n",
      "            average of the recall of each class for the multiclass task.\n",
      "        \n",
      "        See also\n",
      "        --------\n",
      "        precision_recall_fscore_support, balanced_accuracy_score,\n",
      "        multilabel_confusion_matrix\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from sklearn.metrics import recall_score\n",
      "        >>> y_true = [0, 1, 2, 0, 1, 2]\n",
      "        >>> y_pred = [0, 2, 1, 0, 0, 1]\n",
      "        >>> recall_score(y_true, y_pred, average='macro')  # doctest: +ELLIPSIS\n",
      "        0.33...\n",
      "        >>> recall_score(y_true, y_pred, average='micro')  # doctest: +ELLIPSIS\n",
      "        0.33...\n",
      "        >>> recall_score(y_true, y_pred, average='weighted')  # doctest: +ELLIPSIS\n",
      "        0.33...\n",
      "        >>> recall_score(y_true, y_pred, average=None)\n",
      "        array([1., 0., 0.])\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        When ``true positive + false negative == 0``, recall returns 0 and raises\n",
      "        ``UndefinedMetricWarning``.\n",
      "    \n",
      "    roc_auc_score(y_true, y_score, average='macro', sample_weight=None, max_fpr=None)\n",
      "        Compute Area Under the Receiver Operating Characteristic Curve (ROC AUC)\n",
      "        from prediction scores.\n",
      "        \n",
      "        Note: this implementation is restricted to the binary classification task\n",
      "        or multilabel classification task in label indicator format.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <roc_metrics>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y_true : array, shape = [n_samples] or [n_samples, n_classes]\n",
      "            True binary labels or binary label indicators.\n",
      "        \n",
      "        y_score : array, shape = [n_samples] or [n_samples, n_classes]\n",
      "            Target scores, can either be probability estimates of the positive\n",
      "            class, confidence values, or non-thresholded measure of decisions\n",
      "            (as returned by \"decision_function\" on some classifiers). For binary\n",
      "            y_true, y_score is supposed to be the score of the class with greater\n",
      "            label.\n",
      "        \n",
      "        average : string, [None, 'micro', 'macro' (default), 'samples', 'weighted']\n",
      "            If ``None``, the scores for each class are returned. Otherwise,\n",
      "            this determines the type of averaging performed on the data:\n",
      "        \n",
      "            ``'micro'``:\n",
      "                Calculate metrics globally by considering each element of the label\n",
      "                indicator matrix as a label.\n",
      "            ``'macro'``:\n",
      "                Calculate metrics for each label, and find their unweighted\n",
      "                mean.  This does not take label imbalance into account.\n",
      "            ``'weighted'``:\n",
      "                Calculate metrics for each label, and find their average, weighted\n",
      "                by support (the number of true instances for each label).\n",
      "            ``'samples'``:\n",
      "                Calculate metrics for each instance, and find their average.\n",
      "        \n",
      "            Will be ignored when ``y_true`` is binary.\n",
      "        \n",
      "        sample_weight : array-like of shape = [n_samples], optional\n",
      "            Sample weights.\n",
      "        \n",
      "        max_fpr : float > 0 and <= 1, optional\n",
      "            If not ``None``, the standardized partial AUC [3]_ over the range\n",
      "            [0, max_fpr] is returned.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        auc : float\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] `Wikipedia entry for the Receiver operating characteristic\n",
      "                <https://en.wikipedia.org/wiki/Receiver_operating_characteristic>`_\n",
      "        \n",
      "        .. [2] Fawcett T. An introduction to ROC analysis[J]. Pattern Recognition\n",
      "               Letters, 2006, 27(8):861-874.\n",
      "        \n",
      "        .. [3] `Analyzing a portion of the ROC curve. McClish, 1989\n",
      "                <https://www.ncbi.nlm.nih.gov/pubmed/2668680>`_\n",
      "        \n",
      "        See also\n",
      "        --------\n",
      "        average_precision_score : Area under the precision-recall curve\n",
      "        \n",
      "        roc_curve : Compute Receiver operating characteristic (ROC) curve\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> import numpy as np\n",
      "        >>> from sklearn.metrics import roc_auc_score\n",
      "        >>> y_true = np.array([0, 0, 1, 1])\n",
      "        >>> y_scores = np.array([0.1, 0.4, 0.35, 0.8])\n",
      "        >>> roc_auc_score(y_true, y_scores)\n",
      "        0.75\n",
      "    \n",
      "    roc_curve(y_true, y_score, pos_label=None, sample_weight=None, drop_intermediate=True)\n",
      "        Compute Receiver operating characteristic (ROC)\n",
      "        \n",
      "        Note: this implementation is restricted to the binary classification task.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <roc_metrics>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        \n",
      "        y_true : array, shape = [n_samples]\n",
      "            True binary labels. If labels are not either {-1, 1} or {0, 1}, then\n",
      "            pos_label should be explicitly given.\n",
      "        \n",
      "        y_score : array, shape = [n_samples]\n",
      "            Target scores, can either be probability estimates of the positive\n",
      "            class, confidence values, or non-thresholded measure of decisions\n",
      "            (as returned by \"decision_function\" on some classifiers).\n",
      "        \n",
      "        pos_label : int or str, default=None\n",
      "            The label of the positive class.\n",
      "            When ``pos_label=None``, if y_true is in {-1, 1} or {0, 1},\n",
      "            ``pos_label`` is set to 1, otherwise an error will be raised.\n",
      "        \n",
      "        sample_weight : array-like of shape = [n_samples], optional\n",
      "            Sample weights.\n",
      "        \n",
      "        drop_intermediate : boolean, optional (default=True)\n",
      "            Whether to drop some suboptimal thresholds which would not appear\n",
      "            on a plotted ROC curve. This is useful in order to create lighter\n",
      "            ROC curves.\n",
      "        \n",
      "            .. versionadded:: 0.17\n",
      "               parameter *drop_intermediate*.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        fpr : array, shape = [>2]\n",
      "            Increasing false positive rates such that element i is the false\n",
      "            positive rate of predictions with score >= thresholds[i].\n",
      "        \n",
      "        tpr : array, shape = [>2]\n",
      "            Increasing true positive rates such that element i is the true\n",
      "            positive rate of predictions with score >= thresholds[i].\n",
      "        \n",
      "        thresholds : array, shape = [n_thresholds]\n",
      "            Decreasing thresholds on the decision function used to compute\n",
      "            fpr and tpr. `thresholds[0]` represents no instances being predicted\n",
      "            and is arbitrarily set to `max(y_score) + 1`.\n",
      "        \n",
      "        See also\n",
      "        --------\n",
      "        roc_auc_score : Compute the area under the ROC curve\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        Since the thresholds are sorted from low to high values, they\n",
      "        are reversed upon returning them to ensure they correspond to both ``fpr``\n",
      "        and ``tpr``, which are sorted in reversed order during their calculation.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] `Wikipedia entry for the Receiver operating characteristic\n",
      "                <https://en.wikipedia.org/wiki/Receiver_operating_characteristic>`_\n",
      "        \n",
      "        .. [2] Fawcett T. An introduction to ROC analysis[J]. Pattern Recognition\n",
      "               Letters, 2006, 27(8):861-874.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> import numpy as np\n",
      "        >>> from sklearn import metrics\n",
      "        >>> y = np.array([1, 1, 2, 2])\n",
      "        >>> scores = np.array([0.1, 0.4, 0.35, 0.8])\n",
      "        >>> fpr, tpr, thresholds = metrics.roc_curve(y, scores, pos_label=2)\n",
      "        >>> fpr\n",
      "        array([0. , 0. , 0.5, 0.5, 1. ])\n",
      "        >>> tpr\n",
      "        array([0. , 0.5, 0.5, 1. , 1. ])\n",
      "        >>> thresholds\n",
      "        array([1.8 , 0.8 , 0.4 , 0.35, 0.1 ])\n",
      "    \n",
      "    silhouette_samples(X, labels, metric='euclidean', **kwds)\n",
      "        Compute the Silhouette Coefficient for each sample.\n",
      "        \n",
      "        The Silhouette Coefficient is a measure of how well samples are clustered\n",
      "        with samples that are similar to themselves. Clustering models with a high\n",
      "        Silhouette Coefficient are said to be dense, where samples in the same\n",
      "        cluster are similar to each other, and well separated, where samples in\n",
      "        different clusters are not very similar to each other.\n",
      "        \n",
      "        The Silhouette Coefficient is calculated using the mean intra-cluster\n",
      "        distance (``a``) and the mean nearest-cluster distance (``b``) for each\n",
      "        sample.  The Silhouette Coefficient for a sample is ``(b - a) / max(a,\n",
      "        b)``.\n",
      "        Note that Silhouette Coefficient is only defined if number of labels\n",
      "        is 2 <= n_labels <= n_samples - 1.\n",
      "        \n",
      "        This function returns the Silhouette Coefficient for each sample.\n",
      "        \n",
      "        The best value is 1 and the worst value is -1. Values near 0 indicate\n",
      "        overlapping clusters.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <silhouette_coefficient>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        X : array [n_samples_a, n_samples_a] if metric == \"precomputed\", or,              [n_samples_a, n_features] otherwise\n",
      "            Array of pairwise distances between samples, or a feature array.\n",
      "        \n",
      "        labels : array, shape = [n_samples]\n",
      "                 label values for each sample\n",
      "        \n",
      "        metric : string, or callable\n",
      "            The metric to use when calculating distance between instances in a\n",
      "            feature array. If metric is a string, it must be one of the options\n",
      "            allowed by :func:`sklearn.metrics.pairwise.pairwise_distances`. If X is\n",
      "            the distance array itself, use \"precomputed\" as the metric.\n",
      "        \n",
      "        `**kwds` : optional keyword parameters\n",
      "            Any further parameters are passed directly to the distance function.\n",
      "            If using a ``scipy.spatial.distance`` metric, the parameters are still\n",
      "            metric dependent. See the scipy docs for usage examples.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        silhouette : array, shape = [n_samples]\n",
      "            Silhouette Coefficient for each samples.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        \n",
      "        .. [1] `Peter J. Rousseeuw (1987). \"Silhouettes: a Graphical Aid to the\n",
      "           Interpretation and Validation of Cluster Analysis\". Computational\n",
      "           and Applied Mathematics 20: 53-65.\n",
      "           <https://www.sciencedirect.com/science/article/pii/0377042787901257>`_\n",
      "        \n",
      "        .. [2] `Wikipedia entry on the Silhouette Coefficient\n",
      "           <https://en.wikipedia.org/wiki/Silhouette_(clustering)>`_\n",
      "    \n",
      "    silhouette_score(X, labels, metric='euclidean', sample_size=None, random_state=None, **kwds)\n",
      "        Compute the mean Silhouette Coefficient of all samples.\n",
      "        \n",
      "        The Silhouette Coefficient is calculated using the mean intra-cluster\n",
      "        distance (``a``) and the mean nearest-cluster distance (``b``) for each\n",
      "        sample.  The Silhouette Coefficient for a sample is ``(b - a) / max(a,\n",
      "        b)``.  To clarify, ``b`` is the distance between a sample and the nearest\n",
      "        cluster that the sample is not a part of.\n",
      "        Note that Silhouette Coefficient is only defined if number of labels\n",
      "        is 2 <= n_labels <= n_samples - 1.\n",
      "        \n",
      "        This function returns the mean Silhouette Coefficient over all samples.\n",
      "        To obtain the values for each sample, use :func:`silhouette_samples`.\n",
      "        \n",
      "        The best value is 1 and the worst value is -1. Values near 0 indicate\n",
      "        overlapping clusters. Negative values generally indicate that a sample has\n",
      "        been assigned to the wrong cluster, as a different cluster is more similar.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <silhouette_coefficient>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        X : array [n_samples_a, n_samples_a] if metric == \"precomputed\", or,              [n_samples_a, n_features] otherwise\n",
      "            Array of pairwise distances between samples, or a feature array.\n",
      "        \n",
      "        labels : array, shape = [n_samples]\n",
      "             Predicted labels for each sample.\n",
      "        \n",
      "        metric : string, or callable\n",
      "            The metric to use when calculating distance between instances in a\n",
      "            feature array. If metric is a string, it must be one of the options\n",
      "            allowed by :func:`metrics.pairwise.pairwise_distances\n",
      "            <sklearn.metrics.pairwise.pairwise_distances>`. If X is the distance\n",
      "            array itself, use ``metric=\"precomputed\"``.\n",
      "        \n",
      "        sample_size : int or None\n",
      "            The size of the sample to use when computing the Silhouette Coefficient\n",
      "            on a random subset of the data.\n",
      "            If ``sample_size is None``, no sampling is used.\n",
      "        \n",
      "        random_state : int, RandomState instance or None, optional (default=None)\n",
      "            The generator used to randomly select a subset of samples.  If int,\n",
      "            random_state is the seed used by the random number generator; If\n",
      "            RandomState instance, random_state is the random number generator; If\n",
      "            None, the random number generator is the RandomState instance used by\n",
      "            `np.random`. Used when ``sample_size is not None``.\n",
      "        \n",
      "        **kwds : optional keyword parameters\n",
      "            Any further parameters are passed directly to the distance function.\n",
      "            If using a scipy.spatial.distance metric, the parameters are still\n",
      "            metric dependent. See the scipy docs for usage examples.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        silhouette : float\n",
      "            Mean Silhouette Coefficient for all samples.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        \n",
      "        .. [1] `Peter J. Rousseeuw (1987). \"Silhouettes: a Graphical Aid to the\n",
      "           Interpretation and Validation of Cluster Analysis\". Computational\n",
      "           and Applied Mathematics 20: 53-65.\n",
      "           <https://www.sciencedirect.com/science/article/pii/0377042787901257>`_\n",
      "        \n",
      "        .. [2] `Wikipedia entry on the Silhouette Coefficient\n",
      "               <https://en.wikipedia.org/wiki/Silhouette_(clustering)>`_\n",
      "    \n",
      "    v_measure_score(labels_true, labels_pred, beta=1.0)\n",
      "        V-measure cluster labeling given a ground truth.\n",
      "        \n",
      "        This score is identical to :func:`normalized_mutual_info_score` with\n",
      "        the ``'arithmetic'`` option for averaging.\n",
      "        \n",
      "        The V-measure is the harmonic mean between homogeneity and completeness::\n",
      "        \n",
      "            v = (1 + beta) * homogeneity * completeness\n",
      "                 / (beta * homogeneity + completeness)\n",
      "        \n",
      "        This metric is independent of the absolute values of the labels:\n",
      "        a permutation of the class or cluster label values won't change the\n",
      "        score value in any way.\n",
      "        \n",
      "        This metric is furthermore symmetric: switching ``label_true`` with\n",
      "        ``label_pred`` will return the same score value. This can be useful to\n",
      "        measure the agreement of two independent label assignments strategies\n",
      "        on the same dataset when the real ground truth is not known.\n",
      "        \n",
      "        \n",
      "        Read more in the :ref:`User Guide <homogeneity_completeness>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        labels_true : int array, shape = [n_samples]\n",
      "            ground truth class labels to be used as a reference\n",
      "        \n",
      "        labels_pred : array, shape = [n_samples]\n",
      "            cluster labels to evaluate\n",
      "        \n",
      "        beta : float\n",
      "            Ratio of weight attributed to ``homogeneity`` vs ``completeness``.\n",
      "            If ``beta`` is greater than 1, ``completeness`` is weighted more\n",
      "            strongly in the calculation. If ``beta`` is less than 1,\n",
      "            ``homogeneity`` is weighted more strongly.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        v_measure : float\n",
      "           score between 0.0 and 1.0. 1.0 stands for perfectly complete labeling\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        \n",
      "        .. [1] `Andrew Rosenberg and Julia Hirschberg, 2007. V-Measure: A\n",
      "           conditional entropy-based external cluster evaluation measure\n",
      "           <https://aclweb.org/anthology/D/D07/D07-1043.pdf>`_\n",
      "        \n",
      "        See also\n",
      "        --------\n",
      "        homogeneity_score\n",
      "        completeness_score\n",
      "        normalized_mutual_info_score\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        \n",
      "        Perfect labelings are both homogeneous and complete, hence have score 1.0::\n",
      "        \n",
      "          >>> from sklearn.metrics.cluster import v_measure_score\n",
      "          >>> v_measure_score([0, 0, 1, 1], [0, 0, 1, 1])\n",
      "          1.0\n",
      "          >>> v_measure_score([0, 0, 1, 1], [1, 1, 0, 0])\n",
      "          1.0\n",
      "        \n",
      "        Labelings that assign all classes members to the same clusters\n",
      "        are complete be not homogeneous, hence penalized::\n",
      "        \n",
      "          >>> print(\"%.6f\" % v_measure_score([0, 0, 1, 2], [0, 0, 1, 1]))\n",
      "          ...                                                  # doctest: +ELLIPSIS\n",
      "          0.8...\n",
      "          >>> print(\"%.6f\" % v_measure_score([0, 1, 2, 3], [0, 0, 1, 1]))\n",
      "          ...                                                  # doctest: +ELLIPSIS\n",
      "          0.66...\n",
      "        \n",
      "        Labelings that have pure clusters with members coming from the same\n",
      "        classes are homogeneous but un-necessary splits harms completeness\n",
      "        and thus penalize V-measure as well::\n",
      "        \n",
      "          >>> print(\"%.6f\" % v_measure_score([0, 0, 1, 1], [0, 0, 1, 2]))\n",
      "          ...                                                  # doctest: +ELLIPSIS\n",
      "          0.8...\n",
      "          >>> print(\"%.6f\" % v_measure_score([0, 0, 1, 1], [0, 1, 2, 3]))\n",
      "          ...                                                  # doctest: +ELLIPSIS\n",
      "          0.66...\n",
      "        \n",
      "        If classes members are completely split across different clusters,\n",
      "        the assignment is totally incomplete, hence the V-Measure is null::\n",
      "        \n",
      "          >>> print(\"%.6f\" % v_measure_score([0, 0, 0, 0], [0, 1, 2, 3]))\n",
      "          ...                                                  # doctest: +ELLIPSIS\n",
      "          0.0...\n",
      "        \n",
      "        Clusters that include samples from totally different classes totally\n",
      "        destroy the homogeneity of the labeling, hence::\n",
      "        \n",
      "          >>> print(\"%.6f\" % v_measure_score([0, 0, 1, 1], [0, 0, 0, 0]))\n",
      "          ...                                                  # doctest: +ELLIPSIS\n",
      "          0.0...\n",
      "    \n",
      "    zero_one_loss(y_true, y_pred, normalize=True, sample_weight=None)\n",
      "        Zero-one classification loss.\n",
      "        \n",
      "        If normalize is ``True``, return the fraction of misclassifications\n",
      "        (float), else it returns the number of misclassifications (int). The best\n",
      "        performance is 0.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <zero_one_loss>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y_true : 1d array-like, or label indicator array / sparse matrix\n",
      "            Ground truth (correct) labels.\n",
      "        \n",
      "        y_pred : 1d array-like, or label indicator array / sparse matrix\n",
      "            Predicted labels, as returned by a classifier.\n",
      "        \n",
      "        normalize : bool, optional (default=True)\n",
      "            If ``False``, return the number of misclassifications.\n",
      "            Otherwise, return the fraction of misclassifications.\n",
      "        \n",
      "        sample_weight : array-like of shape = [n_samples], optional\n",
      "            Sample weights.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        loss : float or int,\n",
      "            If ``normalize == True``, return the fraction of misclassifications\n",
      "            (float), else it returns the number of misclassifications (int).\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        In multilabel classification, the zero_one_loss function corresponds to\n",
      "        the subset zero-one loss: for each sample, the entire set of labels must be\n",
      "        correctly predicted, otherwise the loss for that sample is equal to one.\n",
      "        \n",
      "        See also\n",
      "        --------\n",
      "        accuracy_score, hamming_loss, jaccard_score\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from sklearn.metrics import zero_one_loss\n",
      "        >>> y_pred = [1, 2, 3, 4]\n",
      "        >>> y_true = [2, 2, 3, 4]\n",
      "        >>> zero_one_loss(y_true, y_pred)\n",
      "        0.25\n",
      "        >>> zero_one_loss(y_true, y_pred, normalize=False)\n",
      "        1\n",
      "        \n",
      "        In the multilabel case with binary label indicators:\n",
      "        \n",
      "        >>> import numpy as np\n",
      "        >>> zero_one_loss(np.array([[0, 1], [1, 1]]), np.ones((2, 2)))\n",
      "        0.5\n",
      "\n",
      "DATA\n",
      "    SCORERS = {'accuracy': make_scorer(accuracy_score), 'adjusted_mutual_i...\n",
      "    __all__ = ['accuracy_score', 'adjusted_mutual_info_score', 'adjusted_r...\n",
      "\n",
      "FILE\n",
      "    /home/dir0417/anaconda3/lib/python3.7/site-packages/sklearn/metrics/__init__.py\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(sklearn.metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis\n",
    "We sought to classify label (0 or 1) for RNA seq data using a random forest method. We used all columns in the data set to train the data to accurately classify each observation. 30% of the data was used for testing and the remaining 70% was used for training. The accuracy rate of our classification was 95% in the testing group. Our confusion matrix indicates that just 1 label in the testing group was incorrect. We now have a model that can accurately classify group identity using RNA seq data 95% of the time. This is almost too good to be true.\n",
    "\n",
    "Labels aren't clear. Kidney disease.\n",
    "Fairly easy thing to predict.\n",
    "Matthew's corrcoef.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns vs df.col\n",
    "toodle = df[0:10].copy()\n",
    "print (toodle)\n",
    "toodle.drop (54, inplace = True)\n",
    "print (toodle)\n",
    "help(toodle.reindex)\n",
    "toodle.drop(-1:)\n",
    "toodle.reindex(np.arange(len(toodle)))\n",
    "toodle.index = np.arange(len(toodle))\n",
    "toodle"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
